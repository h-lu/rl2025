---
title: "第二周：强化学习框架与迷宫环境"
---

## 课程目标

- 理解马尔可夫决策过程 (MDP) 的基本思想
- 掌握策略 (Policy)、价值函数 (Value Function) 的概念
- 理解探索 (Exploration) 与利用 (Exploitation) 的平衡
- 学习使用 Gymnasium 库搭建迷宫环境 (Grid World)
- 掌握使用 AI 辅助工具进行代码补全和修改

## 第一次课：强化学习框架与迷宫环境 (Grid World) 搭建

### 1. 马尔可夫决策过程 (MDP) 基础

::: {.callout-note}
马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习的核心框架，用于形式化描述智能体与环境的交互过程。
:::

#### 1.1  MDP 的核心要素 (无需数学细节，侧重直观理解)

- **状态 (State, S)**
    -  环境的描述，包含了智能体做出决策所需的信息。
    -  **马尔可夫性质**:  当前状态包含了所有历史信息，未来的状态只依赖于当前状态和动作，而与过去的历史无关。
    -  在迷宫环境中，状态可以是智能体在迷宫中的位置坐标。

- **动作 (Action, A)**
    -  智能体在每个状态下可以采取的行为。
    -  在迷宫环境中，动作可以是向上、下、左、右移动。

- **转移概率 (Transition Probability, P)**
    -  智能体在状态 $s$ 采取动作 $a$ 后，转移到下一个状态 $s'$ 的概率。
    -  $P(s'|s, a)$ 表示在状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 的概率。
    -  在确定性迷宫环境中，转移概率是确定的 (例如：向上走，一定到达上方的格子)。在非确定性环境中，转移概率可能存在随机性 (例如：在某些环境中，向上走，可能以 0.8 的概率到达上方格子，0.2 的概率滑到其他格子)。

- **奖励 (Reward, R)**
    -  智能体在与环境交互后获得的反馈信号，用于评价动作的好坏。
    -  $R(s, a, s')$ 表示在状态 $s$ 采取动作 $a$ 转移到状态 $s'$ 后获得的奖励。
    -  在迷宫寻宝游戏中，到达宝藏位置可以获得正奖励，撞墙或者到达陷阱位置可能获得负奖励，正常移动可能获得小的负奖励 (鼓励尽快寻宝)。

- **策略 (Policy, $\pi$)**
    -  智能体根据当前状态选择动作的规则，可以是确定性的或随机性的。
    -  $\pi(a|s)$ 表示在状态 $s$ 下选择动作 $a$ 的概率。
    -  强化学习的目标是学习最优策略，使得智能体在环境中获得最大的累积奖励。

#### 1.2  MDP 过程示意

```{mermaid}
graph LR
    S --> A(动作 Action)
    A --> S'(下一个状态 State)
    S' --> R(奖励 Reward)
    R --> S
```

智能体 (Agent) 处在状态 (State) S，根据策略 (Policy) 选择动作 (Action) A，环境 (Environment) 接收动作后，转移到新的状态 (State) S'，并给智能体返回奖励 (Reward) R。智能体不断与环境交互，目标是学习到最优策略，最大化累积奖励。

### 2. 价值函数 (Value Function)

::: {.callout-note}
价值函数用于评估在给定状态或状态-动作对下，未来预期累积奖励的期望值。价值函数是强化学习算法的核心概念之一。
:::

#### 2.1  V 函数 (State Value Function)

-  $V_{\pi}(s)$ 表示在策略 $\pi$ 下，从状态 $s$ 出发，未来可以获得的期望累积奖励。
-  V 函数评估的是**状态的价值**，即处于某个状态的好坏程度。
-  V 函数越大，表示当前状态越好，未来可以获得的期望奖励越高。

#### 2.2  Q 函数 (Action Value Function)

-  $Q_{\pi}(s, a)$ 表示在策略 $\pi$ 下，从状态 $s$ 出发，**选择动作 $a$ 后**，未来可以获得的期望累积奖励。
-  Q 函数评估的是**状态-动作对的价值**，即在某个状态下，采取某个动作的好坏程度。
-  Q 函数越大，表示在当前状态下，采取该动作越好，未来可以获得的期望奖励越高。

#### 2.3  V 函数和 Q 函数的关系

-  V 函数可以通过 Q 函数计算得到：

    $V_{\pi}(s) = \sum_{a \in A} \pi(a|s) Q_{\pi}(s, a)$

    即状态 $s$ 的价值等于在状态 $s$ 下，所有可能动作的 Q 函数值的**期望** (按照策略 $\pi$ 的动作选择概率进行加权平均)。

-  Q 函数可以通过 V 函数和奖励函数以及转移概率计算得到 (Bellman 方程，下周会详细讲解)。

### 3. 探索 (Exploration) vs. 利用 (Exploitation)

::: {.callout-important}
探索与利用是强化学习中一个核心的权衡问题。智能体需要在探索新策略和利用已知最优策略之间找到平衡。
:::

- **探索 (Exploration)**
    -  尝试新的动作，探索未知的状态和动作空间，以发现潜在的更优策略。
    -  例如：在迷宫寻宝游戏中，智能体可能会尝试一些之前没有走过的路径，以期找到更短的路径或者隐藏的宝藏。

- **利用 (Exploitation)**
    -  根据已知的经验，选择当前认为最优的动作，以最大化当前的累积奖励。
    -  例如：在迷宫寻宝游戏中，如果智能体已经知道某条路径可以到达宝藏，它可能会重复选择这条路径，以快速获得奖励。

#### 3.1  探索与利用的平衡

-  **过度探索**：可能导致智能体花费大量时间在探索无用的区域，而错失已知的最优策略。
-  **过度利用**：可能导致智能体陷入局部最优解，而无法发现全局最优策略。

-  **$\epsilon$-greedy 策略** 是一种常用的平衡探索与利用的策略：
    -  以概率 $\epsilon$ (探索率) 随机选择动作 (探索)。
    -  以概率 $1-\epsilon$ 选择当前 Q 函数值最大的动作 (利用)。
    -  $\epsilon$ 的值通常会随着训练的进行而逐渐减小，从而在训练初期鼓励探索，在训练后期侧重利用。

#### 3.2  商业案例类比

-  **新市场尝试 vs. 现有市场深耕**

    -  **新市场尝试 (探索)**：企业尝试进入新的市场领域，例如：开发新的产品线、拓展新的客户群体、进入新的地理区域。
        -  **优点**：可能发现新的增长机会，拓展业务范围，提高长期竞争力。
        -  **缺点**：风险较高，投入成本较大，短期收益不确定。

    -  **现有市场深耕 (利用)**：企业在现有市场领域深耕细作，例如：优化现有产品、提高客户满意度、提升市场份额。
        -  **优点**：风险较低，收益相对稳定，短期回报可观。
        -  **缺点**：增长空间有限，可能错失新的市场机会，长期竞争力可能不足。

    -  **平衡策略**：企业需要在新市场尝试和现有市场深耕之间找到平衡点，例如：
        -  **双元策略 (Dual Strategy)**：同时进行探索性创新和利用性优化。
        -  **阶段性策略**：在不同发展阶段侧重不同的策略，例如：初创期侧重探索，成长期侧重利用，成熟期再次侧重探索。

### 4. 迷宫环境 (Grid World) 搭建 (Gymnasium)

::: {.callout-note}
Grid World 是强化学习中常用的简单环境，非常适合初学者入门。我们将使用 Gymnasium 库来搭建 Grid World 环境。
:::

#### 4.1  Gymnasium 库安装与基本使用 (回顾)

```python
# 安装 Gymnasium (如果已安装，可以跳过)
# pip install gymnasium

import gymnasium as gym

# 创建 CartPole 环境 (回顾)
env = gym.make("CartPole-v1", render_mode="human")

# 重置环境
observation, info = env.reset()

# 随机选择一个动作
action = env.action_space.sample()

# 执行动作，获取环境反馈
observation, reward, terminated, truncated, info = env.step(action)

# 关闭环境
env.close()
```

#### 4.2  自定义 Grid World 环境

1.  **创建自定义环境类**

    ```python
    import gymnasium as gym
    from gymnasium import spaces
    import numpy as np

    class GridWorldEnv(gym.Env):
        metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}

        def __init__(self, render_mode=None, size=5):
            super().__init__()
            self.size = size  # Grid world size
            self.window_size = 512  # The size of the PyGame window

            # 动作空间：上下左右 (4个离散动作)
            self.action_space = spaces.Discrete(4)
            # 观测空间：智能体在 Grid world 中的位置 (行、列坐标)
            self.observation_space = spaces.Discrete(size * size)

            """
            Map of the grid world:
            - 0: 空格 (可以自由移动)
            - 1: 障碍物 (无法通过)
            - 2: 宝藏 (目标位置，获得奖励)
            - 3: 陷阱 (负奖励)
            - 4: 智能体起始位置
            """
            self._grid_map = np.array([
                [0, 0, 0, 0, 0],
                [0, 1, 1, 1, 0],
                [0, 1, 2, 1, 0],
                [0, 1, 1, 1, 0],
                [0, 0, 0, 0, 0]
            ])

            # 宝藏位置
            self._target_location = np.array([2, 2])
            # 陷阱位置 (可以添加多个陷阱位置)
            self._trap_location = np.array([4, 4])
            # 智能体起始位置
            self._agent_start_location = np.array([0, 0])

            self._agent_location = np.array([0, 0])

            self.render_mode = render_mode

            """
            对于 render_mode="human" 或 "rgb_array"，需要初始化 PyGame
            """
            if render_mode is not None:
                import pygame

                pygame.init()
                pygame.font.init()
                self.window = pygame.display.set_mode(
                    (self.window_size, self.window_size)
                )
                self.clock = pygame.time.Clock()

        def _get_obs(self):
            # 将 2D 坐标转换为 1D 状态表示
            return np.ravel_multi_index(self._agent_location, (self.size, self.size))

        def _get_info(self):
            return {
                "agent_location": self._agent_location,
                "target_location": self._target_location,
                "distance_to_target": np.linalg.norm(self._agent_location - self._target_location, ord=1)
            }

        def reset(self, seed=None, options=None):
            super().reset(seed=seed)
            # 将智能体放置在起始位置
            self._agent_location = self._agent_start_location

            observation = self._get_obs()
            info = self._get_info()

            if self.render_mode == "human":
                self._render_frame()

            return observation, info

        def step(self, action):
            # 动作编码：0=上, 1=右, 2=下, 3=左
            direction = np.array([
                [-1, 0], # 上
                [0, 1],  # 右
                [1, 0],  # 下
                [0, -1]  # 左
            ])
            # 智能体移动到下一个位置 (如果超出边界，则保持原位置)
            proposed_location = self._agent_location + direction[action]

            # 判断是否超出边界
            if not (0 <= proposed_location[0] < self.size and 0 <= proposed_location[1] < self.size):
                # 超出边界，保持原位置
                self._agent_location = self._agent_location
            elif self._grid_map[proposed_location[0], proposed_location[1]] == 1:
                # 撞到障碍物，保持原位置
                self._agent_location = self._agent_location
            else:
                # 移动到新位置
                self._agent_location = proposed_location

            # 获取新的观测
            observation = self._get_obs()
            info = self._get_info()

            # 根据智能体位置计算奖励 (稀疏奖励)
            if np.array_equal(self._agent_location, self._target_location):
                reward = 10  # 到达宝藏，正奖励
                terminated = True # 游戏结束
            elif np.array_equal(self._agent_location, self._trap_location):
                reward = -10 # 到达陷阱，负奖励
                terminated = True # 游戏结束
            else:
                reward = -1 # 正常移动，小负奖励 (鼓励尽快寻宝)
                terminated = False

            truncated = False #  Grid world 环境没有时间限制，所以 truncated 始终为 False

            if self.render_mode == "human":
                self._render_frame()

            return observation, reward, terminated, truncated, info

        def render(self):
            if self.render_mode == "rgb_array":
                return self._render_frame()
            elif self.render_mode == "human":
                self._render_frame()
                return None

        def _render_frame(self):
            if self.window is None and self.render_mode == "human":
                import pygame
                pygame.init()
                pygame.font.init()
                self.window = pygame.display.set_mode(
                    (self.window_size, self.window_size)
                )
            if self.clock is None and self.render_mode == "human":
                import pygame
                self.clock = pygame.time.Clock()

            canvas = pygame.Surface((self.window_size, self.window_size))
            canvas.fill((255, 255, 255)) # 白色背景
            pix_square_size = (
                self.window_size / self.size
            )  # 一格像素大小

            # 绘制 Grid world 地图
            for row in range(self.size):
                for col in range(self.size):
                    if self._grid_map[row, col] == 1: # 障碍物
                        pygame.draw.rect(
                            canvas,
                            (0, 0, 0), # 黑色
                            pygame.Rect(
                                pix_square_size * col,
                                pix_square_size * row,
                                pix_square_size,
                                pix_square_size,
                            ),
                        )
                    elif self._grid_map[row, col] == 2: # 宝藏
                        pygame.draw.rect(
                            canvas,
                            (255, 255, 0), # 黄色
                            pygame.Rect(
                                pix_square_size * col,
                                pix_square_size * row,
                                pix_square_size,
                                pix_square_size,
                            ),
                        )
                    elif self._grid_map[row, col] == 3: # 陷阱
                        pygame.draw.rect(
                            canvas,
                            (255, 0, 0), # 红色
                            pygame.Rect(
                                pix_square_size * col,
                                pix_square_size * row,
                                pix_square_size,
                                pix_square_size,
                            ),
                        )

            # 绘制智能体
            import pygame
            pygame.draw.circle(
                canvas,
                (0, 0, 255), # 蓝色
                (self._agent_location + 0.5) * pix_square_size,
                min(pix_square_size, pix_square_size) / 3,
            )

            if self.render_mode == "human":
                # The following line copies our drawings from `canvas` to the visible window
                self.window.blit(canvas, canvas.get_rect())
                pygame.event.pump()
                pygame.display.flip()

                # We need to ensure that human-rendering occurs at the predefined FPS.
                # The following line will automatically add a delay to keep the FPS stable.
                self.clock.tick(self.metadata["render_fps"])
            else:  # rgb_array
                return np.transpose(
                    np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)
                )

        def close(self):
            if self.window is not None:
                import pygame
                pygame.display.quit()
                pygame.quit()
    ```

2.  **注册自定义环境**

    ```python
    from gymnasium.envs.registration import register

    register(
        id="GridWorld-v0", # 环境 ID
        entry_point="__main__:GridWorldEnv", #  指向自定义环境类
        kwargs={"render_mode": "human", "size": 5}, #  环境初始化参数
        max_episode_steps=300, #  最大 episode 步数
    )
    ```

    **注意**:  `entry_point="__main__:GridWorldEnv"`  假设你的 `GridWorldEnv` 类定义在当前脚本文件中。如果你的环境类定义在单独的文件中，需要修改 `entry_point` 指向你的环境类。 例如，如果你的环境类定义在 `envs/grid_world.py` 文件中，并且类名为 `CustomGridWorldEnv`,  则 `entry_point`  应该修改为  `entry_point="envs.grid_world:CustomGridWorldEnv"`。

3.  **测试自定义环境**

    ```python
    import gymnasium as gym
    # 导入注册自定义环境 (确保注册代码在运行前被执行)
    import week2

    # 创建自定义 Grid World 环境
    env = gym.make("GridWorld-v0")
    observation, info = env.reset()

    for _ in range(10):
        action = env.action_space.sample() # 随机动作
        observation, reward, terminated, truncated, info = env.step(action)
        if terminated or truncated:
            observation, info = env.reset()
        env.render() # 渲染环境 (human 模式下会显示窗口)

    env.close()
    ```

#### 4.3  AI 辅助编程实践

-  **使用 GitHub Copilot 或 Tabnine 等 AI 工具**，辅助完成 Grid World 环境代码的编写。
-  **例如**：
    -  可以尝试输入注释  `# 创建 GridWorldEnv 类`，让 AI 工具自动补全类定义代码。
    -  可以尝试输入函数签名  `def step(self, action):`，让 AI 工具自动生成 `step` 函数的代码框架。
    -  可以利用 AI 工具的代码补全、代码片段生成、代码解释等功能，提高代码编写效率，并理解代码逻辑。

## 第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建

### 1.  小组项目一：迷宫寻宝 (Grid World) 环境搭建

-  **项目目标**：
    -  以小组为单位，基于第一次课的代码框架，**独立完成**迷宫环境 (Grid World) 的搭建。
    -  **扩展迷宫地图**，设计更复杂的迷宫场景 (例如：增加更多障碍物、宝藏、陷阱等)。
    -  **实现基本的环境渲染**，能够可视化智能体在迷宫中的探索过程。

-  **代码框架**：
    -  提供第一次课中  `GridWorldEnv`  类的代码框架 (作为项目的基础)。
    -  小组需要**自行完成代码的补全、修改和扩展**。

-  **AI 辅助工具**：
    -  鼓励学生**充分利用** GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高开发效率。
    -  **但强调**：AI 工具是辅助手段，学生需要理解代码逻辑，**不能完全依赖** AI 工具生成代码，而忽略代码理解和调试。

-  **项目提交**：
    -  小组**提交**完整的 Grid World 环境代码 (Python 文件)。
    -  **无需提交**项目报告。

### 2.  答疑与指导

-  **解答学生在环境搭建过程中遇到的问题**。
-  **重点关注**：
    -  Gymnasium 库的使用方法。
    -  自定义环境类的结构和接口。
    -  状态空间、动作空间、奖励函数的设计。
    -  环境渲染的实现。
    -  AI 辅助工具的使用技巧。

### 3.  布置小组项目一：迷宫寻宝 (Grid World) 环境搭建

-  **明确项目要求、提交时间和评分标准** (本次项目**不评分**，作为后续项目的基础)。
-  **鼓励小组积极探索、尝试**，遇到问题及时提问。
-  **建议小组**：
    -  **提前开始**项目，预留充足的开发和调试时间。
    -  **分工合作**，提高开发效率。
    -  **充分利用** AI 辅助工具，但也要注重代码理解和调试能力。
    -  **相互交流、学习**，共同解决问题。

### 课后作业

1.  **完成小组项目一：迷宫寻宝 (Grid World) 环境搭建**。
2.  **思考题**：
    -  在 Grid World 环境中，如何设计更有效的奖励函数，以引导智能体更快地找到宝藏？
    -  如果迷宫地图非常大，状态空间会变得很大，对强化学习算法会产生什么影响？
    -  如何使用 AI 辅助工具更高效地进行强化学习代码开发？

### 预习资料

1.  **阅读材料**：
    -  Gymnasium 官方文档 (自定义环境部分)。
    -  强化学习算法基础：Q-Learning 算法初步。
    -  探索-利用平衡的更多策略 (例如：$\epsilon$-greedy 退火策略、UCB 算法等)。

2.  **视频资源**：
    -  Grid World 环境搭建详解。
    -  Q-Learning 算法原理讲解 (初步了解)。
    -  探索与利用的平衡策略讲解。

3.  **下周预习重点**：
    -  Q-Learning 算法原理和步骤。
    -  Q-Table 的更新规则。
    -  使用 Q-Learning 算法解决 Grid World 迷宫寻宝问题。

---

**请注意**:

-   **代码框架**:  第一次课的代码框架，指的是第一次课中  `GridWorldEnv`  类的代码。学生需要基于这个代码框架进行扩展和修改，完成小组项目一。
-   **环境注册**:  请确保在运行 Grid World 环境代码之前，已经执行了环境注册代码 (`register(...)`)。可以将注册代码放在单独的文件中，或者放在运行环境代码的脚本文件的开头。
-   **AI 辅助工具**:  鼓励学生使用 AI 辅助工具，但务必强调理解代码逻辑的重要性，避免过度依赖 AI 工具。
-   **小组项目**:  小组项目一旨在让学生熟悉 Gymnasium 库和自定义环境的流程，为后续的强化学习算法实践打下基础。
