---
title: "Week 8 - 教师指导手册"
subtitle: "Q-Learning 应用讨论与中期回顾"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生能够将 Q-Learning 的思想应用于分析和建模一个简化的商业问题（如动态定价）。
    *   学生能够识别在将 RL 应用于商业问题时，进行 MDP 定义（特别是状态、动作、奖励设计）的关键考量和挑战。
    *   学生理解表格型 Q-Learning 在处理复杂商业问题时的局限性（如状态空间爆炸），为后续引入函数逼近做铺垫。
    *   学生系统性地复习并巩固前半学期所学的核心概念 (MDP, Bellman 方程, MC, TD, SARSA, Q-Learning)。
    *   学生了解中期测试的可能形式和考察重点。
*   **次要目标:**
    *   培养学生批判性思考 RL 应用局限性的能力。
    *   加强理论知识与实际应用场景的联系。
    *   帮助学生梳理知识体系，为后续学习和期末项目做准备。

# 重点概念 (Key Concepts)

*   将商业问题映射到 MDP (S, A, R, P, γ) 的实践考量
*   状态表示 (State Representation) 的挑战 (维度、连续性、近似马尔可夫性)
*   动作空间 (Action Space) 设计 (离散化 vs. 连续)
*   奖励工程 (Reward Engineering) 的挑战 (短期 vs. 长期, 稀疏性, 多目标, 潜在偏差)
*   表格型 RL 的局限性 (维度灾难, 样本效率)
*   中期核心概念回顾:
    *   RL 框架要素
    *   MDP & Bellman 方程 (期望 vs. 最优)
    *   无模型预测 (MC vs. TD, 偏差-方差)
    *   无模型控制 (On-Policy/SARSA vs. Off-Policy/Q-Learning)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 15 (约 90 分钟): Q-Learning 应用讨论**
    *   回顾 Q-Learning 算法 (10 分钟)。
    *   引入简单动态定价案例 (10 分钟): 描述场景和目标。
    *   **引导讨论：MDP 定义 (35-40 分钟):** **重点环节**。引导学生分组或全班讨论如何定义 S, A, R。
        *   **S:** 讨论需要哪些信息？如何处理时间和库存？是否需要其他信息？强调状态设计的权衡。
        *   **A:** 讨论离散价格点。
        *   **R:** **重点讨论**。短期收入？利润？长期影响？如何量化？引导学生认识到奖励设计的复杂性和关键性。
        *   P 和 γ 的讨论。
    *   Q-Learning 应用思路讲解 (10 分钟): 简述如何用 Q 表解决这个简化问题。
    *   **讨论：设计选择的影响与局限性 (15-20 分钟):** 引导学生思考状态/动作/奖励设计不当的影响，以及表格型方法在此问题上的局限性（状态空间大小），自然引出函数逼近的必要性。
    *   Q&A (5 分钟)
*   **Session 16 (约 90 分钟): 中期复习**
    *   **系统性回顾 (60-70 分钟):** **重点环节**。按照讲义中的回顾提纲，系统性地梳理前半学期的核心概念。
        *   **框架与 MDP:** 重新强调基础。
        *   **Bellman 方程:** 对比期望和最优方程。
        *   **无模型预测:** 对比 MC 和 TD，强调偏差-方差。
        *   **无模型控制:** 对比 SARSA 和 Q-Learning，强调 On/Off-Policy 区别和策略偏好。
        *   **使用对比表格和流程图**帮助学生理解。
        *   **穿插提问**，检验学生理解程度。
    *   中期测试准备说明 (10-15 分钟): 告知可能的题型（概念、分析）、考察重点。提供学习建议。
    *   答疑 (5-10 分钟): 解答学生关于复习内容或测试的疑问。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 15

*   **动态定价案例讨论:**
    *   **开放性:** 鼓励学生提出不同的 S, A, R 定义方式，没有标准答案。重点是引导他们思考定义背后的**理由**以及可能带来的**后果**。
    *   **奖励设计 (深入讨论):**
        *   提问：“如果只奖励当天收入，智能体会学到什么策略？”（可能早期高价，后期疯狂降价清仓）。“这符合商店的长期利益吗？”
        *   提问：“如何将‘用户满意度’或‘品牌形象’纳入奖励？”（引导学生认识到量化这些长期、模糊目标的困难）。
    *   **局限性讨论:**
        *   计算状态空间大小：假设时间 T=30天，库存 K=100件，动作 A=5个价格点，Q 表大小是多少？(30*100*5 = 15000)。如果库存是 1000 件？如果再加入一个竞争对手价格状态（假设 10 种）？(状态数急剧增加)。以此说明表格方法的局限。
        *   引出下一部分：“如果状态空间太大，或者状态是连续的（比如价格本身是状态的一部分），表格存不下了怎么办？” → 函数逼近。

## Session 16

*   **中期复习:**
    *   **结构化:** 按照 RL 问题的解决流程（定义问题 MDP -> 评估策略 -> 改进策略/寻找最优策略）来组织复习内容。
    *   **强调对比:** 在讲解 MC/TD 和 SARSA/Q-Learning 时，始终进行对比，突出它们的核心差异和适用场景。使用表格总结关键区别。
    *   **概念关联:** 将不同概念联系起来，例如 Bellman 方程是 TD 学习的基础，GPI 是控制算法的框架等。
    *   **互动提问:** 设计一些选择题或简答题，检验学生对关键概念（如 On/Off-Policy, Bootstrapping, 偏差方差）的理解。
*   **测试准备:**
    *   明确告知学生不需要死记硬背复杂的数学公式，重点是理解**概念、思想、算法流程、优缺点和应用场景**。
    *   强调对 Lab 中涉及的算法和环境（如 CliffWalking 中 SARSA vs Q-Learning 的策略差异）的理解和分析能力。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 动态定价案例中，如果需求模型未知，Q-Learning 怎么知道执行某个价格后的销量和下一个状态？**
    *   A: Q-Learning 是**无模型**的，它不需要知道底层的需求模型（即转移概率 P）。它通过**实际交互**（或者与模拟器交互）来获取数据。流程是：1) 在状态 (t, k) 选择价格 p；2) **环境（或模拟器）**根据其内部（可能未知）的规则返回实际销量和奖励 R；3) 计算出下一个状态 s' = (t-1, k-sales)；4) 使用 (s, p, R, s') 这个**经验样本**来更新 Q((t, k), p)。算法本身不预测销量，而是从经验中学习哪个价格在哪个状态下能带来更好的长期回报。
*   **Q: 表格 Q-Learning 看起来处理不了复杂问题，那我们学它还有用吗？**
    *   A: 非常有用！表格方法是理解更高级的深度强化学习算法（如 DQN）的基础。DQN 本质上就是用神经网络来近似 Q-Learning 中的那个巨大的 Q 表，其核心更新逻辑（TD 误差、目标值计算）与表格 Q-Learning 是一致的。理解了表格 Q-Learning 的原理和局限，才能更好地理解 DQN 为何要引入经验回放、目标网络等技巧。
*   **Q: 中期测试会考代码实现吗？**
    *   A: 根据课程目标，重点是概念理解和分析能力。一般不会要求从零手写复杂的算法代码。但可能会要求你**阅读和理解**给定的算法伪代码或简单代码片段，或者**分析**某个算法在特定场景下的行为和结果（类似 Lab 中的分析）。
*   **Q: 前面这么多概念和算法，怎么复习比较有效？**
    *   A: 建议：1) **梳理主线：**按照“问题定义(MDP) -> 价值评估(Bellman, MC, TD) -> 策略控制(GPI, SARSA, Q-Learning)”这条主线来组织知识。2) **理解核心差异：**重点理解 MC vs TD (偏差方差, 更新时机)，SARSA vs Q-Learning (On/Off-Policy, TD 目标)。3) **回顾 Lab：** Lab 中的例子（Gridworld, Blackjack, CliffWalking）是理解算法行为差异的关键。4) **尝试解释：**用自己的话解释每个核心概念。

# 与后续课程的联系 (Connections to Future Topics)

*   本周对表格方法局限性的讨论，直接引出了下周开始的**函数逼近**和**深度强化学习 (DRL)** 部分。
*   动态定价和推荐系统这两个案例将在后续讨论 DRL 应用时可能再次提及，作为 DRL 发挥作用的典型场景。
*   中期复习巩固的基础知识，是理解后续更复杂算法（DQN, A2C, PG）的前提。

# 教师准备建议 (Preparation Suggestions)

*   准备好引导动态定价案例讨论的问题和不同角度的观点（特别是奖励设计）。
*   清晰地梳理中期复习的核心知识点和它们之间的联系，可以准备总结性的表格或思维导图。
*   准备一些简单的概念测试题用于课堂互动。
*   明确中期测试的范围、形式和评分标准。