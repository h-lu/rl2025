**课程名称：** 商业决策的智能优化：强化学习方法与应用 (Intelligent Optimization for Business Decisions: Reinforcement Learning Methods and Applications)

**面向对象：** 经济管理学院大三学生

**先修课程：**
*   概率论与数理统计基础。
*   基本的经济学/管理学原理。
*   **基本 Python 编程了解：** 能够理解和运行 Python 脚本，了解基本的数据类型、循环、条件语句、函数。不要求精通，可以使用 AI 辅助（如 Copilot, ChatGPT）生成或理解部分代码，但**核心逻辑和结果分析必须由学生完成**。课程会提供必要的代码框架和库的使用说明。

**课程目标：**
学完本课程后，学生应能：
1.  深入理解 RL 核心概念，并熟练将其应用于**分析和构建商业决策问题模型**。
2.  掌握 Q-Learning 等核心算法的原理，并能**运行、调试和修改**基于这些算法的简单模拟。
3.  理解函数逼近的必要性，并能**使用现成库（如 Stable Baselines3）运行**基于 DQN、A2C 等算法的实验。
4.  **设计并实施**简单的 RL 实验来探索商业策略（如定价、库存）。
5.  **批判性地分析和解释** RL 实验结果，并将其与商业目标联系起来。
6.  识别在商业中部署 RL 的**关键数据需求、技术挑战、伦理风险**，并提出应对思路。
7.  培养利用计算工具和数据驱动方法解决复杂商业决策问题的能力。

**教材与参考资料：**
*   **核心教材：** 教师编写的讲义（结合理论、代码示例、商业案例）。
*   **核心代码库：** OpenAI Gym/Gymnasium (环境), NumPy (数值计算), Matplotlib/Seaborn (可视化), **Stable Baselines3 (预置的 DRL 算法库 - 重点)**。
*   **辅助参考：**
    *   Sutton & Barto 书中相关章节（供深入理解）。
    *   Stable Baselines3 文档。
    *   商业分析和 AI 应用相关文章、案例研究。

**教学方式：**
*   **理论讲授 (Lecture):** 40% - 集中在概念、原理、算法思想、商业联系。
*   **编程实验 (Lab):** 40% - 在机房进行，教师引导，学生动手操作、修改代码、运行模拟、分析结果。**代码可部分由 AI 生成，但理解、调试和分析是重点。**
*   **案例分析与讨论 (Case Study & Discussion):** 20% - 结合商业场景，讨论模型设计、结果解读、挑战与伦理。

**课程大纲（16 周，每周 2 次课）：**

**第一部分：强化学习与商业决策基础 (Weeks 1-3)**

*   **Week 1: 商业决策智能化与 RL 概览**
    *   Session 1: 课程介绍，评分标准。商业决策的复杂性（动态、不确定性）。AI 与商业智能。为什么需要 RL？（超越监督学习）RL 成功案例简介（游戏 AI 到商业应用）。
    *   Session 2: RL 核心要素详解 (S, A, R, π, V, Q)。**互动练习：** 将不同商业场景（定价、库存、营销、客服）分解为 RL 要素。探索与利用的商业实例。
*   **Week 2: 序贯决策建模 - MDP**
    *   Session 3: 马尔可夫性质的直观理解与商业近似。形式化 MDP (S, A, P, R, γ)。回报与折扣因子（商业含义：短视 vs. 远见）。
    *   Session 4: 策略 π（规则 vs. 学习）。值函数 Vπ 和 Qπ。Bellman 期望方程（直观推导与商业解读：价值 = 即时收益 + 未来预期价值）。 **Python & Gym/Gymnasium 环境介绍与安装指导。**
*   **Week 3: 最优决策与 Bellman 最优方程**
    *   Session 5: 最优值函数 V* 和 Q*。Bellman 最优方程（直观：最优价值 = 选择最佳动作带来的[即时收益 + 未来最优预期价值]）。理解“最优”的含义。
    *   Session 6: **Lab 1 (热身):** 熟悉 Gym/Gymnasium 环境。运行一个简单的随机策略智能体。观察状态、动作、奖励。理解环境交互循环。**练习：** 定义一个简单的商业场景（如单商品库存）为 Gym 环境（概念上或简化代码）。

**第二部分：核心无模型学习算法与实践 (Weeks 4-8)**

*   **Week 4: 蒙特卡洛方法 - 从完整经验中学习**
    *   Session 7: 无模型预测：MC 评估 Vπ 和 Qπ。首次访问/每次访问 MC。
    *   Session 8: **Lab 2:** 在 Gridworld 或 Blackjack 环境中实现/运行 MC 预测。可视化价值函数。讨论 MC 的优缺点（需要完整回合、高方差）。
*   **Week 5: 时序差分学习 - 从不完整经验中学习**
    *   Session 9: TD(0) 预测 Vπ。Bootstraping 思想。TD vs. MC 比较（偏差-方差，在线学习）。
    *   Session 10: **Lab 3:** 在 Gridworld 环境中实现/运行 TD(0) 预测。对比 MC 和 TD 的收敛速度和结果。
*   **Week 6: 到策略控制 - SARSA**
    *   Session 11: 从预测到控制。On-Policy 思想。SARSA 算法详解 (S,A,R,S',A')。ε-greedy 探索策略。
    *   Session 12: **Lab 4:** 在 Gridworld 或 CliffWalking 环境中实现/运行 SARSA。观察学习过程和最终策略。分析 ε 对探索和最终性能的影响。
*   **Week 7: 离策略控制 - Q-Learning (重点)**
    *   Session 13: Off-Policy 思想。Q-Learning 算法详解 (S,A,R,S', max_a Q)。与 SARSA 的关键区别。
    *   Session 14: **Lab 5:** 在 Gridworld 或 CliffWalking 环境中实现/运行 Q-Learning。与 SARSA 对比策略和收敛性。讨论 Off-Policy 的优势（可以利用历史/他人数据）。
*   **Week 8: Q-Learning 应用讨论与中期回顾**
    *   Session 15: **案例模拟/讨论：** 使用简化的 Q-Learning 模型模拟简单动态定价或资源分配问题。讨论状态、动作、奖励设计的影响。局限性分析。
    *   Session 16: **中期复习 (Midterm Review):** 涵盖 MDP, Bellman, MC, TD, SARSA, Q-Learning 的核心概念和区别。准备 Midterm Quiz/Exam (可能是概念题+简单模拟结果分析)。

**第三部分：应对复杂性 - 函数逼近与深度强化学习 (Weeks 9-12)**

*   **Week 9: 函数逼近入门**
    *   Session 17: 为何需要函数逼近（状态/动作空间巨大或连续）。基本思想：V(s, w), Q(s, a, w)。线性函数逼近概念。
    *   Session 18: **概念 Lab/演示:** 展示如何用简单的线性模型或查找表无法处理稍复杂问题（如 CartPole 的连续状态）。**引入 Stable Baselines3 库：** 介绍其用途和基本用法。
*   **Week 10: 深度 Q 网络 (DQN)**
    *   Session 19: DQN 核心思想：用神经网络逼近 Q 函数。经验回放 (Experience Replay) 与 目标网络 (Target Network) 的作用（直观解释）。
    *   Session 20: **Lab 6:** 使用 Stable Baselines3 运行 DQN 算法解决 CartPole 问题。学习如何设置超参数、训练模型、可视化训练过程（奖励曲线）、评估训练好的模型。**重点是使用和理解，非实现。**
*   **Week 11: 策略梯度方法**
    *   Session 21: 直接学习策略 π(a|s, θ)。策略梯度定理（直观）。REINFORCE 算法思想。基线 (Baseline) 的作用（减小方差）。
    *   Session 22: **概念讨论:** PG 方法的优势（连续动作空间）和劣势（高方差、可能收敛慢）。**引出 Actor-Critic。**
*   **Week 12: Actor-Critic 方法**
    *   Session 23: Actor-Critic 框架：策略 (Actor) + 价值评估 (Critic)。优势函数 A(s, a)。A2C/A3C 概念。
    *   Session 24: **Lab 7:** 使用 Stable Baselines3 运行 A2C 算法解决 CartPole 或 Pendulum (连续动作) 问题。与 DQN 对比训练过程和结果。**再次强调使用和理解。**

**第四部分：商业应用、挑战与展望 (Weeks 13-16)**

*   **Week 13: 商业案例分析 1 - 动态定价/资源优化**
    *   Session 25: **深度案例分析:** 网约车/酒店/广告位的动态定价。MDP 定义 (状态特征、动作空间、奖励设计 - 考虑长期影响)。数据需求。可选算法 (DQN/PG/AC)。
    *   Session 26: **模拟与讨论:** 使用提供的简化代码/工具模拟动态定价场景。调整参数（如探索率、学习率），观察对利润和市场占有率的影响。讨论实施挑战（冷启动、多智能体竞争）。
*   **Week 14: 商业案例分析 2 - 个性化推荐/营销**
    *   Session 27: **深度案例分析:** 电商/新闻/音乐推荐系统。MDP 定义 (用户状态表示、推荐动作、奖励设计 - 点击率 vs. 用户满意度 vs. LTV)。探索（新内容）与利用（已知偏好）。
    *   Session 28: **讨论与伦理思辨:** 推荐系统中的偏差（过滤气泡）、公平性问题。RL 如何加剧或缓解这些问题？数据隐私考量。
*   **Week 15: 实践挑战、伦理规范与项目指导**
    *   Session 29: RL 落地挑战总结：数据获取与质量、模拟环境构建 (Sim-to-Real)、奖励函数设计的艺术与陷阱、安全性与鲁棒性测试、部署与维护。
    *   Session 30: 负责任的 AI 与 RL 伦理框架。公平性、透明度、可解释性、问责制。**期末项目选题指导与 Q&A。**
*   **Week 16: 课程总结与未来展望 / 项目展示**
    *   Session 31: 课程核心内容回顾。RL 与其他 AI/数据科学技术（监督学习、优化）的结合。前沿方向（Offline RL, Multi-Agent RL 在商业中的潜力）。
    *   Session 32: **期末项目展示 (Final Project Presentations)** 或 **期末考试 (Final Exam)**。

**评估方式（示例，可调整权重）：**
*   **编程实验与 Lab 报告 (7-8 次, 占 35-40%):** 提交运行成功的代码（或修改部分）、结果截图/可视化、关键参数设置说明、简短的结果分析与讨论。**允许使用 AI 辅助，但报告需体现独立思考和理解。**
*   **案例分析与讨论参与 (占 10-15%):** 课堂发言、小组讨论贡献、简短案例分析报告。
*   **中期测试 (Midterm Quiz/Exam, 占 15-20%):** 覆盖前 8 周核心概念和算法理解。
*   **期末项目 (Final Project, 占 30-35%):**
    *   **选题方向：**
        1.  **模拟实验与分析：** 选择一个商业问题（简化版），使用 Stable Baselines3 或提供的框架进行 RL 实验，深入分析结果，提出见解。
        2.  **应用方案设计：** 针对一个具体商业场景，设计一套完整的 RL 解决方案（定义 MDP、选择算法、数据需求、挑战与应对、伦理考量）。
        3.  **文献综述与批判性分析：** 调研 RL 在某一特定商业领域（金融、营销、运营等）的应用现状、成功案例、失败教训和未来趋势。
    *   **形式：** 项目报告 + （可选）简短展示。
