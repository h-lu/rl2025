---
title: "Week 11 - 教师指导手册"
subtitle: "策略梯度方法 (Policy Gradient Methods)"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解基于价值 (Value-Based) RL 方法的局限性（连续动作空间、随机策略）。
    *   学生掌握策略梯度 (Policy Gradient, PG) 方法的核心思想：直接参数化和优化策略 π(a|s, θ)。
    *   学生理解策略梯度定理的基本形式和直观含义（增加好动作概率，降低坏动作概率）。
    *   学生掌握 REINFORCE 算法（蒙特卡洛策略梯度）的基本流程和更新规则。
    *   学生理解 REINFORCE 算法高方差的缺点。
    *   学生理解引入基线 (Baseline) 的目的（减小方差）和原理（不改变梯度期望）。
    *   学生了解优势函数 (Advantage Function) Aπ(s, a) 的概念。
*   **次要目标:**
    *   区分基于价值和基于策略两类 RL 方法。
    *   为下周学习 Actor-Critic 方法（使用基线）打下基础。
    *   培养学生对不同 RL 范式优缺点的分析能力。

# 重点概念 (Key Concepts)

*   基于价值 (Value-Based) vs. 基于策略 (Policy-Based) RL
*   策略梯度 (Policy Gradient, PG) 方法
*   参数化策略 π(a|s, **θ**)
*   性能指标 J(**θ**)
*   策略梯度定理: ∇J(**θ**) = E[∇logπ * Q] 或 E[∇logπ * G]
*   Score Function: ∇logπ(a|s, θ)
*   REINFORCE 算法 (蒙特卡洛策略梯度)
*   高方差 (High Variance) 问题
*   基线 (Baseline) b(s)
*   优势函数 (Advantage Function) Aπ(s, a) = Qπ(s, a) - Vπ(s)
*   使用基线减小方差

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 21 (约 90 分钟): 策略梯度基础**
    *   回顾基于价值的方法 (DQN) 及其局限性 (15 分钟): 特别是连续动作空间和随机策略问题。
    *   策略梯度核心思想 (20-25 分钟): **重点环节**。引入直接优化策略 π(a|s, θ) 的概念。讲解目标是最大化性能 J(θ)，优化方式是梯度上升。
    *   策略梯度定理 (25-30 分钟): **重点概念**。展示定理公式（使用 Q 或 G 的形式），**不必强求数学推导**，重在**直观解释**：∇logπ (动作可能性变化方向) * Q/G (动作好坏程度)。强调“增加好动作概率，降低坏动作概率”。
    *   REINFORCE 算法 (15-20 分钟): 讲解其作为策略梯度定理的直接应用（使用蒙特卡洛回报 G_t）。介绍算法流程和伪代码。
    *   Q&A (5 分钟)
*   **Session 22 (约 90 分钟): REINFORCE 缺点与基线**
    *   回顾 REINFORCE 算法 (10 分钟)。
    *   讨论 REINFORCE 的高方差问题 (15-20 分钟): **重点环节**。解释为何 G_t 方差大，导致梯度估计不稳定、收敛慢。
    *   引入基线 (Baseline) (25-30 分钟): **重点概念**。讲解从 G_t 或 Q 中减去基线 b(s) 的动机（减小方差）和原理（不改变梯度期望）。
    *   优势函数 (Advantage Function) (15 分钟): 介绍 Aπ = Qπ - Vπ 作为一种重要的基线选择。解释其含义（动作比平均好多少）。
    *   策略梯度方法优劣势总结 (10 分钟): 总结 PG 方法的优点（连续动作、随机策略）和缺点（高方差、样本效率低）。
    *   引出 Actor-Critic (5 分钟): 提出需要估计基线 Vπ 的问题，自然过渡到下周的 Actor-Critic。
    *   Q&A (5 分钟)

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 21

*   **引入 PG:**
    *   **提问:** “DQN 如何处理油门踩踏力度（连续动作）这样的问题？” (无法直接处理 `max_a`)。“如果最优策略就是随机出拳（石头剪刀布），DQN 能学到吗？” (困难，因为 `argmax` 是确定性的)。引出需要直接学习策略的需求。
*   **策略梯度定理:**
    *   **直观解释:** “∇logπ 告诉我们，稍微调整参数 θ，让动作 A 在状态 S 下出现的概率是增加还是减少。Q 或 G 告诉我们这个动作 A 到底好不好。如果动作好 (Q/G 大)，我们就希望它出现的概率增加；如果动作差 (Q/G 小或负)，就希望它概率减小。梯度上升就是沿着这个‘增加好动作概率，降低坏动作概率’的方向调整 θ。”
    *   **简化:** 可以暂时忽略期望 E，只看单一样本的更新方向：θ 的调整方向 ≈ ∇logπ(A|S, θ) * Q(S, A)。
*   **REINFORCE:**
    *   **与 MC 联系:** 强调 REINFORCE 使用的是蒙特卡洛方法估计 Q (即 G_t)。
    *   **伪代码:** 讲解时突出回合生成、G_t 计算、梯度计算和参数更新这几个步骤。

## Session 22

*   **高方差讨论:**
    *   **例子:** “假设一个回合很长，中间某个动作 A_t 其实很好，但后面运气不好遇到随机事件导致最终回报 G_t 很低。REINFORCE 会错误地认为 A_t 是个坏动作，降低它的概率。反之亦然。这种随机性导致梯度估计波动很大。”
*   **基线:**
    *   **原理:** 重点解释为什么减去 b(s) 不影响期望梯度（数学推导可选，直观解释更重要：基线不依赖于动作 a，对所有 a 的梯度贡献求和为零）。
    *   **效果:** 强调减小了乘积项 `(G_t - b(S_t))` 的方差。如果 G_t 普遍很高，减去一个平均值 Vπ(S_t) 可以让一些“相对较差”的高回报动作得到负的梯度信号，反之亦然。
*   **优势函数:**
    *   **直观含义:** “动作 A 比在状态 S 下随便乱走（平均策略）要好多少？”
*   **引出 Actor-Critic:**
    *   **提问:** “我们用 Vπ 作为基线很好，但 Vπ 本身怎么得到呢？我们不是无模型吗？” → 需要一个模型来学习 Vπ → Critic。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 策略梯度定理的推导复杂吗？**
    *   A: 完整的数学推导涉及一些微积分和期望的技巧，我们课程不要求掌握。关键是理解定理给出的结果以及它的直观含义：梯度方向由 Score Function (∇logπ) 和动作价值 (Q 或 G) 共同决定。
*   **Q: log π 里面的 log 有什么特别的作用？**
    *   A: 使用 log 主要有数学上的便利：1) ∇logπ(a|s) = ∇π(a|s) / π(a|s)，这个形式在推导和计算中更方便。2) 对概率取 log 可以将乘积转化为求和，并且数值稳定性更好。
*   **Q: REINFORCE 算法看起来很简单，为什么实际中用得不多？**
    *   A: 主要就是因为它的**高方差**问题。梯度估计噪声太大，导致训练非常不稳定，收敛很慢，需要大量样本。实际应用中几乎总是会使用带基线的版本，或者更进一步使用 Actor-Critic 方法。
*   **Q: 基线 b(s) 可以是任意不依赖于动作 a 的函数吗？**
    *   A: 理论上是的，只要不依赖于 a，减去它就不会引入偏差。但**好的基线**应该能够有效减小方差。Vπ(s) 是一个理论上很好的选择，因为它就是 Qπ(s, a) 对动作 a 求期望的平均值。使用 Vπ 作为基线，实际上就是在使用优势函数 Aπ。
*   **Q: 策略梯度方法和基于价值的方法哪个更好？**
    *   A: 两者各有优劣，适用于不同类型的问题。
        *   **基于价值 (DQN):** 样本效率通常更高（尤其 Off-Policy 时），在离散动作空间问题上表现稳定。但难以处理连续动作，可能学到次优确定性策略。
        *   **基于策略 (PG, Actor-Critic):** 可以自然处理连续动作空间，可以学习随机策略。但通常样本效率较低，方差较大（需要技巧缓解）。
    *   现代很多先进算法（如 PPO, SAC）实际上是两者的结合（Actor-Critic 框架）。

# 与后续课程的联系 (Connections to Future Topics)

*   策略梯度和基线的概念是理解下周 **Actor-Critic** 方法的基础。Actor-Critic 就是一种带函数逼近基线的策略梯度方法。
*   策略梯度方法是处理**连续动作空间**问题的主要途径，这将在后续的案例分析和算法（如 PPO, SAC - 如果涉及）中体现。

# 教师准备建议 (Preparation Suggestions)

*   准备好解释基于价值方法局限性的例子（连续动作、随机策略）。
*   清晰地阐述策略梯度定理的直观含义。
*   准备好 REINFORCE 算法的伪代码讲解。
*   用形象的例子解释高方差问题以及基线的作用。
*   （可选）准备一个简单的数学例子说明减去基线不改变梯度期望。
*   清晰地引出 Actor-Critic 的动机。