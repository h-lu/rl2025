---
title: "Week 1: 商业决策智能化与强化学习概览"
---

# 课程介绍与商业决策的挑战

## 欢迎与课程概览

欢迎来到《商业决策的智能优化：强化学习方法与应用》！

*   **课程目标:** (回顾大纲中的课程目标 1-7)
*   **面向对象:** 经济管理学院大三学生
*   **先修要求:** 概率统计、经济/管理基础、基本 Python 了解 (AI 辅助可用)
*   **教学方式:** 理论讲授 (40%), 编程实验 (40%), 案例讨论 (20%)
*   **评估方式:** (回顾大纲中的评估方式)
    *   编程实验与 Lab 报告 (35-40%)
    *   案例分析与讨论参与 (10-15%)
    *   中期测试 (15-20%)
    *   期末项目 (30-35%)

::: {.callout-note title="评分标准"}
详细的评分细则将在后续说明。请注意 Lab 报告的要求，即使使用 AI 辅助，也需要体现独立思考和理解。
:::

## 商业决策的复杂性

传统的商业决策方法往往面临挑战：

*   **动态性 (Dynamics):** 市场环境、客户偏好、竞争对手策略不断变化。今天的最优决策明天可能不再适用。
*   **不确定性 (Uncertainty):** 决策结果往往受到随机因素的影响（如供应链中断、突发事件、消费者情绪波动）。
*   **延迟反馈 (Delayed Feedback):** 很多决策（如长期投资、品牌建设）的效果需要很长时间才能显现，难以快速评估和调整。
*   **大规模与高维度 (Large Scale & High Dimension):** 现代商业涉及海量数据和众多决策变量（如管理数千种商品的库存、对百万级用户进行个性化营销）。

::: {.callout-tip title="思考"}
你能想到哪些具体的商业决策场景，同时具备动态性、不确定性和延迟反馈的特点？
:::

## 人工智能与商业智能 (AI & BI)

*   **商业智能 (BI):** 侧重于**描述性分析 (Descriptive Analytics)** 和**诊断性分析 (Diagnostic Analytics)**。利用历史数据理解发生了什么 (What happened?) 以及为什么发生 (Why did it happen?)。常用工具包括报表、仪表盘、数据可视化。
*   **人工智能 (AI):** 涵盖更广泛的技术，包括**预测性分析 (Predictive Analytics)** (预测未来会发生什么 - What will happen?) 和**处方性分析 (Prescriptive Analytics)** (应该采取什么行动 - What should we do?)。机器学习是 AI 的核心组成部分。

## 为何需要强化学习 (RL)？

监督学习 (Supervised Learning) 在许多领域取得了巨大成功（如图像识别、语音识别），它依赖于带有明确标签的数据 (输入 -> 正确输出)。

然而，许多商业决策问题缺乏明确的“正确答案”标签：

*   **没有唯一的“最优”定价:** 最优价格取决于市场反应、竞争对手行为等动态因素。
*   **没有完美的营销策略:** 效果依赖于用户反馈和长期影响。
*   **序贯决策 (Sequential Decisions):** 决策不是一次性的，而是一系列相互影响的决策。当前决策不仅影响即时收益，更影响未来的状态和可选动作。

**强化学习 (Reinforcement Learning, RL)** 提供了一种不同的范式：

*   **通过与环境交互学习:** 智能体 (Agent) 在环境 (Environment) 中采取行动 (Action)，观察结果 (State) 和奖励 (Reward)，并据此调整策略 (Policy) 以最大化长期累积奖励。
*   **关注长期目标:** RL 不仅仅追求即时奖励，而是学习能够带来最大化未来总回报的策略。
*   **试错学习 (Trial-and-Error):** 智能体通过尝试不同的行动来发现哪些行动能带来好的结果。

::: {.callout-important title="RL vs. 监督学习"}
*   **监督学习:** 从“老师”提供的标签中学习 (Learn from labels)。
*   **强化学习:** 从与环境交互的经验中学习 (Learn from experience/interaction)。
:::

## RL 成功案例简介

*   **游戏 AI:** AlphaGo (围棋), AlphaStar (星际争霸), OpenAI Five (Dota 2) - 超越人类水平。
*   **机器人控制:** 学习复杂的抓取、行走任务。
*   **推荐系统:** 优化长期用户参与度和满意度，而不仅仅是短期点击率。
*   **动态定价:** 根据供需关系实时调整价格（网约车、酒店）。
*   **资源优化:** 数据中心能源优化、网络流量调度。
*   **金融交易:** (虽然挑战重重) 尝试制定交易策略。

# 强化学习核心要素

理解 RL 的基本构成模块至关重要。

*   **智能体 (Agent):** 学习者和决策者。它可以是你的定价算法、库存管理系统、推荐引擎等。
*   **环境 (Environment):** 智能体交互的外部世界。它包含了除智能体之外的一切。例如，市场、客户群体、供应链系统。
*   **状态 (State, $S$):** 对环境当前状况的描述。智能体根据状态来决定下一步行动。
    *   *例子 (定价):* 当前库存水平、竞争对手价格、近期销售趋势、时间（如季节、节假日）。
    *   *例子 (库存):* 当前各种商品的库存量、预测的需求、在途库存。
*   **动作 (Action, $A$):** 智能体可以采取的操作。
    *   *例子 (定价):* 提高价格 5%，降低价格 10%，保持不变。
    *   *例子 (库存):* 订购 100 单位 A 商品，订购 50 单位 B 商品，不订购。
*   **奖励 (Reward, $R$):** 环境对智能体在某个状态下采取某个动作后给出的即时反馈信号。它定义了智能体的目标。奖励可以是正面的（收益、利润、用户满意度）或负面的（成本、损失、客户流失）。
    *   *例子 (定价):* 该动作带来的即时销售额或利润。
    *   *例子 (库存):* 满足需求的收益 - 库存持有成本 - 缺货损失。
*   **策略 (Policy, $\pi$):** 智能体的行为方式，即从状态到动作的映射。它定义了智能体在特定状态下会选择哪个（或哪些）动作。
    *   *确定性策略 (Deterministic):* $\pi(s) = a$ (在状态 $s$ 下，总是选择动作 $a$)
    *   *随机性策略 (Stochastic):* $\pi(a|s) = P(A=a | S=s)$ (在状态 $s$ 下，选择动作 $a$ 的概率)
*   **值函数 (Value Function, $V$):** 评估一个状态（或状态-动作对）有多好。它表示从该状态开始，遵循特定策略 $\pi$，预期未来能获得的累积奖励。
    *   *状态值函数 $V_{\pi}(s)$:* 从状态 $s$ 开始，遵循策略 $\pi$ 的预期总回报。
*   **动作值函数 (Action-Value Function, $Q$):** 也称为 Q 函数。
    *   *$Q_{\pi}(s, a)$:* 在状态 $s$ 下，采取动作 $a$，然后继续遵循策略 $\pi$ 的预期总回报。$Q$ 函数直接关联了具体动作的好坏，对于决策至关重要。

::: {.callout-note title="核心循环"}
1.  智能体观察当前状态 $S_t$。
2.  智能体根据策略 $\pi$ 选择动作 $A_t$。
3.  环境接收动作 $A_t$，转移到新状态 $S_{t+1}$，并给出奖励 $R_{t+1}$。
4.  智能体利用 $(S_t, A_t, R_{t+1}, S_{t+1})$ 这个经验来学习和改进策略 $\pi$。
:::

## 互动练习：分解商业场景

请尝试将以下商业场景分解为 RL 的核心要素 ($S$, $A$, $R$)。思考可能的策略 $\pi$ 和值函数 $V$/$Q$ 的含义。

1.  **动态定价 (Dynamic Pricing):** 单一易腐烂商品（如机票、酒店房间），需要在到期前售出。
    *   $S$: ? (e.g., 剩余时间，剩余库存，近期预订速率...)
    *   $A$: ? (e.g., 设定具体价格，提价/降价幅度...)
    *   $R$: ? (e.g., 即时销售收入，一天结束时的总收入...)
    *   $\pi$: ? (e.g., 如果剩余时间少且库存多，则大幅降价...)
    *   $V$/$Q$: ? (e.g., V(t天, k库存) = 从现在开始到售罄/过期的预期总收入)

2.  **库存管理 (Inventory Management):** 单一商品，需要决定每天订购多少。
    *   $S$: ? (e.g., 当前库存水平，预测的未来几天需求...)
    *   $A$: ? (e.g., 订购数量...)
    *   $R$: ? (e.g., 销售收入 - 订购成本 - 库存持有成本 - 缺货惩罚...)
    *   $\pi$: ? (e.g., 如果库存低于阈值，则订购一定量...)
    *   $V$/$Q$: ? (e.g., Q(k库存, d订购量) = 采取订购动作后的长期预期净利润)

3.  **个性化营销 (Personalized Marketing):** 向网站访客推送优惠券。
    *   $S$: ? (e.g., 用户画像 [浏览历史、购买记录、人口统计学信息], 当前访问页面...)
    *   A: ? (e.g., 推送 A 类优惠券, 推送 B 类优惠券, 不推送...)
    *   $R$: ? (e.g., 用户是否点击/使用优惠券, 购买转化金额, 长期用户价值 LTV 的变化...)
    *   $\pi$: ? (e.g., 对高价值历史用户推送高折扣券...)
    *   $V$/$Q$: ? (e.g., Q(用户u, 优惠券c) = 向用户 u 推送优惠券 c 的预期长期价值贡献)

4.  **智能客服路由 (Intelligent Customer Service Routing):** 将来电分配给最合适的客服代表。
    *   $S$: ? (e.g., 客户类型, 问题类型, 可用客服代表的技能/状态...)
    *   $A$: ? (e.g., 分配给代表 1, 分配给代表 2...)
    *   $R$: ? (e.g., 问题解决时长, 客户满意度评分, 是否需要二次呼入...)
    *   $\pi$: ? (e.g., 技术问题分配给技术专家...)
    *   $V$/$Q$: ? (e.g., V(客户类型c, 问题类型p) = 该类电话的最佳处理方式下的预期服务质量指标)

## 探索 (Exploration) vs. 利用 (Exploitation)

这是 RL 中的一个核心权衡：

*   **利用 (Exploitation):** 根据当前已知的最优策略采取行动，以获得当前看来最好的回报。
*   **探索 (Exploration):** 尝试新的、未知的行动，即使它们当前看起来不是最优的，目的是为了收集更多信息，发现可能更好的策略。

**商业实例:**

*   **餐厅:**
    *   *利用:* 只做最受欢迎的招牌菜。
    *   *探索:* 尝试推出新菜品，可能发现新的爆款，但也可能不受欢迎。
*   **广告投放:**
    *   *利用:* 将预算集中投放在已知效果最好的渠道和人群。
    *   *探索:* 分配一部分预算尝试新的广告平台、创意或目标受众。
*   **产品推荐:**
    *   *利用:* 总是推荐用户过去喜欢或购买过的同类商品。
    *   *探索:* 推荐一些用户可能感兴趣但从未接触过的新品类。

::: {.callout-warning title="权衡的重要性"}
*   **过度利用:** 可能陷入局部最优，错失发现更好策略的机会。
*   **过度探索:** 可能浪费过多资源在次优的行动上，导致整体性能不佳。
RL 算法需要有效地平衡探索与利用。
:::

---

**下周预告:** 序贯决策建模 - 马尔可夫决策过程 (MDP)