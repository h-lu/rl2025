---
title: "Week 5 - 教师指导手册"
subtitle: "时序差分学习 - 从不完整经验中学习"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解时序差分 (TD) 学习的核心思想：结合 MC（从经验学习）和 DP（自举 Bootstrapping）。
    *   学生掌握 TD(0) 预测算法的更新规则，理解 TD 目标和 TD 误差的含义。
    *   学生理解自举 (Bootstrapping) 的概念及其带来的优缺点（在线学习 vs. 引入偏差）。
    *   学生能够清晰地比较 TD(0) 和 MC 方法的关键区别（更新时机、更新目标、偏差-方差权衡、适用任务等）。
    *   学生能够运行 Lab 3 代码（Gridworld 环境），使用 TD(0) 算法进行策略评估。
    *   学生能够通过实验对比 TD(0) 和 MC 的收敛速度和结果。
*   **次要目标:**
    *   加深对偏差-方差权衡的理解。
    *   培养学生实现、运行和对比不同 RL 算法的能力。
    *   为下周学习 TD 控制算法（SARSA）打下基础。

# 重点概念 (Key Concepts)

*   时序差分 (Temporal-Difference, TD) 学习
*   自举 (Bootstrapping)
*   TD(0) 预测算法
*   TD 目标 (TD Target): R + γV(S')
*   TD 误差 (TD Error) δ: R + γV(S') - V(S)
*   学习率 (Learning Rate) α
*   TD(0) vs. MC 对比：
    *   更新时机 (Online vs. Offline/End-of-Episode)
    *   更新目标 (TD Target vs. Full Return G_t)
    *   偏差-方差权衡 (Bias-Variance Tradeoff)
    *   适用任务 (Continuing vs. Episodic)
*   Gridworld 环境 (作为 Lab 示例)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 9 (约 90 分钟):**
    *   回顾 MC 方法及其局限性 (10-15 分钟): 特别是高方差和需等待回合结束的问题。
    *   TD 学习核心思想 (25-30 分钟): **重点环节**。引入 TD 概念，强调其结合 MC 和 DP 的特点。类比“朝令夕改”：根据下一步的实际情况和对未来的**当前估计**来修正**当前**的看法，而不是等最终结果出来再说。
    *   TD(0) 预测算法 (25-30 分钟): 详细讲解更新规则 V(S) ← V(S) + α [R + γV(S') - V(S)]。解释 TD 目标和 TD 误差。
    *   自举 (Bootstrapping) 概念 (10 分钟): 解释其含义（用估计更新估计）及其优缺点（在线学习 vs. 偏差）。
    *   Q&A (5 分钟)
*   **Session 10 (约 90 分钟): Lab 3**
    *   回顾 TD(0) 预测 (5 分钟)。
    *   TD(0) vs. MC 对比 (20-25 分钟): **重点环节**。系统性地比较两者在更新时机、目标、偏差方差、适用性等方面的差异。强调偏差-方差权衡。
    *   Lab 3 目标与环境介绍 (10 分钟): 介绍 Gridworld 环境（如果学生上周没完成，需要快速过一下或提供现成代码），明确 Lab 任务是实现 TD(0) 并与 MC 对比。
    *   指导运行 Lab 3 代码 (35-40 分钟): **重点环节**。带领学生实现 TD(0) 的核心更新逻辑。指导学生设置实验，运行 TD(0) 和 MC 相同的回合数，并可视化价值函数（热力图）。
    *   结果对比与讨论 (10-15 分钟): 引导学生观察并比较两种方法的收敛速度（哪个更快达到稳定价值？）和最终结果。讨论实验中观察到的现象是否与理论预期一致。
    *   Lab 提交要求说明 & Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 9

*   **TD 核心思想引入:**
    *   **类比:**
        *   天气预报：MC 像是在年底总结今年天气如何；TD 像是每天根据第二天的实际天气和对未来的预报来修正对今天天气的长期判断。
        *   开车导航：MC 是走完全程才知道哪条路好；TD 是每经过一个路口，根据下一个路口的实时路况和导航对后续路程的估计，来更新对当前路段选择的评价。
    *   **强调“差分”:** TD 利用的是**连续时间步**之间价值估计的**差异** (Temporal Difference) 来学习。
*   **TD(0) 更新规则:**
    *   **拆解:** V(S) ← (1-α)V(S) + α * [R + γV(S')]。可以看作是向 TD 目标做了一个小的移动。
    *   **与 Bellman 期望方程联系:** TD 目标 R + γV(S') 是对 Bellman 期望方程右侧 E[R + γV(S')] 的一个**采样估计**。TD 学习可以看作是用采样的方式来“求解”Bellman 方程。
*   **自举:**
    *   **形象解释:** “自己把自己拎起来”。用一个可能不准的估计 (V(S')) 去更新另一个估计 (V(S))。
    *   **优缺点:** 优点是在线学习，缺点是如果初始估计错得离谱，错误可能会传播。

## Session 10 (Lab 3)

*   **TD vs. MC 对比 (重点):**
    *   **表格总结:** 用表格清晰列出两者的关键区别。
    *   **偏差-方差图示 (可选):** 可以画一个靶子图，MC 像是一堆散布广但中心正确的弹孔（无偏，高方差），TD 像是一堆集中但可能偏离靶心的弹孔（有偏，低方差）。
    *   **适用性讨论:** 为什么 TD 更适用于持续性任务或长回合任务？
*   **Lab 代码指导:**
    *   **Gridworld 环境:** 确保学生有一个可用的 Gridworld 环境（自己实现或使用提供的）。
    *   **TD(0) 实现:** 重点在于 `step` 循环内部的 TD 误差计算和 V(S) 更新。
    *   **MC 实现:** 学生可以复用上周的部分逻辑，或者提供一个简单的 MC 实现用于对比。
    *   **实验设置:** 强调要控制变量，让 TD 和 MC 运行相同的回合数，使用相同的策略（随机策略）和 γ。学习率 α 是 TD 特有的。
    *   **可视化对比:** 将 TD 和 MC 得到的最终价值函数热力图放在一起比较。引导学生观察哪个更快收敛（例如，比较运行 1000 回合和 10000 回合后的价值图）。
*   **结果讨论:**
    *   **预期结果:** 通常 TD(0) 会比 MC 收敛更快，尤其是在早期回合。最终结果可能相似，但也可能因为 TD 的偏差而略有不同。
    *   **引导思考:** “为什么 TD 通常更快？” (低方差，每步都更新) “为什么 TD 的结果可能有偏差？” (依赖于 V(S') 的估计)。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: TD 目标 R + γV(S') 里的 V(S') 是真实的 Vπ(S') 吗？**
    *   A: 不是。它是在当前时间步 t，我们对 Vπ(S') 的**估计值** V(S')。因为我们不知道真实的 Vπ，所以只能用当前的估计来进行自举。这就是 TD 偏差的来源。随着学习的进行，V(S') 会越来越接近真实的 Vπ(S')，TD 目标的估计也会越来越准。
*   **Q: 学习率 α 如何选择？**
    *   A: 学习率 α 是一个重要的超参数。它控制了学习的速度和稳定性。
        *   α 太大：更新步子太大，可能导致 V(S) 震荡甚至发散。
        *   α 太小：学习太慢。
    *   通常选择一个较小的值（如 0.1, 0.01, 0.001）。有时也会使用衰减的学习率（早期大，后期小）。选择合适的 α 通常需要实验调整。
*   **Q: TD(0) 看起来比 MC 好，那 MC 还有用吗？**
    *   A: TD(0) 在很多方面（尤其收敛速度和适用性）优于 MC，是更常用的基础算法。但 MC 也有其价值：1) 它是无偏的，如果需要精确的无偏估计，MC 是首选。2) MC 对函数逼近的适应性更好（TD+非线性函数逼近+Off-policy 可能不稳定）。3) MC 的思想是策略梯度等其他算法的基础。理解 MC 有助于理解更广泛的 RL 领域。
*   **Q: Lab 里的 Gridworld 代码哪里找/怎么写？**
    *   A: 可以鼓励学生尝试自己实现一个简单的 Gridworld 类，遵循 Gym API（`__init__`, `reset`, `step`）。或者教师可以提供一个参考实现。网上也有很多开源的 Gridworld 代码库可以参考。重点是理解环境的交互逻辑。

# 与后续课程的联系 (Connections to Future Topics)

*   TD(0) 预测是理解 **TD 控制算法**（SARSA, Q-Learning）的基础。SARSA 和 Q-Learning 都是将 TD(0) 的思想应用于学习 Q 函数，并结合策略改进。
*   TD 误差 δ 的概念在 **Actor-Critic** 方法中也非常重要，通常用作优势函数的估计来指导 Actor 的更新。
*   TD 学习是 **DQN** 等深度强化学习算法的核心更新机制。

# 教师准备建议 (Preparation Suggestions)

*   准备好 TD 核心思想的类比和解释。
*   清晰地梳理 TD(0) 的更新公式和伪代码。
*   准备好 TD vs. MC 的对比表格和解释。
*   确保有一个可用的 Gridworld 环境代码，并准备好 Lab 3 的指导代码或框架。
*   预先运行 TD 和 MC 对比实验，准备好展示和讨论预期结果。