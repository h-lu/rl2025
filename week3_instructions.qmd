---
title: "Week 3 - 教师指导手册"
subtitle: "最优决策与 Bellman 最优方程"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解最优策略 (π\*)、最优状态值函数 (V\*) 和最优动作值函数 (Q\*) 的概念。
    *   学生理解 Bellman 最优方程 (for V\* and Q\*) 的形式和含义，并能区分它与 Bellman 期望方程。
    *   学生理解“最优”的含义，认识到最优策略不一定是单步最优。
    *   学生熟悉 Gym/Gymnasium 环境的基本交互（`reset`, `step`, `render`）。
    *   学生能够运行简单的随机策略智能体，并观察 S, A, R。
    *   学生尝试将简单商业场景概念化为 Gym 环境要素。
*   **次要目标:**
    *   强调 Q\* 对于直接导出最优策略的重要性。
    *   初步认识到奖励函数设计对“最优”结果的影响。
    *   确保学生为后续更复杂的 Lab 做好环境和基础操作准备。

# 重点概念 (Key Concepts)

*   最优策略 (Optimal Policy) π\*
*   最优状态值函数 (Optimal State-Value Function) V\*(s)
*   最优动作值函数 (Optimal Action-Value Function) Q\*(s, a)
*   Bellman 最优方程 (Bellman Optimality Equation) for V\* and Q\*
*   与 Bellman 期望方程的区别 (max 操作)
*   贪心策略 (Greedy Policy) w.r.t Q\*
*   Gym/Gymnasium 环境交互: `reset`, `step`, `render`, `observation_space`, `action_space`
*   回合 (Episode), 终止状态 (Terminated), 截断状态 (Truncated)
*   将商业场景映射到 Gym 环境要素 (概念练习)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 5 (约 90 分钟):**
    *   回顾 MDP 和 Bellman 期望方程 (10 分钟)。
    *   引出最优性概念 (5 分钟): 目标是找到最好的策略。
    *   最优价值函数 V\* 和 Q\* (25-30 分钟): 定义、含义，重点强调 Q\* 与最优策略 (贪心) 的直接关系。解释 V\* 和 Q\* 之间的联系。
    *   Bellman 最优方程 (30-35 分钟): **重点环节**。推导（直观）V\* 和 Q\* 的最优方程，与期望方程对比，强调 `max` 操作的含义。解释其非线性特性。
    *   理解“最优”的含义 (10 分钟): 讨论最优不等于单步最优，依赖于 MDP 定义，强调奖励设计的关键性。
    *   Q&A (5 分钟)
*   **Session 6 (约 90 分钟): Lab 1 (热身)**
    *   Lab 目标与步骤介绍 (10 分钟)。
    *   环境安装检查与答疑 (15-20 分钟): 确保学生都能运行基本 Gym 脚本。**这是关键步骤，务必保证大部分学生环境配置成功。**
    *   指导运行随机智能体 (20 分钟): 带领学生运行 CartPole 或其他简单环境代码，解释 `reset`, `step`, `render`，观察 S, A, R, terminated, truncated。鼓励学生尝试修改环境或简单参数。
    *   指导商业场景概念定义练习 (30-35 分钟): **重点练习**。分组讨论或独立完成讲义中的库存管理/广告投放场景的概念设计 (S, A, R, P, γ)。鼓励学生思考状态/动作空间的离散化、奖励的量化。如果时间允许，可以展示或引导学生思考 `SimpleInventoryEnv` 的简化代码框架。
    *   Lab 提交要求说明 & Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 5

*   **最优价值函数:**
    *   **类比:** 在所有可能的开车回家路线（策略）中，V\*(家) 代表了从家出发能达到的最快时间（假设奖励是负的时间）。Q\*(家, 走高速) 代表了从家出发，先选择走高速这条路，然后继续按最优路线走所能达到的最快时间。
    *   **强调 Q\* -> π\*:** 反复强调如果我们知道了 Q\*，最优策略就是简单的贪心选择 `argmax_a Q*(s, a)`。这是后续 Q-Learning 等算法的基础。
*   **Bellman 最优方程:**
    *   **对比期望方程:** 清晰地展示两个方程的结构差异，突出 `max_a` 操作。期望方程是关于某个**固定**策略 π 的，而最优方程是关于**所有**策略中最好的那个 π\* 的。
    *   **直观解释:** V\*(s) = max_a {在状态 s 选择动作 a 能得到的 (即时奖励 + 未来最优价值期望)}。Q\*(s, a) = E [即时奖励 + 在下一个状态采取最优动作的未来价值]。
*   **理解“最优”:**
    *   **例子:** 讲一个“欲速则不达”的例子，说明最优策略可能需要短期牺牲。例如，学习一项新技能（短期投入时间，负奖励），但长期带来更高回报。
    *   **奖励讨论:** 再次强调奖励函数定义了“最优”的目标。如果奖励只考虑短期利润，那么 RL 找到的“最优”策略就是最大化短期利润的策略，即使它损害长期发展。

## Session 6 (Lab 1)

*   **环境检查:** 这是第一次 Lab，环境配置是关键。准备好处理常见安装问题的方案（如 Python 版本、pip 问题、虚拟环境使用）。可以提前发布详细的安装指南。
*   **随机智能体演示:**
    *   逐步执行代码，解释每一步的作用。
    *   让学生观察 `observation` (状态向量)、`action` (0 或 1)、`reward` (+1)、`terminated` (杆倒下时为 True)、`truncated` (达到最大步数时为 True)。
    *   鼓励学生尝试不同的环境 (`MountainCar-v0`, `Acrobot-v1`)，感受不同状态/动作空间和奖励。
*   **商业场景概念定义 (重点):**
    *   **目标:** 不是要求学生写出完美代码，而是练习**将商业问题映射到 MDP 框架**的思维过程。
    *   **引导:**
        *   **状态 S:** 哪些信息是**必要**的？如何处理连续变量（库存量、需求预测）？（引导思考离散化/分箱）
        *   **动作 A:** 有哪些决策选项？如何离散化？（例如，订购量可以简化为“不订”、“少量”、“大量”）
        *   **奖励 R:** 如何量化利润？成本（订购、持有、缺货）如何考虑？（引导思考奖励函数的构成）
        *   **转移 P:** 下一个状态如何依赖当前状态和动作？（描述性说明即可，如“下一天库存 = 当前库存 + 订购量 - 实际需求”）
        *   **折扣 γ:** 这个问题更关注短期还是长期？
    *   **代码框架 (可选):** 如果展示 `SimpleInventoryEnv` 代码，重点解释如何定义 `observation_space`, `action_space`，以及 `step` 函数如何模拟状态转移和奖励计算。**强调这不是要求学生必须写代码，而是理解接口。**
*   **提交要求:** 明确说明需要提交概念设计文档，代码是可选的加分项。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: Bellman 最优方程怎么求解？看起来比期望方程还难。**
    *   A: 你说得对，由于 `max` 操作，Bellman 最优方程是非线性的，通常没有简单的闭式解。经典的求解方法包括**值迭代 (Value Iteration)** 和**策略迭代 (Policy Iteration)**（这些属于动态规划 DP 的范畴，需要知道模型 P 和 R）。但更重要的是，这个方程是后续**无模型控制算法**（如 Q-Learning）的理论基础。Q-Learning 可以看作是一种**采样和逼近**的方式来间接“求解”Bellman 最优方程，而不需要知道 P 和 R。
*   **Q: 为什么不直接学习最优策略 π\*，而要学习最优价值函数 Q\*？**
    *   A: 对于基于价值的方法（如 Q-Learning, DQN），学习 Q\* 通常更容易，因为 Q\* 的学习目标（TD 目标）相对明确。一旦学好了 Q\*，导出最优策略 π\* 就很简单（贪心）。我们后面会学习另一类方法——策略梯度方法，它们确实是直接学习策略 π\* 的参数。两类方法各有优劣。
*   **Q: Lab 1 的商业场景定义，我需要写代码吗？**
    *   A: 主要目标是进行**概念设计**，清晰地定义 S, A, R, P (描述性), γ。提交这部分思考是必须的。提供 Python 代码框架是为了帮助你理解 Gym 环境的接口，如果你能尝试编写简化的代码框架会很有帮助，但这部分是**可选的**。重点是理解如何将问题映射到 MDP。
*   **Q: 运行 Gym 代码时报错/看不到窗口怎么办？**
    *   A: 首先检查环境是否安装成功 (`pip list` 查看 `gymnasium`)。看不到窗口可能是 `render_mode` 设置问题，尝试设置为 `"human"`。如果仍然报错，请记录详细的错误信息，在答疑时间或论坛提问。确保你的系统安装了必要的图形库（有时需要）。

# 与后续课程的联系 (Connections to Future Topics)

*   Bellman 最优方程是理解 Q-Learning 算法更新规则的关键。Q-Learning 的 TD 目标 `R + γ max_a' Q(S', a')` 正是 Bellman 最优方程右侧的采样近似。
*   Lab 1 对 Gym 环境的熟悉是后续所有 Lab 的基础。
*   商业场景的 MDP 建模练习，为后续将 RL 应用于更复杂案例打下基础。

# 教师准备建议 (Preparation Suggestions)

*   准备好 Bellman 最优方程的清晰解释，特别是与期望方程的对比。
*   准备好 Lab 1 的演示代码和步骤说明。
*   预先思考库存管理/广告投放场景的多种 MDP 定义方式，以便引导讨论。
*   准备好应对学生 Gym 环境配置问题的常见解决方案。
*   （可选）准备一个简单的值迭代或策略迭代的例子（如果想简单介绍 DP）。