---
title: "第七周：深入探索DQN——改进、调优与实战"
---

::: {.callout-tip appearance="simple"}
## 本周学习目标

1.  **理解** 标准DQN的局限性，**掌握** 至少两种核心改进方法（如Double DQN, Dueling DQN）的原理与应用场景。
2.  **分析** 不同探索策略（ε-greedy退火, Noisy Networks, Boltzmann）的优缺点，并能**选择** 合适的策略。
3.  **学习** DQN训练过程中的关键超参数调优技巧和奖励设计原则。
4.  **应用** TensorBoard等工具**监控**和**分析**训练过程。
5.  **实践** 将所学改进和调优技巧应用于小组项目（如CartPole），**解决** 遇到的实际问题，**提升** 模型性能。
6.  **了解** Rainbow DQN等前沿进展，拓宽视野。
:::

## 温故知新：为何要改进DQN？

在上周的学习中，我们掌握了标准DQN算法的核心思想及其在解决离散动作空间问题上的优势。然而，标准DQN并非完美，它在实际应用中也暴露出一些局限性。

**思考一下：** 结合你上周的学习和实验经验，标准DQN在哪些方面可能存在不足？（提示：可以从Q值估计的准确性、学习效率、探索方式等方面考虑。）

本周，我们将深入探讨标准DQN的主要局限性，并学习一系列强大的改进方法，为你的DQN武器库增添更多利器。

---

## DQN的核心改进方法

为了克服标准DQN的局限性，研究者们提出了多种有效的改进策略。下面我们来学习其中最核心的几种：

::: {.panel-tabset}
#### Double DQN：告别Q值过高估计

*   **核心问题:** 标准DQN在计算目标Q值时，使用同一个网络既选择最大Q值对应的动作，又评估该动作的Q值 ( $r + \gamma \max_{a'} Q(s', a'; \theta^-)$ )。这容易导致对Q值的**过高估计 (overestimation)**，尤其是在学习初期或存在噪声时，某个被随机高估的Q值更容易被`max`操作选中，导致策略学习产生偏差。

*   **解决方案:** Double DQN巧妙地将**动作选择**和**动作评估**解耦。它使用当前的主网络 $Q(s, a; \theta)$ 来选择下一个状态 $s'$ 的最优动作 $a^* = \arg\max_{a'} Q(s', a'; \theta)$，然后使用目标网络 $Q(s', a; \theta^-)$ 来评估这个选定动作的价值。
    *   **Double DQN 目标:** $r + \gamma Q(s', a^*; \theta^-) = r + \gamma Q(s', \arg\max_{a'} Q(s', a'; \theta); \theta^-)$

*   **代码关键点:** 在计算目标Q值时，需要先用主网络预测下一个状态的所有动作的Q值，找出最大Q值对应的动作索引，然后用目标网络预测下一个状态的所有动作的Q值，并取出该索引对应的Q值用于计算目标。

*   **思考:** Double DQN是否在所有情况下都优于标准DQN？它在哪些场景下能发挥更大优势？

#### Dueling DQN：状态价值与动作优势的分离

*   **思考:** 想象一个场景，无论你向左还是向右移动，最终的奖励可能都差不多，因为当前所处的状态本身就很有价值（或很危险）。这启发我们：状态本身的价值 (Value) 和在特定状态下采取某个动作相对于其他动作的优势 (Advantage) 是不是可以分开考虑？

*   **原理:** Dueling DQN正是基于这种思想。它将Q值函数 $Q(s,a)$ 分解为两部分：
    1.  **状态价值函数 $V(s)$:** 表示处于状态 $s$ 本身的价值，与具体动作无关。
    2.  **动作优势函数 $A(s,a)$:** 表示在状态 $s$ 下，采取动作 $a$ 相对于采取平均动作的好坏程度。

    网络结构上，通常有一个共享的特征提取层，然后分叉为两个流：一个输出标量 $V(s)$，另一个输出每个动作的优势 $A(s,a)$。最后将两者结合得到Q值。为了保证 $V(s)$ 真正学到的是状态价值，并解决 $V$ 和 $A$ 的不可辨识问题 (identifiability issue)，通常采用以下聚合方式：
    *   $Q(s,a) = V(s) + (A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}} A(s,a'))$
    （减去优势函数的均值可以使优势函数的均值为零，让 $V(s)$ 更好地代表状态价值。）

*   **代码关键点:** 需要设计一个包含共享层、价值流分支和优势流分支的网络结构，并根据上述公式在`forward`方法中组合输出。

*   **讨论:** Dueling DQN在哪些类型的环境中可能特别有效？为什么？（提示：考虑那些状态价值比动作选择更重要的环境，或者动作空间很大但很多动作影响相似的环境。）

#### 优先经验回放 (PER)：让学习更聚焦

*   **核心问题:** 标准DQN的经验回放是均匀采样，这意味着所有经验（无论重要与否）被选中的概率都相同。但直觉上，那些带来"惊喜"（即预测误差很大）的经验应该包含更多值得学习的信息。

*   **解决方案:** 优先经验回放 (Prioritized Experience Replay, PER) 根据经验的**重要性**来赋予不同的采样概率，让智能体更频繁地从重要的经验中学习。
    *   **重要性度量:** 通常使用**TD误差 (Temporal Difference error)** 的绝对值 $|\delta_i| = |y_i - Q(s_i, a_i; \theta)|$ 来衡量。TD误差越大，表示当前的Q值预测与目标值差距越大，该经验越"令人惊讶"，学习价值可能越高。
    *   **采样概率:** 经验 \(i\) 被采样的概率 \(P(i)\) 与其优先级 \(p_i\) 成正比。优先级通常基于TD误差计算：$p_i = (|\delta_i| + \epsilon)^\alpha$，其中 $\epsilon$ 是一个很小的正数（防止优先级为0），$\alpha \in [0, 1]$ 是一个超参数，控制优先级的程度（$\alpha=0$ 时退化为均匀采样）。
    *   $ P(i) = \frac{p_i}{\sum_k p_k} $
    *   **偏差修正:** 优先采样会引入偏差，因为重要的样本被重复学习的次数更多。为了修正这种偏差，PER引入了**重要性采样 (Importance Sampling, IS) 权重** $w_i$ 来调整这些样本在梯度更新中的贡献：
        *   $w_i = \left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^\beta = \left(\frac{\sum_k p_k}{N \cdot p_i}\right)^\beta$
        其中 $N$ 是缓冲区大小，$\beta \in [0, 1]$ 是另一个超参数，用于控制权重的大小（通常从一个较小的值开始，在训练过程中逐渐增加到1）。这个权重 $w_i$ 会被用来缩放对应样本的损失。

*   **思考:** PER如何提高样本效率？它在实现上可能引入哪些新的复杂性和计算开销？如何选择合适的 $\alpha$ 和 $\beta$ 值？

*   **选学/挑战:** 为了高效地实现按优先级采样和更新优先级，PER通常使用一种称为**SumTree**的数据结构。尝试理解SumTree的工作原理。

#### 多步学习：看得更远一点

*   **核心问题:** 标准DQN使用单步TD目标 $r_t + \gamma \max_a Q(s_{t+1}, a)$，只利用了一步的真实奖励 $r_t$，而后续的价值完全依赖于估计值 $Q(s_{t+1}, a)$。这可能导致学习信号传播较慢，并且过于依赖可能有偏差的Q值估计。

*   **解决方案:** 多步学习 (Multi-step Learning) 使用未来 $n$ 步的累计折扣奖励和第 $n$ 步之后的Q值估计来构建目标值，看得更"远"。
    *   **n步回报 (n-step return):**
        $R_t^{(n)} = r_{t+1} + \gamma r_{t+2} + \dots + \gamma^{n-1} r_{t+n} + \gamma^n \max_{a'} Q(s_{t+n}, a'; \theta^-)$
        （注意：这里用 $r_{t+1}$ 表示 $s_t, a_t$ 得到的奖励，与之前公式的 $r$ 对应）
    *   这个 $R_t^{(n)}$ 将替代单步TD目标 $r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a'; \theta^-)$ 用于计算TD误差和更新Q网络。

*   **优势:**
    *   更快地传播奖励信息，特别是对于奖励稀疏的环境。
    *   减少了对早期不准确的Q值估计的依赖。

*   **讨论:** 选择n步学习的步数 $n$ 时需要考虑哪些因素？ $n$ 越大越好吗？（提示：考虑偏差和方差的权衡。更大的 $n$ 通常有更低的偏差但更高的方差。）
:::

---

## 探索的艺术：如何更聪明地探索？

探索 (Exploration) 与利用 (Exploitation) 的平衡是强化学习中的经典问题。除了我们熟悉的ε-greedy策略及其退火方法，还有更"聪明"的探索策略。

*   **回顾:** ε-greedy策略是如何工作的？它的主要缺点是什么？（提示：随机性、与状态无关、需要手动调整ε衰减）

*   **思考与对比:**

    *   **Noisy Networks (噪声网络):**
        *   **核心思想:** 不再依赖于外部的随机决策（如ε-greedy），而是直接在网络的**权重**中引入可学习的**参数化噪声**。智能体通过学习噪声的大小来自适应地进行探索。
        *   **实现:** 将网络中的线性层（`nn.Linear`）替换为`NoisyLinear`层。在训练时，层的权重和偏置会加入通过网络学习到的噪声；在评估时，则关闭噪声，使用确定的权重进行决策。
        *   **优势:** 探索是**状态相关的** (因为噪声通过网络传播，受输入状态影响)，并且探索的程度是**自适应学习**的，无需手动设计复杂的ε衰减方案。通常比ε-greedy更有效地探索环境。

    *   **Boltzmann探索 (Softmax Exploration):**
        *   **原理:** 根据当前状态下各个动作的Q值估计，计算一个**概率分布**，然后根据这个概率分布来随机选择动作。Q值越高的动作被选中的概率越大。
        *   **公式:** $P(a|s) = \frac{\exp(Q(s,a)/\tau)}{\sum_{a'}\exp(Q(s,a')/\tau)}$
        *   $\tau$ 称为**温度 (temperature)** 参数。$\tau$ 越大，概率分布越接近均匀分布（探索性越强）；$\tau$ 越小，概率分布越集中在Q值最高的动作上（利用性越强）。通常也需要对 $\tau$ 进行退火。
        *   **与ε-greedy对比:** Boltzmann探索不是完全随机地选择非最优动作，而是根据Q值的相对大小来决定探索的方向，可能比ε-greedy更倾向于探索"看起来有潜力"的次优动作。

*   **为你的项目选择:** 考虑你的CartPole项目或其他正在进行的项目，你会选择哪种探索策略（ε-greedy退火, Noisy Networks, Boltzmann）？说明你的理由。

---

## DQN训练调优与监控技巧

获得良好性能的DQN模型往往需要细致的调优和有效的监控。

*   **网络结构:**
    *   对于简单问题（如CartPole），通常2-3个隐藏层，每层64-256个神经元就足够了。
    *   对于更复杂的视觉输入问题（如Atari游戏），通常使用卷积层提取特征，再接全连接层。
    *   并非越深或越宽的网络越好，需要根据问题复杂度进行选择。
    *   **激活函数:** 隐藏层常用ReLU及其变种（如LeakyReLU），输出层（计算Q值）通常不需要激活函数。

*   **关键超参数:**
    *   **学习率 (Learning Rate):** 常用范围 1e-4 到 1e-3。太大会导致训练不稳定，太小则收敛缓慢。Adam等自适应优化器通常效果较好，也可以配合学习率调度（逐步降低学习率）。
    *   **批次大小 (Batch Size):** 常用范围 32 到 256。更大的批次通常提供更稳定的梯度估计，但也更消耗内存和计算资源。
    *   **经验回放缓冲区大小 (Replay Buffer Size):** 常用范围 1万到100万。缓冲区需要足够大以保证样本多样性，打破数据相关性，但过大可能包含太多陈旧的、不再符合当前策略的经验。
    *   **目标网络更新频率 (Target Network Update Frequency):** 对于硬更新，常用每隔几百到几万个训练步更新一次。更新太频繁可能导致目标不稳定，太慢则可能导致学习滞后。也可以使用**软更新 (Soft Update)**：每次训练步都按一个小的比例 $\tau$ (e.g., 0.001, 0.01) 更新目标网络参数：$\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-$。

*   **奖励设计 (Reward Engineering):**
    *   **奖励塑形 (Reward Shaping):** 设计中间奖励引导智能体学习，但要小心引入不期望的偏差。
    *   **奖励归一化/裁剪 (Reward Normalization/Clipping):** 将奖励缩放到一个固定范围（如[-1, 1]）通常有助于稳定训练，防止Q值爆炸。

*   **使用TensorBoard监控训练:**
    *   可视化是理解和调试强化学习过程的关键。TensorBoard是一个强大的工具。
    *   **为什么要监控？** 观察学习进展，发现潜在问题（如不收敛、震荡、梯度消失/爆炸），比较不同超参数/算法的效果。
    *   **关键监控指标:**
        *   `Episode Reward`: 每个回合的总奖励，是我们最关心的性能指标。
        *   `Loss`: TD误差或损失函数的值，观察其是否逐渐下降并稳定。
        *   `Epsilon` (如果使用ε-greedy): 观察探索率是否按预期衰减。
        *   `Q-values`: 观察平均或最大Q值的变化趋势，可以帮助判断是否存在过高估计等问题。
        *   `Gradients/Weights`: 监控梯度范数和权重分布有助于发现训练不稳定的问题。
    *   **实践:** 学习如何在你的代码中集成TensorBoard (`torch.utils.tensorboard.SummaryWriter`) 来记录上述指标。

---

## 小组项目实战：挑战与优化

现在，是时候将我们学到的新知识应用到实际项目中了！第二次课我们将聚焦于解决你在项目中遇到的问题，并尝试应用各种改进和调优技巧来提升模型性能。

### 常见挑战与应对策略

在训练DQN（尤其是在优化如CartPole这样的项目时）可能会遇到各种问题。以下是一些常见问题及其可能的分析方向和解决思路：

*   **模型不收敛 / 性能很差:**
    *   **检查清单:** 奖励函数设计是否合理？状态或奖励是否进行了归一化？学习率设置是否合适（过大或过小）？探索策略是否有效（探索不足或过早停止探索）？网络结构是否过于复杂或简单？经验回放缓冲区是否太小？目标网络更新频率是否恰当？代码实现是否存在Bug？
    *   **调试工具:** 打印关键变量（如状态、奖励、Q值、损失）；单步调试；利用TensorBoard监控指标变化；编写简单的测试用例验证代码逻辑。
    *   **尝试方案:** 调整学习率；更换或调整探索策略参数；简化或复杂化网络结构；调整缓冲区大小和更新频率；尝试Double/Dueling DQN等改进；仔细检查环境交互和数据处理代码。

*   **训练不稳定 / 奖励大幅震荡:**
    *   **可能原因:** 学习率过大；目标网络更新过于频繁（硬更新）或 $\tau$ 过大（软更新）；批次大小过小导致梯度方差大；经验回放缓冲区未能有效打破数据相关性；奖励尺度过大。
    *   **尝试方案:** 降低学习率；降低目标网络更新频率或减小软更新的 $\tau$；尝试梯度裁剪 (Gradient Clipping) 限制梯度范数；增大批次大小；确保缓冲区足够大且采样有效；对奖励进行归一化或裁剪。

*   **探索不足 / 过早收敛到次优策略:**
    *   **可能原因:** 初始探索率 $\epsilon$ 或温度 $\tau$ 设置过低；探索率衰减过快；探索策略本身效率不高。
    *   **尝试方案:** 提高初始探索率；减缓探索率衰减速度或调整衰减方式；尝试Noisy Networks或Boltzmann探索等更高级的探索策略；检查奖励函数是否会过早惩罚探索行为。

**分享与讨论:** 你在项目中遇到了哪些具体问题？尝试过哪些解决方法？和其他同学交流你的经验和困惑。

---

## 进阶视野：彩虹DQN (Rainbow)

DQN的研究并未停止。一个里程碑式的工作是**Rainbow DQN**，它并非提出全新的思想，而是巧妙地将之前我们讨论过的多种DQN改进技术（以及一种称为**分布式RL**的技术）集成到了一个统一的框架中。

*   **核心组件:** Rainbow通常集成了：
    1.  **Double DQN**
    2.  **优先经验回放 (PER)**
    3.  **Dueling Networks**
    4.  **多步学习 (Multi-step Learning)**
    5.  **分布式RL (Distributional RL):** 不再只学习Q值的期望，而是学习Q值的完整分布。
    6.  **Noisy Nets**

*   **核心思想:** "集大成者"。研究者发现，这些改进之间存在一定的**协同效应**，将它们组合在一起的效果往往优于单个改进或简单的两两组合。

*   **意义:** Rainbow展示了组合已有改进可以大幅提升性能和样本效率，为后续研究提供了基准。它也启发我们，在解决复杂问题时，可以考虑融合多种方法的优点。虽然我们不要求完整实现Rainbow，但了解它的构成有助于我们理解各项改进的重要性和相互关系。

---

::: {.callout-tip}
## 课后活动与作业

1.  **代码实践:** 在你的小组项目中（如CartPole或其他环境），选择并完整实现**至少一项**本周学习的DQN改进（Double DQN, Dueling DQN, PER, Multi-step Learning, Noisy Nets），并与你之前的基线DQN进行**实验对比**。
2.  **实验报告:** 撰写一份简单的实验报告，包含以下内容：
    *   清晰描述你选择并实现的改进方法的原理。
    *   展示你的对比实验结果（**必须包含**使用TensorBoard或其他绘图工具生成的关键性能图表，如：**每个回合奖励**随训练步数/回合数的变化曲线，**损失函数**变化曲线等）。清晰标注图中不同曲线对应的算法。
    *   分析和讨论你的实验结果：改进方法是否带来了预期的性能提升？效果如何？如果没有达到预期效果，可能的原因是什么？
    *   简述你在实现和实验过程中遇到的主要挑战以及你是如何解决的。
3.  **思考:** Rainbow DQN集成了多种技术。你认为这些技术之间可能存在怎样的相互作用（例如，哪些技术组合可能效果特别好，哪些可能存在冲突或冗余）？请阐述你的想法。
:::

::: {.callout-warning}
## 下周预习重点

在深入了解了基于价值的DQN方法之后，下周我们将探索另一大类强化学习算法：**基于策略的方法 (Policy-Based Methods)**。请思考并尝试了解以下问题：

1.  **策略梯度 (Policy Gradient) 的核心思想是什么？** 它与基于价值的方法（如DQN直接学习Q值）有何根本不同？（提示：策略梯度方法直接学习策略函数 $\pi(a|s)$）
2.  **REINFORCE 算法是如何工作的？** 它是最基础的策略梯度算法之一，尝试理解其更新规则背后的直觉。它的主要优缺点是什么？
3.  **Actor-Critic 方法试图解决什么问题？** 它如何结合基于价值和基于策略的方法？其基本框架是怎样的？
4.  **思考:** 为什么DQN通常不适用于**连续动作空间**（例如，控制机器人的关节角度）？下周我们将学习的基于策略的方法（特别是Actor-Critic的变种）如何解决这个问题？
::: 