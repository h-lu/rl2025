# 第一周：强化学习入门

::: {.callout-tip appearance="simple"}
## 本周学习目标
- 了解什么是强化学习，以及强化学习的核心概念
- 区分强化学习与监督学习、无监督学习
- 了解强化学习在商业领域的应用案例
- 初步体验 AI 辅助编程，并使用 AI 工具进行简单的 Python 编程练习
- 熟悉强化学习常用的 Python 库
:::

## 第一次课：什么是强化学习？

::: {.callout-important}
## 核心定义
强化学习是一种通过与环境交互来学习的机器学习方法。智能体通过不断尝试和犯错来学习最优策略。
:::

### 1. 强化学习的核心概念

::: {.callout-note}
## 以电商推荐系统为例理解核心概念

#### 智能体 (Agent)
- 负责学习和做出决策的实体
- 例如：推荐算法就是一个智能体，它决定向用户推荐什么商品

#### 环境 (Environment)
- 智能体所处的外部世界
- 例如：用户群体、商品库、市场状况等
- 环境会对智能体的动作做出响应

#### 状态 (State)
- 环境在某一时刻的描述
- 例如：用户的历史浏览记录、搜索关键词、当前页面停留时间等
- 智能体根据状态来决定下一步动作

#### 动作 (Action)
- 智能体可以采取的操作
- 例如：推荐特定商品、调整商品展示顺序、发送个性化优惠券等

#### 奖励 (Reward)
- 环境对智能体动作的反馈
- 例如：用户点击推荐商品(正奖励)、直接关闭页面(负奖励)
- 奖励信号指导智能体改进决策
:::

::: {.callout-tip}
## 强化学习交互过程可视化

![强化学习交互过程](image/week1/rl_interaction.svg)

## 强化学习基本流程
1. 智能体观察环境，获取当前状态
2. 基于当前状态和策略选择一个动作
3. 执行动作，与环境交互
4. 环境转移到新状态，并给予智能体奖励
5. 智能体根据获得的奖励更新其策略
6. 重复上述过程，直到学习收敛或达到终止条件
:::

### 2. 机器学习方法对比

::: {.callout-important}
## 学习方法对比

![机器学习方法对比](image/week1/ml_comparison.svg)
:::

::: {.callout-note}
## 三种学习方法的本质差异

**监督学习**：有"老师"（标签）直接告诉模型正确答案，模型通过对比其预测与正确答案之间的差距来学习。

**无监督学习**：没有"老师"，模型需要自己从数据中寻找规律和模式，不依赖外部反馈。

**强化学习**：有"评价者"而非"老师"，评价者只告诉模型其行为的好坏（奖励信号），但不直接告诉正确答案，模型需要通过尝试和探索来找到获取最大奖励的策略。
:::

### 3. 商业应用案例详解

::: {.callout-note}
## 智能推荐系统
- 应用场景：电商平台、内容平台
- 实现方式：
  - 收集用户行为数据作为状态
  - 推荐算法作为智能体
  - 用户反馈作为奖励信号
- 商业价值：
  - 提高用户转化率
  - 增加平台收入
  - 改善用户体验
:::

::: {.callout-note}
## 动态定价系统
- 应用场景：酒店预订、网约车平台
- 实现方式：
  - 市场供需作为状态
  - 价格调整作为动作
  - 成交量和收入作为奖励
- 商业价值：
  - 优化收入管理
  - 平衡供需关系
  - 提高资源利用率
:::

## 第二次课：AI辅助编程入门与平衡杆实践

### 1. AI辅助编程工具入门

::: {.callout-warning}
## 注意事项
AI辅助编程工具可以显著提高编程效率，但要注意理解生成的代码逻辑
:::

::: {.callout-tip}
## GitHub Copilot 安装步骤
1. 打开 VS Code
2. 转到扩展市场
3. 搜索 "GitHub Copilot"
4. 点击安装
:::

::: {.callout-note}
## 实用技巧
- 使用清晰的注释引导生成
- 分步骤编写复杂逻辑
- 及时检查生成的代码
- 理解并修改不合适的建议
:::

### 2. 强化学习环境搭建

::: {.callout-tip}
## Gymnasium 库基础使用
```python
# 安装 Gymnasium
pip install gymnasium

# 基本使用示例
import gymnasium as gym

# 创建环境
env = gym.make("CartPole-v1", render_mode="human")

# 获取环境信息
print(f"动作空间: {env.action_space}")
print(f"状态空间: {env.observation_space}")
```
:::

::: {.callout-tip}
## Stable Baselines3 库基础使用
```python
# 安装 Stable Baselines3
pip install stable-baselines3

# 基本使用示例
from stable_baselines3 import DQN

# 创建模型
model = DQN("MlpPolicy", env, verbose=1)

# 训练模型
model.learn(total_timesteps=10000)
```
:::

### 3. CartPole 环境实践

::: {.callout-important}
## CartPole 环境说明
CartPole 是强化学习入门的经典环境，目标是通过左右移动小车来保持杆子平衡
:::

::: {.callout-tip}
## CartPole 环境可视化

![CartPole环境](image/week1/cartpole_env.svg)
:::

::: {.callout-note}
## 环境参数
```python
import gymnasium as gym
import numpy as np

# 创建环境
env = gym.make("CartPole-v1", render_mode="human")

# 环境参数说明
print("状态空间包含4个连续值:")
print("- 小车位置: [-4.8, 4.8]")
print("- 小车速度: [-∞, ∞]")
print("- 杆子角度: [-0.418, 0.418]")
print("- 杆子角速度: [-∞, ∞]")

print("\n动作空间包含2个离散动作:")
print("- 0: 向左推")
print("- 1: 向右推")
```
:::

::: {.callout-tip}
## 随机动作示例代码
```python
# 运行随机策略
observation, info = env.reset()
total_reward = 0

for step in range(100):
    # 随机选择动作
    action = env.action_space.sample()
    
    # 执行动作
    observation, reward, terminated, truncated, info = env.step(action)
    total_reward += reward
    
    # 打印当前状态
    print(f"Step {step}: State = {observation}, Reward = {reward}")
    
    # 判断是否需要重置
    if terminated or truncated:
        print(f"Episode finished after {step + 1} steps")
        print(f"Total reward: {total_reward}")
        break

env.close()
```
:::

::: {.callout-note}
## 简单规则策略实现示例
对于CartPole任务，我们可以实现一个基于当前状态的简单规则策略，而不是随机选择动作：

```python
def simple_rule_policy(observation):
    """
    基于杆子角度和角速度的简单规则策略
    
    参数：
        observation: 环境观测值，包含4个状态变量
        
    返回：
        action: 0(向左推)或1(向右推)
    """
    # 解析观测值
    cart_position, cart_velocity, pole_angle, pole_velocity = observation
    
    # 如果杆子正在向右倾斜，则向右推车（让杆子回到中心）
    if pole_angle > 0:
        return 1
    # 如果杆子正在向左倾斜，则向左推车
    else:
        return 0

# 使用规则策略运行环境
observation, info = env.reset()
total_reward = 0

for step in range(1000):  # 设置更大的步数上限
    # 使用规则策略选择动作
    action = simple_rule_policy(observation)
    
    # 执行动作
    observation, reward, terminated, truncated, info = env.step(action)
    total_reward += reward
    
    # 判断是否需要重置
    if terminated or truncated:
        print(f"Episode finished after {step + 1} steps")
        print(f"Total reward: {total_reward}")
        break

env.close()
```

这个简单的规则策略基于杆子的倾斜角度来决定将小车向哪个方向推动，虽然简单，但通常比随机策略能获得更好的性能。在强化学习中，我们的目标是通过学习来自动发现这样的策略，甚至找到更好的策略。
:::

::: {.callout-important}
## 课后作业
1. 环境配置
   - 安装 Python 3.8+
   - 配置 VS Code 和 GitHub Copilot
   - 安装所需 Python 库

2. 编程练习
   - 修改随机动作示例，尝试实现简单的规则策略
   - 记录并分析不同策略的表现
   - 使用 AI 辅助工具优化代码

3. 思考题
   - 强化学习如何应用到你的专业领域？
   - CartPole 环境中的状态空间设计有什么特点？
   - 为什么需要重置环境？
:::

::: {.callout-note}
## 预习资料
1. 阅读材料
   - Gymnasium 官方文档
   - Stable Baselines3 入门教程
   - 马尔可夫决策过程基础概念

2. 视频资源
   - CartPole 环境详解
   - 强化学习基础概念讲解

3. 下周预习重点
   - 马尔可夫决策过程
   - 价值函数与策略函数
   - Grid World 环境介绍
:::

