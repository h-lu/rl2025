---
title: "Week 9 - 教师指导手册"
subtitle: "函数逼近入门"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解表格型 RL 方法的局限性（维度灾难、连续空间、泛化能力差）。
    *   学生掌握函数逼近 (Function Approximation) 的核心思想：用带参数的函数近似价值函数 (V̂ 或 Q̂)。
    *   学生理解函数逼近的优势（泛化、处理大规模/连续空间）。
    *   学生了解函数逼近的学习过程（类比监督学习，最小化损失，梯度下降）。
    *   学生理解线性函数逼近的基本概念（特征向量、权重、更新规则），并认识其局限性。
    *   学生通过 CartPole 案例理解为何对于连续状态空间问题必须使用函数逼近。
    *   学生初步了解 Stable Baselines3 (SB3) 库的用途和基本用法。
*   **次要目标:**
    *   为后续学习 DQN 和其他 DRL 算法奠定基础。
    *   让学生认识到从表格方法到函数逼近是 RL 走向实际应用的关键一步。
    *   激发学生对使用现成 DRL 库解决复杂问题的兴趣。

# 重点概念 (Key Concepts)

*   表格型 RL 的局限性 (维度灾难, 连续空间, 泛化)
*   函数逼近 (Function Approximation)
*   参数化价值函数: V̂(s, **w**), Q̂(s, a, **w**)
*   函数逼近的优势 (泛化, 处理复杂空间)
*   学习过程: 目标值 (Target), 预测值, 损失函数, 梯度下降
*   半梯度 TD(0) (Semi-gradient TD(0)) 更新规则 (概念)
*   线性函数逼近: 特征向量 φ(s), 权重 **w**, V̂(s, w) = wᵀφ(s)
*   特征工程 (Feature Engineering)
*   CartPole 环境 (作为函数逼近必要性的例子)
*   Stable Baselines3 (SB3) 库介绍 (用途, 基本用法概览)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 17 (约 90 分钟):**
    *   回顾表格型 RL 方法及其局限性 (15-20 分钟): 结合上周动态定价案例的讨论，强调状态空间爆炸问题。
    *   为何需要函数逼近 (15 分钟): 清晰阐述维度灾难、连续空间、泛化需求这三个核心动机。
    *   函数逼近基本思想 (25-30 分钟): **重点环节**。讲解用 V̂(s, w) / Q̂(s, a, w) 近似 V/Q。类比监督学习的拟合过程。介绍学习目标（最小化预测与目标的误差）。
    *   学习过程与半梯度 TD(0) (15-20 分钟): 简要介绍损失函数和梯度下降思想。展示半梯度 TD(0) 更新公式 **w** ← **w** + α * δ_t * ∇ V̂(S_t, **w**)，重点解释其结构（用 TD 误差指导参数更新），提及“半梯度”概念（不必深入）。
    *   Q&A (5 分钟)
*   **Session 18 (约 90 分钟):**
    *   回顾函数逼近思想 (5 分钟)。
    *   线性函数逼近 (20-25 分钟): 讲解特征向量 φ(s) 和权重 w 的概念，线性模型 V̂ = wᵀφ(s)。讨论其优缺点（简单 vs. 依赖特征工程、表达力有限）。
    *   概念 Lab/演示：表格方法的局限性 (CartPole) (25-30 分钟): **重点环节**。介绍 CartPole 环境（连续状态）。引导学生思考为何无法用表格 Q-Learning 解决（状态无限多、离散化挑战）。强调函数逼近的必要性。
    *   引入 Stable Baselines3 (SB3) 库 (20-25 分钟): 介绍 SB3 的定位（提供可靠 DRL 实现），核心特点（统一接口、兼容 Gym），展示 DQN 示例代码的基本结构（创建环境、定义模型、训练、保存、评估）。**强调本课程后续重点是使用而非实现 DRL 算法。**
    *   下周预告 & Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 17

*   **函数逼近引入:**
    *   **类比:** “我们无法记住每个人的具体长相（状态太多），但可以学习一些‘特征’（如发型、脸型、眼睛大小）和它们与‘类别’（如帅/不帅）的关系（参数化的函数），然后根据新面孔的特征来‘泛化’判断。”
    *   **图示:** 画一个简单的二维状态空间，如果用表格需要很多格子，但如果价值函数比较平滑，可能用一条直线或曲线（函数逼近）就能很好地拟合。
*   **学习过程:**
    *   **连接监督学习:** “我们将 RL 的目标值（如 TD 目标）看作监督学习里的‘标签 y’，将函数逼近器的输出 V̂(s, w) 看作‘预测值 ŷ’，然后用梯度下降调整参数 w 来最小化 (y - ŷ)²。”
    *   **半梯度 TD(0):** 重点解释更新方向由 TD 误差 δ_t 和梯度 ∇V̂ 共同决定。δ_t 告诉我们预测偏高还是偏低，∇V̂ 告诉我们调整哪个参数 w_i 能最有效地改变 V̂(s, w)。

## Session 18

*   **线性函数逼近:**
    *   **例子:** 如果状态是 (库存量 k, 剩余时间 t)，特征可以是 φ(s) = (1, k, t, k*t, k², t²)。V̂(s, w) = w₀ + w₁k + w₂t + w₃kt + w₄k² + w₅t²。
    *   **强调特征工程:** 效果好坏极大依赖于是否选对了特征。对于复杂问题，手动设计好特征非常困难。这正是深度学习（自动学习特征）的优势所在。
*   **CartPole 演示:**
    *   **运行环境:** 再次运行 CartPole 环境，让学生观察状态值的连续变化。
    *   **提问:** “这个状态有多少种可能？” (无限多)。“如果我们要用 Q 表，每个状态动作对都要存一个值，可行吗？” (不可行)。“如果把每个状态变量（位置、速度、角度、角速度）分成 10 段，总共有多少状态？” (10⁴=10000)。“如果分成 100 段呢？” (10⁸=1亿)。以此强调维度灾难。
*   **Stable Baselines3 介绍:**
    *   **定位:** “造车 vs. 开车”。SB3 让我们能“开上”别人造好的、可靠的 DRL “车”，专注于解决问题，而不是从头造轮子。
    *   **代码概览:** 快速过一遍 DQN 示例代码的几个主要步骤（`make_vec_env`, `DQN(...)`, `model.learn()`, `model.save()`, `evaluate_policy`），让学生对使用流程有个初步印象。**不必深入每个超参数的细节，下周会详细讲。**
    *   **设定预期:** 明确告知学生，后续 Lab 的重点是**理解算法思想、运行 SB3 代码、调整超参数、分析结果**，而不是要求他们手写 DQN/A2C。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 函数逼近会不会不准确？表格方法不是能精确表示每个状态的值吗？**
    *   A: 表格方法理论上可以精确表示每个状态的值，但前提是你能访问并更新所有状态足够多次，这在巨大状态空间中是不可能的。函数逼近确实是一种**近似**，它可能无法完美拟合真实的价值函数，会引入**近似误差 (Approximation Error)**。但它的**泛化能力**使得我们可以用有限的经验来估计大量状态的价值，这是表格方法做不到的。我们需要在近似误差和泛化能力之间做权衡。好的函数逼近器（如足够深的神经网络）可以在很多复杂问题上达到非常好的近似效果。
*   **Q: 线性函数逼近里的特征 φ(s) 怎么选？**
    *   A: 这通常需要领域知识和反复试验。好的特征应该能够捕捉状态中与价值相关的关键信息，并且数量不能太多。例如，在棋类游戏中，特征可能包括棋子数量、控制的中心区域、威胁等。特征工程本身就是一个复杂的任务。这也是为什么端到端的深度学习（直接从原始状态学习，让网络自己提取特征）如此受欢迎的原因。
*   **Q: 半梯度 TD(0) 更新规则里的梯度 ∇V̂(S_t, w) 怎么算？**
    *   A: 这取决于 V̂ 的具体形式。
        *   如果是**线性函数** V̂ = wᵀφ(s)，梯度就是特征向量 φ(s)。
        *   如果是**神经网络**，梯度就是网络输出相对于网络参数 w 的梯度，这可以通过**反向传播 (Backpropagation)** 算法自动计算（PyTorch, TensorFlow 等深度学习框架会自动处理）。我们不需要手动计算复杂网络的梯度。
*   **Q: Stable Baselines3 是唯一的 DRL 库吗？**
    *   A: 不是。还有其他流行的 DRL 库，例如 TF-Agents (TensorFlow), RLlib (Ray), Acme (DeepMind) 等。SB3 的优点是接口友好、文档清晰、基于 PyTorch（目前学术界和工业界广泛使用），并且提供了稳定可靠的经典算法实现，非常适合教学和快速原型开发。

# 与后续课程的联系 (Connections to Future Topics)

*   函数逼近是后续所有**深度强化学习 (DRL)** 算法的基础，包括下周要学习的 DQN。
*   理解函数逼近的学习过程（目标、损失、梯度下降）有助于理解 DRL 算法的训练机制。
*   Stable Baselines3 将是后续 Lab 中使用的主要工具。

# 教师准备建议 (Preparation Suggestions)

*   准备好解释表格方法局限性的例子（如状态空间计算）。
*   准备好函数逼近思想的类比和图示。
*   清晰地解释线性函数逼近的概念和局限。
*   确保能流畅地演示 CartPole 环境并解释其状态特点。
*   熟悉 Stable Baselines3 的基本安装和 DQN 示例代码的运行流程。
*   （可选）准备一个简单的线性函数逼近的 TD 更新示例（如果想更具体地展示）。