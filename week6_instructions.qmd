---
title: "Week 6 - 教师指导手册"
subtitle: "同策略控制 - SARSA"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解从预测 (Prediction) 到控制 (Control) 的转变，掌握广义策略迭代 (GPI) 的基本思想（评估 ↔ 改进）。
    *   学生理解为何在无模型控制中通常学习动作值函数 Q(s, a) 而不是 V(s)。
    *   学生理解同策略 (On-Policy) 与异策略 (Off-Policy) 的核心区别。
    *   学生掌握 SARSA 算法的核心思想、更新规则 (基于 S, A, R, S', A') 和其同策略特性。
    *   学生理解 ε-greedy 探索策略及其在平衡探索与利用中的作用。
    *   学生能够运行 Lab 4 代码（Gridworld 或 CliffWalking），使用 SARSA 算法学习策略。
    *   学生能够分析探索率 ε 对 SARSA 学习过程和最终策略的影响。
*   **次要目标:**
    *   巩固 TD 学习的概念。
    *   培养学生实现和调试基本 RL 控制算法的能力。
    *   通过 CliffWalking 案例初步感受不同算法可能产生的策略差异。

# 重点概念 (Key Concepts)

*   控制 (Control) vs. 预测 (Prediction)
*   广义策略迭代 (Generalized Policy Iteration, GPI)
*   策略评估 (Policy Evaluation) & 策略改进 (Policy Improvement)
*   基于动作值函数 (Q-function) 的控制
*   同策略 (On-Policy) vs. 异策略 (Off-Policy)
*   SARSA 算法 (On-Policy TD Control)
*   SARSA 更新规则: Q(S, A) ← Q(S, A) + α [R + γ Q(S', A') - Q(S, A)]
*   五元组 (S, A, R, S', A')
*   ε-greedy 探索策略
*   CliffWalking 环境 (作为 Lab 示例)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 11 (约 90 分钟):**
    *   回顾无模型预测 (MC & TD) (10 分钟)。
    *   从预测到控制 & GPI (20 分钟): 讲解控制的目标是找到最优策略，引入 GPI 框架（评估↔改进）。强调策略改进定理（贪心改进）。
    *   为何学习 Q 函数 (10 分钟): 解释在无模型下，Q 函数对于策略改进更直接。
    *   同策略 vs. 异策略 (15-20 分钟): **重点概念**。清晰解释两者的定义和核心区别（行为策略 vs. 目标策略）。可以用简单的例子类比。
    *   SARSA 算法详解 (25-30 分钟): **重点环节**。讲解 SARSA 的同策略特性，推导其更新规则，强调五元组 (S, A, R, S', A') 的作用，对比 TD(0) 预测。
    *   Q&A (5 分钟)
*   **Session 12 (约 90 分钟): Lab 4**
    *   回顾 SARSA 算法 (5 分钟)。
    *   ε-greedy 探索策略 (10-15 分钟): 讲解其机制、ε 的作用以及衰减的思路。
    *   Lab 4 目标与环境介绍 (15 分钟): 介绍 CliffWalking 环境的规则和目标，明确 Lab 任务是实现 SARSA 并分析 ε 的影响。
    *   指导运行 Lab 4 代码 (35-40 分钟): **重点环节**。带领学生理解 SARSA 的实现，特别是 `choose_action_epsilon_greedy` 函数和核心更新步骤。确保学生能运行代码并观察奖励曲线和最终策略图。
    *   讨论 ε 的影响与 SARSA 路径特点 (10-15 分钟): 引导学生分析不同 ε 值对收敛速度、最终奖励和策略路径的影响。讨论为什么 SARSA 在 CliffWalking 中倾向于走“安全”路径。
    *   Lab 提交要求说明 & Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 11

*   **GPI:**
    *   **图示:** 使用讲义中的 GPI 图进行讲解，强调评估和改进两个过程的交替和相互促进。
    *   **类比:** “学习做菜”：先按某个菜谱（策略 π）做一次（收集经验），然后品尝评估（评估 E），根据评估结果调整菜谱（改进 I），再做一次... 直到做出完美的菜（最优策略 π\*）。
*   **为何学 Q:**
    *   **反例:** “如果只知道每个路口（状态 S）到终点的最短距离 V(S)，但不知道走哪条路（动作 A）能达到这个最短距离（需要地图 P），那 V(S) 对我开车（决策）的直接帮助有限。但如果我知道从路口 S 走某条路 A 到达终点的最短距离 Q(S, A)，我就可以直接选 Q 值最小（或最大，取决于奖励定义）的那条路 A。”
*   **On-Policy vs. Off-Policy:**
    *   **核心区别:** On-Policy 是“边做边学边改进自己的做法”，Off-Policy 是“看着别人（或者自己过去）的做法，学习一套更好的做法”。
    *   **例子:** On-Policy 像是一个新手司机自己开车练习并改进驾驶技术；Off-Policy 像是这个新手司机观看老司机的行车记录仪视频来学习驾驶技术。
*   **SARSA 算法:**
    *   **名字来源:** 强调更新需要 (S, A, R, S', A') 这五个信息。
    *   **对比 TD(0):** 清晰展示两者更新公式的差异，SARSA 更新 Q(S, A)，TD(0) 更新 V(S)。
    *   **对比 Q-Learning (预告):** 可以简单提及 SARSA 的 TD 目标是 R + γQ(S', A')，而下周要学的 Q-Learning 用的是 R + γ max_a' Q(S', a')，引出两者核心差异。
    *   **同策略体现:** 强调 A' 是**实际**根据当前策略 π (ε-greedy) 选择的动作，更新考虑了探索行为可能带来的影响。

## Session 12 (Lab 4)

*   **ε-greedy:**
    *   **代码实现:** 讲解 `choose_action_epsilon_greedy` 函数的逻辑：生成随机数，与 ε 比较，决定是随机选还是选 Q 最大的。注意处理 Q 值相等时随机选择一个最佳动作。
    *   **ε 衰减 (可选):** 可以讨论为何需要衰减（早期探索，后期利用），并给出简单的衰减方式（如 `epsilon *= decay_rate`）。
*   **Lab 代码指导:**
    *   **核心更新:** 定位到 `Q[state, action] = Q[state, action] + alpha * td_error` 这一行，确保学生理解 `td_target` 是如何用 `Q[next_state, next_action]` 计算的。
    *   **状态转移:** 强调 `state = next_state` 和 `action = next_action` 这两步，将当前的“下一个状态/动作”变为下一次循环的“当前状态/动作”。
    *   **可视化:** 解释奖励曲线图反映了学习效率和稳定性，策略图直观展示了学到的行为模式。
*   **结果讨论:**
    *   **ε 的影响:** 引导学生观察：ε 太小可能收敛慢或陷入次优（奖励低），ε 太大可能导致奖励曲线震荡剧烈、最终奖励不高（因为总在随机走）。
    *   **SARSA 路径:** 提问：“为什么 SARSA 不愿意贴着悬崖走？” 引导学生思考：因为 ε-greedy 策略有时会随机选择向下走到悬崖里的动作 (A')，SARSA 在更新悬崖边状态的 Q 值时，会把这个负的 Q(S', A') 考虑进去，导致悬崖边的 Q 值降低，从而倾向于避开悬崖。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: GPI 听起来会一直进行下去，什么时候停止？**
    *   A: 理论上，当策略不再改进时，GPI 就收敛到了最优策略和最优价值函数。在实践中，我们通常设定一个最大迭代次数、最大训练步数，或者当策略性能（如评估回报）在一段时间内不再显著提升时停止训练。
*   **Q: On-Policy 和 Off-Policy 哪个更好？**
    *   A: 没有绝对的哪个更好，它们各有优劣，适用于不同的场景。On-Policy（如 SARSA）通常更稳定，易于实现和收敛，但可能样本效率较低，且学习到的策略受探索影响。Off-Policy（如 Q-Learning）可以利用历史数据，理论上能学习最优策略，但可能更不稳定，实现更复杂。选择哪个取决于具体问题、数据可用性和算法特性。
*   **Q: SARSA 更新需要下一个动作 A'，这和 Q-Learning 有什么本质区别？**
    *   A: 这是核心区别！SARSA 使用**实际将要执行**的下一个动作 A' 来计算 TD 目标 (R + γQ(S', A'))，因此它的学习目标是**遵循当前这个包含探索的策略**能得到的回报。而 Q-Learning 使用**理论上最优**的下一个动作 a' (max_a' Q(S', a')) 来计算 TD 目标 (R + γ max_a' Q(S', a'))，它的学习目标是**最优策略**的回报，而不管实际执行了哪个动作。这导致了它们在处理探索和最终策略上的差异。
*   **Q: Lab 里的 CliffWalking 环境，掉下悬崖奖励是 -100，这个值怎么定的？**
    *   A: 奖励值的设定是奖励工程的一部分。-100 是一个相对较大的负奖励，目的是强烈地惩罚掉下悬崖的行为，促使智能体学习避开它。这个具体数值可以调整，但需要足够大以超过走弯路累积的负奖励 (-1 per step)，否则智能体可能宁愿掉下去也不愿绕远路。

# 与后续课程的联系 (Connections to Future Topics)

*   理解 SARSA (On-Policy TD Control) 是与下周学习 Q-Learning (Off-Policy TD Control) 进行对比的基础。CliffWalking 环境是展示两者策略差异的经典案例。
*   GPI 框架是理解所有基于价值迭代的控制算法（包括 DQN）的基础。
*   ε-greedy 策略是后续许多 RL 算法中常用的探索机制。

# 教师准备建议 (Preparation Suggestions)

*   准备好 GPI 框架的图示和类比。
*   清晰地解释 On-Policy 和 Off-Policy 的区别，准备好相应的例子。
*   熟悉 SARSA 的更新公式和伪代码，能清楚解释其同策略特性。
*   确保 Lab 4 的 CliffWalking 代码能运行，并准备好分析 ε 的影响和 SARSA 策略特点。
*   （可选）可以准备一个简单的 Gridworld 环境运行 SARSA，与 CliffWalking 对比。