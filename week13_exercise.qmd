---
title: "Week 13 - 学生练习"
subtitle: "商业案例分析 1 - 动态定价与资源优化"
format:
  html:
    toc: true
    toc-location: left
    code-fold: show
---

# 练习目标

*   练习将 RL 概念应用于分析具体的商业问题（动态定价/资源优化）。
*   深入思考 MDP 定义中状态、动作、奖励设计的挑战和权衡。
*   理解数据需求和模拟环境在 RL 应用中的重要性。
*   识别 RL 策略部署到现实世界中的关键挑战。

# 练习内容

## 练习 1: 动态定价 MDP 定义挑战

回顾本周讲义中讨论的网约车动态定价案例。

1.  **状态表示 (S):**
    *   讲义中提到了多种可能的状态信息（时空、供需、上下文）。如果让你来设计状态向量，你会优先选择哪些信息？为什么？（选择 3-5 个最重要的）
    *   对于“时间”这个特征，直接使用小时数（0-23）作为状态的一部分有什么潜在问题？你会如何更好地表示时间特征？（提示：周期性）
    *   如果状态维度过高，除了使用更强大的函数逼近器（如 DRL），还有哪些方法可以尝试降低状态空间的复杂度？（提示：特征选择、特征组合、状态抽象）
2.  **奖励设计 (R):**
    *   假设平台只使用“平台短期收入”作为奖励 R。你认为学习到的 RL 策略可能会有什么特点？这种策略对平台的长期发展可能带来什么风险？
    *   如果想在奖励中同时考虑“订单完成率”，你会如何设计这个复合奖励函数？（例如，`R = w₁ * 收入 + w₂ * 完成率`，如何设定权重 w₁, w₂？或者有其他方式？）
    *   为什么说奖励函数设计是 RL 应用中最具挑战性的环节之一？

## 练习 2: 资源优化 MDP 建模练习

选择以下一个资源优化场景（或自选一个你熟悉的场景），尝试进行 MDP 定义：

**场景选项:**

1.  **共享单车调度:** 一个城市有多个区域，每个区域有一定数量的共享单车。智能体（调度系统）需要决定每天晚上从哪些区域调出多少单车，运往哪些区域，以最大化第二天的用户骑行总时长（或总收入），同时考虑调度成本。
2.  **数据中心能源管理:** 数据中心有服务器集群和储能设备（如电池）。智能体需要根据实时电价、预测的计算负载和当前电池电量，决定何时给电池充电、何时用电池供电、何时调整服务器负载，以最小化总电费成本。

**对于你选择的场景，请回答：**

*   **目标:** 明确定义优化的商业目标。
*   **状态 (S):** 列出关键的状态变量（至少 3-5 个），说明离散/连续。
*   **动作 (Action, A):** 列出智能体可以采取的动作，说明离散/连续。
*   **奖励 (Reward, R):** 定义一个合理的奖励函数，使其与你的优化目标对齐。
*   **(思考)** 这个问题的状态空间和动作空间可能有多大？使用表格型 RL 是否可行？如果不可行，为什么？

## 练习 3: RL 实践挑战思考

结合本周学习的动态定价/资源优化案例以及你对其他商业场景的理解，思考以下实践挑战：

1.  **Sim-to-Real Gap:** 为什么在模拟环境中训练好的 RL 策略，直接部署到真实世界中效果可能会打折扣？请至少列举两个可能导致这种 Gap 的原因。
2.  **冷启动:** 对于一个新上线的业务（如一个新的网约车市场或一个新的推荐功能），几乎没有历史数据，如何启动 RL 模型的训练？（至少提出一种可能的策略）
3.  **非平稳性:** 真实商业环境（如用户偏好、竞争对手策略、宏观经济）是不断变化的。标准的 MDP 假设环境是平稳的，这会对 RL 应用带来什么挑战？如何应对这种非平稳性？（至少提出一种可能的策略）
4.  **评估与安全:** 为什么不能直接将新训练好的 RL 定价策略或推荐策略大规模上线？在上线前，可以通过哪些方式来评估其效果和安全性？

# 提交要求

*   请将你的答案整理成一个文档（如 Word, PDF, 或 Markdown 文件）。
*   对于练习 1, 2, 3，请清晰地回答问题并阐述理由。
*   文件命名格式：`姓名_学号_Week13_Exercise.xxx`。
*   通过教学平台提交。