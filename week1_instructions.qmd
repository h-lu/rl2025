---
title: "Week 1 - 教师指导手册"
subtitle: "商业决策智能化与强化学习概览"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解课程的整体结构、目标、要求和评估方式。
    *   学生认识到传统商业决策面临的挑战（动态性、不确定性）。
    *   学生理解 AI 与 BI 的区别，并认识到 RL 在解决序贯决策问题上的独特性和必要性。
    *   学生掌握 RL 的核心要素 (Agent, Environment, S, A, R, π, V, Q) 并能将其初步应用于分析商业场景。
    *   学生理解探索 (Exploration) 与利用 (Exploitation) 的基本概念及其在商业中的重要性。
*   **次要目标:**
    *   激发学生对 RL 应用于商业决策的兴趣。
    *   建立课堂互动和讨论的氛围。

# 重点概念 (Key Concepts)

*   商业决策的复杂性 (动态、不确定、延迟反馈、高维)
*   RL vs. 监督学习 (从交互/经验中学习 vs. 从标签中学习)
*   RL 核心要素：Agent, Environment, State (S), Action (A), Reward (R), Policy (π), Value Function (V/Q)
*   RL 交互循环
*   探索 (Exploration) vs. 利用 (Exploitation)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 1 (约 90 分钟):**
    *   课程介绍 (15-20 分钟): 目标、大纲、评估方式、先修要求、激发兴趣。
    *   商业决策的挑战 (15 分钟): 结合实例讨论动态性、不确定性等。
    *   AI & BI & 为何需要 RL (25-30 分钟): 强调 RL 在序贯决策和缺乏明确标签场景下的优势。引用 AlphaGo 等案例（但不必深入技术细节），重点是引出 RL 的不同范式。
    *   RL 成功案例简介 (10 分钟): 简要介绍不同领域的应用，拓宽学生视野。
    *   Q&A (5-10 分钟)
*   **Session 2 (约 90 分钟):**
    *   回顾 Session 1 要点 (5 分钟)
    *   RL 核心要素详解 (35-40 分钟): 重点讲解 S, A, R, π, V, Q 的定义和相互关系。使用简单直观的例子（如走迷宫、订外卖）。强调 R 定义了目标。
    *   **互动练习：分解商业场景 (30-35 分钟):** 这是本周的重点互动环节。分组或全班讨论，引导学生将讲义中的定价、库存、营销、客服场景分解为 S, A, R。鼓励学生思考不同定义方式及其影响。
    *   探索 vs. 利用 (10 分钟): 讲解概念并结合商业实例。
    *   下周预告 & Q&A (5 分钟)

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 1

*   **开场:** 用一个引人入胜的商业决策难题开场（例如，高峰期网约车如何定价既能满足需求又能保证司机积极性？），引出课程主题。
*   **课程介绍:** 清晰、简洁，强调课程的实践性和与商业决策的联系。明确 AI 工具（如 Copilot）的使用边界：允许辅助编程，但核心逻辑和分析必须独立完成。
*   **商业决策挑战:** 多用 relatable 的例子，如“双十一”的库存准备、新产品上市的定价策略、疫情对供应链的影响等。引导学生思考这些决策的难点。
*   **为何需要 RL:** 对比监督学习的局限性。可以用一个简单的例子说明：监督学习可以预测明天的股价（如果数据足够好），但无法告诉你今天应该买入还是卖出（这需要考虑长期回报和风险，是序贯决策）。
*   **RL 案例:** 选择几个不同领域的案例，强调 RL 的通用性。不必深入技术，重点是展示 RL 能解决什么样的问题。

## Session 2

*   **核心要素讲解:**
    *   **类比:** 可以用游戏（如超级马里奥）来类比：马里奥是 Agent，游戏关卡是 Environment，马里奥的位置/状态是 State，按键（跳、跑）是 Action，得分/过关是 Reward，玩家的操作习惯是 Policy，对某个位置危险程度的判断是 Value Function。
    *   **强调 R 的重要性:** 奖励函数的设计直接决定了智能体的目标和最终行为。可以举例说明不当的奖励设计可能导致的问题（如只奖励点击率导致标题党）。
    *   **区分 V 和 Q:** V 评估状态好坏，Q 评估状态-动作对好坏。Q 对于决策更直接。
*   **互动练习 (关键环节):**
    *   **分组讨论:** 将学生分成小组，每个小组负责分析 1-2 个商业场景。
    *   **引导思考:**
        *   **状态 S:** 需要包含哪些信息才能做出决策？哪些信息是关键的？信息是离散的还是连续的？（引导学生思考状态表示的简洁性与完备性权衡）
        *   **动作 A:** 有哪些可选的操作？动作是离散的还是连续的？动作空间有多大？
        *   **奖励 R:** 如何量化“好”的决策？是短期收益还是长期价值？如何平衡多个目标？（这是最开放也最重要的问题）
    *   **分享与点评:** 请小组代表分享他们的 S, A, R 定义，教师进行点评，引导全班讨论不同定义方式的优劣和可能带来的影响。强调没有唯一“正确”的答案，建模本身就是一种权衡和简化。
*   **探索 vs. 利用:**
    *   用简单的例子说明，如“尝试新餐馆 vs. 去常去的餐馆”。
    *   强调这是 RL 中的基本困境，算法需要机制来平衡两者。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: RL 听起来很高深，我 Python 基础不太好能学会吗？**
    *   A: 课程重点在于理解 RL 的**思想、概念**以及如何将其**应用于商业问题建模和分析**。编程实验部分会提供代码框架和库（如 Stable Baselines3）的使用说明，允许使用 AI 工具辅助理解和生成部分代码。关键在于理解代码逻辑、调试运行并分析结果，而不是从零手写复杂算法。我们会循序渐进。
*   **Q: 状态 S 需要包含所有历史信息吗？那不是无限多了？**
    *   A: 这是一个很好的问题！理论上，如果严格满足马尔可夫性质，当前状态 S 就包含了所有相关的历史信息。但在实践中，我们会选择**最重要的、与未来决策最相关**的信息构成状态，这是一种近似。如何选择合适的状态表示是 RL 应用的关键挑战之一，我们后面会更深入讨论。
*   **Q: 奖励 R 怎么定？如果定了错误的奖励会怎么样？**
    *   A: 奖励设计是 RL 中最核心也最困难的部分。它直接定义了智能体的目标。如果奖励 R 设计不当（例如，只奖励短期点击率），RL 智能体会非常“聪明”地找到最大化这个错误奖励的方法，但可能导致与我们真正商业目标相悖的行为（例如，产生大量点击但没有转化）。我们需要仔细思考如何让奖励函数尽可能对齐长期的商业价值。
*   **Q: 探索和利用具体怎么平衡？有什么算法吗？**
    *   A: 非常好的问题！如何平衡探索和利用是 RL 的核心研究方向。有很多策略，比如简单的 ε-greedy（我们后面会讲），还有更复杂的 UCB、Thompson Sampling 等。不同的 RL 算法内部会有不同的机制来处理这个问题。我们将在后续课程中接触到一些具体方法。

# 与后续课程的联系 (Connections to Future Topics)

*   本周定义的 RL 核心要素和 MDP 框架是整个课程的基础。
*   下周将深入 MDP 的数学细节，特别是状态转移概率 P 和 Bellman 方程，为理解后续算法打下基础。
*   互动练习中对商业场景的分解，将在后续学习具体算法（Q-Learning, DQN, A2C）和进行案例分析时反复用到。
*   探索与利用的问题将在学习控制算法（SARSA, Q-Learning）时再次强调，并介绍 ε-greedy 等具体策略。

# 教师准备建议 (Preparation Suggestions)

*   准备 1-2 个生动形象的商业决策案例，用于开场和讨论。
*   熟悉讲义中的 RL 核心要素定义和例子。
*   预先思考互动练习中商业场景的多种 S, A, R 定义方式，准备引导学生讨论。
*   （可选）准备一个极简的 RL 交互演示（如文字版的 Gridworld 交互），帮助学生直观理解 Agent-Environment 循环。