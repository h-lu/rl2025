---
title: "Week 2 - 教师指导手册"
subtitle: "序贯决策建模 - 马尔可夫决策过程 (MDP)"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解马尔可夫性质的含义及其在商业决策建模中的近似应用。
    *   学生掌握马尔可夫决策过程 (MDP) 的形式化定义 (S, A, P, R, γ) 及其每个元素的意义。
    *   学生理解回报 (Return) 和折扣因子 (γ) 的概念，并能解释折扣因子在商业决策中的含义（短视 vs. 远见）。
    *   学生理解策略 (π) 和价值函数 (Vπ, Qπ) 的定义。
    *   学生能够直观理解 Bellman 期望方程，并能解释其商业含义（价值 = 即时收益 + 未来预期价值）。
    *   学生了解 Gym/Gymnasium 环境的基本概念和用法，并完成环境安装。
*   **次要目标:**
    *   培养学生将抽象数学模型 (MDP) 与具体商业问题联系起来的能力。
    *   为后续学习 Bellman 最优方程和无模型算法打下基础。
    *   确保学生具备运行后续 Lab 的基本环境。

# 重点概念 (Key Concepts)

*   马尔可夫性质 (Markov Property) & 商业近似
*   马尔可夫决策过程 (MDP): S, A, P, R, γ
*   回报 (Return) G_t
*   折扣因子 (Discount Factor) γ & 商业含义
*   策略 (Policy) π (确定性 vs. 随机性)
*   状态值函数 (State-Value Function) Vπ(s)
*   动作值函数 (Action-Value Function) Qπ(s, a)
*   Bellman 期望方程 (Bellman Expectation Equation) for Vπ and Qπ
*   Gym/Gymnasium 环境: `make`, `reset`, `step`, `render`, `observation_space`, `action_space`

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 3 (约 90 分钟):**
    *   回顾上周核心要素 (5 分钟)
    *   马尔可夫性质 (25-30 分钟): 直观解释，数学表达，重点讨论商业近似和状态设计的重要性。结合上周的商业场景例子讨论状态 S 如何设计才能更好地满足马尔可夫性。
    *   MDP 形式化定义 (30-35 分钟): 详细讲解 S, A, P, R, γ。重点是 P 和 R 的含义。再次使用商业场景例子来实例化这些元素。
    *   回报与折扣因子 (15-20 分钟): 讲解 G_t 的计算，重点讨论 γ 的作用和商业含义（短视 vs. 远见）。用投资决策、促销活动等例子说明。
    *   Q&A (5 分钟)
*   **Session 4 (约 90 分钟):**
    *   回顾 MDP 定义 (5 分钟)
    *   策略 π (10 分钟): 确定性 vs. 随机性。
    *   价值函数 Vπ 和 Qπ (20-25 分钟): 定义、含义、区分 V 和 Q。强调 Q 对决策更直接。
    *   Bellman 期望方程 (25-30 分钟): **重点环节**。从 Vπ 定义出发，直观推导 Bellman 期望方程。强调其递归结构和“即时收益 + 未来预期价值”的商业解读。对 Qπ 的方程做类似讲解。**不必强求学生手推数学公式，重在理解其结构和含义。**
    *   Gym/Gymnasium 环境介绍与安装指导 (15-20 分钟): 介绍 Gym 的目的和核心 API。演示安装过程。运行简单的随机智能体代码 (CartPole)，让学生直观感受交互过程。**提醒学生课后务必完成安装，为下周 Lab 做准备。**
    *   下周预告 & Q&A (5 分钟)

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 3

*   **马尔可夫性质:**
    *   用简单的例子说明：“明天的天气只取决于今天的天气状况（温度、湿度、气压等），而与昨天或更早的天气无关（假设今天的状况已包含所有相关信息）。”
    *   **讨论:** 让学生思考上周分解的商业场景（定价、库存等），它们的状态 S 是否满足马尔可夫性质？如果不满足，需要加入哪些额外信息才能更好地近似？（例如，库存问题只看当前库存不够，可能需要看近期销售趋势或预测的需求）。强调状态设计的重要性。
*   **MDP 定义:**
    *   **具象化:** 使用一个简单的 Gridworld 例子贯穿讲解 S, A, P, R, γ。
        *   S: 每个格子坐标 (r, c)。
        *   A: 上、下、左、右。
        *   P: 描述移动规则（e.g., 80% 概率按预期方向移动，10% 概率向左偏，10% 概率向右偏；撞墙则原地不动）。
        *   R: 到达目标格+1，掉入陷阱-1，其他格子-0.01。
        *   γ: 设定一个值，如 0.9。
    *   **强调 P 和 R:** P 描述环境的**动态 (dynamics)**，R 定义了智能体的**目标 (goal)**。
*   **折扣因子 γ:**
    *   **金融类比:** γ < 1 类似于金融中的贴现现金流 (DCF) 的概念，未来的钱不如现在的钱值钱。
    *   **极端情况:** 讨论 γ=0 (只看眼前) 和 γ=1 (未来和现在同等重要) 的策略差异。

## Session 4

*   **价值函数:**
    *   **直观解释:** Vπ(s) 是“从状态 s 出发，遵循策略 π 能得到多少好处”；Qπ(s, a) 是“在状态 s 先做动作 a，然后遵循策略 π 能得到多少好处”。
    *   **图示:** 可以画一个简单的状态转移图，标示 V 和 Q 的含义。
*   **Bellman 期望方程 (重点):**
    *   **逐步推导:** 从 Vπ(s) = E[G_t | S_t=s] 开始，将 G_t 展开为 R_{t+1} + γG_{t+1}，然后利用期望的性质和 MDP 定义逐步推导出方程。**避免过于复杂的数学推导，重在逻辑链条。**
    *   **强调递归:** 当前状态的价值可以通过**下一个**状态的价值来表示。
    *   **商业解读:** 反复强调“价值 = 即时收益 + 未来预期价值”的含义。例如，一个投资决策的价值 = 短期回报 + γ * (投资后公司状态的价值)。
*   **Gym/Gymnasium:**
    *   **现场演示:** 演示 `pip install gymnasium`，运行 CartPole 随机智能体代码，展示 `reset`, `step` 的输入输出，以及 `render` 的可视化效果。
    *   **强调 API:** 让学生理解 `step` 函数返回的 `observation, reward, terminated, truncated, info` 的含义。
    *   **安装提醒:** 强调课后安装的重要性，可以提供简单的安装检查脚本或步骤。鼓励学生在虚拟环境中安装。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 状态转移概率 P(s'|s, a) 和奖励 R(s, a, s') 在现实中怎么得到？**
    *   A: 非常好的问题！在很多现实商业问题中，我们**无法**精确知道 P 和 R。这就是为什么我们需要**无模型 (Model-Free)** 强化学习方法（如 MC, TD, Q-Learning），它们不需要知道 P 和 R，而是直接从与环境交互的经验中学习。我们将在后面几周重点学习这些方法。不过，理解 MDP 这个**模型已知**的框架对于理解无模型方法非常有帮助。
*   **Q: Bellman 方程看起来很复杂，我需要记住并手推公式吗？**
    *   A: 理解 Bellman 方程的**核心思想和结构**比死记硬背公式更重要。你需要理解它表达了当前价值与未来价值的递归关系，以及“即时收益 + 未来预期价值”的含义。我们不会要求你进行复杂的数学推导，但理解这个方程有助于你理解后续 TD 算法的更新规则。
*   **Q: 为什么需要折扣因子 γ？直接加总所有奖励不行吗？**
    *   A: 有几个原因：1) **数学收敛性:** 对于没有终点的持续性任务，如果不打折扣 (γ=1)，回报总和可能是无限大，无法比较策略好坏。γ<1 保证了回报是有限的。2) **未来不确定性:** 未来的奖励通常比眼前的奖励更不确定，打折扣可以体现这种不确定性。3) **商业偏好:** 很多商业决策确实更看重短期回报（类似贴现率）。当然，对于有明确终点的回合制任务，有时也可以设置 γ=1。
*   **Q: Gym/Gymnasium 安装失败怎么办？**
    *   A: 安装问题很常见。首先，强烈建议在**虚拟环境**中安装，避免与系统或其他项目的库冲突。其次，检查 Python 版本是否符合要求。查看错误信息，通常会提示缺少哪些依赖。可以尝试搜索错误信息，或者在课程论坛/群里提问。我们下周 Lab 开始前会留出时间解决环境问题。

# 与后续课程的联系 (Connections to Future Topics)

*   MDP 和 Bellman 期望方程是理解**动态规划 (DP)**（如果课程涉及）和所有**时序差分 (TD)** 学习算法（TD(0), SARSA, Q-Learning, DQN, A2C）的基础。TD 算法的更新规则本质上是 Bellman 方程的采样版本。
*   价值函数 Vπ 和 Qπ 的概念将贯穿整个课程，是评估策略和进行控制的核心。
*   Gym/Gymnasium 环境将在后续所有的 Lab 中使用。

# 教师准备建议 (Preparation Suggestions)

*   准备一个清晰的 Gridworld 示例，用于解释 MDP 各元素和 Bellman 方程。
*   准备好 Bellman 期望方程的直观推导步骤和商业解读。
*   确保自己的电脑上 Gym/Gymnasium 环境安装成功，并能流畅演示 CartPole 示例。
*   准备好应对常见的 Gym 安装问题的建议。