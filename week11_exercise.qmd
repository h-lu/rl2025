---
title: "Week 11 - 学生练习"
subtitle: "策略梯度方法 (Policy Gradient Methods)"
---

# 练习目标

*   理解基于策略 (Policy-Based) RL 方法与基于价值 (Value-Based) 方法的核心区别。
*   掌握策略梯度 (PG) 方法的基本思想：直接优化参数化策略。
*   理解策略梯度定理的直观含义。
*   掌握 REINFORCE 算法（蒙特卡洛策略梯度）的基本流程和更新方式。
*   理解 REINFORCE 算法高方差的缺点及其原因。
*   掌握使用基线 (Baseline) 降低方差的原理和优势函数 Aπ 的概念。

# 练习内容

## 练习 1: 基于策略 vs. 基于价值

1.  **核心区别:** 请用你自己的话，再次阐述基于策略的 RL 方法（如策略梯度）与基于价值的 RL 方法（如 Q-Learning, DQN）在学习目标和策略导出方式上的主要区别。
2.  **适用场景:** 在什么情况下，基于策略的方法通常比基于价值的方法更有优势？（至少列举两点，并说明原因）

## 练习 2: 策略梯度定理与 REINFORCE

1.  **策略梯度定理 (直观解释):** 策略梯度定理给出的梯度估计 $\nabla J(\theta) \approx E[\nabla \log \pi(A|S, \theta) * Q_{\pi}(S, A)]$ (或使用 $G_t$) 中，$\nabla \log \pi(A|S, \theta)$ 和 $Q_{\pi}(S, A)$ (或 $G_t$) 这两部分各自代表了什么含义？它们相乘后如何指导参数 $\theta$ 的更新方向（即“增加好动作概率，降低坏动作概率”）？
2.  **REINFORCE 算法:**
    *   REINFORCE 算法使用什么来估计策略梯度定理中的 $Q_{\pi}(S, A)$？
    *   为什么 REINFORCE 算法需要等到一个**完整的回合**结束后才能进行参数更新？
    *   REINFORCE 算法最主要的缺点是什么？为什么会产生这个缺点？

## 练习 3: 基线 (Baseline) 与优势函数

1.  **引入基线的目的:** 在策略梯度方法中，为什么要引入基线 (Baseline) $b(S)$？它主要解决了 REINFORCE 算法的什么问题？
2.  **基线的作用原理:** 从梯度估计 $E[\nabla \log \pi * (G_t - b(S))]$ 来看，为什么减去一个不依赖于动作 $A$ 的基线 $b(S)$ 不会改变梯度的期望值（即不引入偏差）？（提示：回顾讲义中的简单推导或用语言解释）
3.  **优势函数:**
    *   优势函数 $A_{\pi}(S, A)$ 的定义是什么？（写出公式）
    *   它直观地衡量了什么？
    *   为什么使用优势函数 $A_{\pi}$ 作为策略梯度更新中的“动作好坏衡量标准”通常比直接使用回报 $G_t$ 或 $Q_{\pi}$ 更好？

## 练习 4: 算法选择思考

假设你遇到以下两个不同的 RL 问题，你会倾向于优先考虑哪类算法（基于价值如 DQN，或基于策略/Actor-Critic 如 A2C）？请说明理由。

1.  **问题一：机器人抓取物体。** 状态是摄像头图像和关节角度（高维、连续），动作是控制机械臂每个关节的**精确力矩**（连续动作空间）。目标是成功抓取物体。
2.  **问题二：玩简单的棋盘游戏（如井字棋）。** 状态是棋盘布局（低维、离散），动作是在空格处落子（离散动作空间）。目标是赢得游戏。
