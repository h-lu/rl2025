{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习的基石：马尔可夫决策过程 (MDP) 核心公式 (新手友好版)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习的数学基础。  如果你想搞懂强化学习是怎么回事，理解 MDP 就非常重要。  别担心公式看起来吓人，我们会用最简单的方式，像讲故事一样，把这 9 个核心公式讲清楚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  “不念过去，只看现在”：马尔可夫性质 (Markov Property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**公式:**\n",
    "$$\n",
    "P(S_{t+1} = s' | S_t = s, A_t = a, H_t) = P(S_{t+1} = s' | S_t = s, A_t = a)\n",
    "$$\n",
    "*   $S_t$:  在时间步 $t$ 的状态 (current state at time step t)\n",
    "*   $S_{t+1}$: 在时间步 $t+1$ 的状态 (next state at time step t+1)\n",
    "*   $s$:  一个特定的当前状态 (a specific current state)\n",
    "*   $s'$:  一个特定的下一个状态 (a specific next state)\n",
    "*   $A_t$:  在时间步 $t$ 的动作 (action at time step t)\n",
    "*   $a$:  一个特定的动作 (a specific action)\n",
    "*   $H_t$:  到时间步 $t$ 的历史 (history up to time step t)\n",
    "*   $P(\\cdot)$:  概率 (probability of ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  就好像你现在站在一个岔路口，要决定下一步往哪条路走。  **马尔可夫性质** 就是说，你只需要关注 **“现在”** 你在哪里 (当前位置)，以及 **“现在”** 你打算怎么走 (当前选择)。  你 **不需要** 回忆之前走过的路，或者过去发生了什么。  因为所有重要的“过去”信息，都已经包含在你 **“现在”** 的位置里了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你玩游戏时，决定下一步怎么操作，通常只需要看 **“现在”** 游戏画面是什么样的，以及 **“现在”** 你想做什么操作。  你不需要记住之前每一步是怎么操作的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  “下一步会怎样？”：状态转移概率 (State Transition Probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**公式:**\n",
    "$$\n",
    "P_{ss'}^a = P(S_{t+1} = s' | S_t = s, A_t = a)\n",
    "$$\n",
    "*   $P_{ss'}^a$:  在状态 $s$ 下，执行动作 $a$ 后，转移到状态 $s'$ 的概率 (probability of transitioning to state $s'$ from state $s$ after taking action $a$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **状态转移概率** 就是告诉你，如果你 **“现在”** 在状态 $s$ (比如，你在迷宫的某个位置)，并且你 **“做”** 了一个动作 $a$ (比如，你选择向左走)，那么 **“有多大的可能性”** 你会 **“到达”** 状态 $s'$ (比如，你走到了迷宫的下一个位置)。  $P_{ss'}^a$  这个数值就代表了这个 “可能性” 有多大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你按电梯的 “上楼” 按钮 (动作)，状态转移概率告诉你，电梯 **“有多大可能”** 会到达 “上一层楼” (新的状态)，而不是卡住不动或者下楼。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  “做对了有奖励，做错了要挨罚”：奖励函数 (Reward Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**公式 (以状态-动作-状态奖励函数为例):**\n",
    "$$\n",
    "R(s, a, s') = E[R_{t+1} | S_t = s, A_t = a, S_{t+1} = s']\n",
    "$$\n",
    "*   $R(s, a, s')$:  在状态 $s$ 下，执行动作 $a$ 并转移到状态 $s'$ 后获得的奖励 (reward received after transitioning to state $s'$ from state $s$ by taking action $a$)\n",
    "*   $R_{t+1}$:  在时间步 $t+1$ 收到的奖励 (reward received at time step $t+1$)\n",
    "*   $E[\\cdot]$:  期望值 (expected value of ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **奖励函数** 就是用来告诉我们，在某个状态下，做了某个动作，结果变成了另一个状态，我们能 **“得到多少好处”** (奖励)。  这个 “好处” 可以是正面的 (得分、奖励金币)，也可以是负面的 (扣分、受到惩罚)。  奖励函数就像游戏里的 “打分规则”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你帮妈妈洗碗 (动作)，如果洗得很干净 (状态转移好)，妈妈会给你奖励 (奖励 +10 元)；如果打破了碗 (状态转移不好)，妈妈可能会批评你 (奖励 -5 元)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  “行动指南”：策略 (Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **确定性策略公式:**  $a = \\pi(s)$\n",
    "    *   $\\pi$:  策略函数 (policy function)，表示在每个状态下选择哪个动作\n",
    "*   **随机性策略公式:**  $\\pi(a|s) = P(A_t = a | S_t = s)$\n",
    "    *   $\\pi(a|s)$:  在状态 $s$ 下，选择动作 $a$ 的概率 (probability of choosing action $a$ in state $s$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **策略** 就是你的 **“行动指南”**，它告诉你在 **“每个状态”** 下，你 **“应该怎么做”** (选择哪个动作)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **确定性策略** 就像 **“固定的规则”**，比如 “红灯停，绿灯行”。  每次遇到相同的状态 (红灯)，都做相同的动作 (停车)。\n",
    "*   **随机性策略** 就像 **“概率指南”**，比如 “在状态 $s$ 时，有 80% 的概率向左走，20% 的概率向右走”。  每次遇到相同的状态，**不一定做相同的动作**，而是按照概率来选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你的 “驾驶策略” 就是你在开车时，根据不同的路况 (状态)，决定 “踩油门”、“刹车” 还是 “转弯” (动作) 的一套方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.  “评估状态和动作的好坏”：价值函数 (Value Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **状态价值函数 $V^\\pi(s)$ 公式:**  $V^\\pi(s) = E_\\pi [G_t | S_t = s]$\n",
    "    *   $V^\\pi(s)$:  在策略 $\\pi$ 下，状态 $s$ 的价值 (value of state $s$ under policy $\\pi$)\n",
    "    *   $G_t$:  从时间步 $t$ 开始到 episode 结束的总回报 (total return from time step $t$ onwards)\n",
    "    *   $E_\\pi[\\cdot]$:  在策略 $\\pi$ 下的期望值 (expected value under policy $\\pi$)\n",
    "*   **动作价值函数 $Q^\\pi(s, a)$ 公式:**  $Q^\\pi(s, a) = E_\\pi [G_t | S_t = s, A_t = a]$\n",
    "    *   $Q^\\pi(s, a)$:  在策略 $\\pi$ 下，在状态 $s$ 执行动作 $a$ 的价值 (value of taking action $a$ in state $s$ under policy $\\pi$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **价值函数** 就是用来 **“评估”**  一个 **“状态”** 或者一个 **“状态-动作组合”**  的 **“好坏程度”**。  “好” 的状态或动作，意味着从这里开始，**“未来能获得更多的总奖励”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **状态价值 $V^\\pi(s)$**  评估的是 **“当前所处的状态 $s$ 本身有多好”**。  就像评估 “现在这个位置，未来能给我带来多少好处”。\n",
    "*   **动作价值 $Q^\\pi(s, a)$**  评估的是 **“在当前状态 $s$ 下，如果我执行动作 $a$，会有多好”**。  就像评估 “在这个位置，如果我选择这样做，未来能给我带来多少好处”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  评估 “你现在所在城市” 的价值 (状态价值)，就是看 “在这个城市生活，未来你能获得多少幸福感、发展机会等等 (总奖励)”。  评估 “在当前城市，选择一份工作” 的价值 (动作价值)，就是看 “如果做这份工作，未来你能获得多少职业发展、收入提升等等 (总奖励)”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  “价值的秘密公式”：贝尔曼方程 (Bellman Equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **状态价值贝尔曼方程公式:**  $V^\\pi(s) = E_\\pi [R_{t+1} + \\gamma V^\\pi(S_{t+1}) | S_t = s]$\n",
    "    *   $\\gamma$:  折扣因子 (discount factor)，取值范围通常为 $[0, 1]$，表示未来奖励的折扣程度\n",
    "*   **动作价值贝尔曼方程公式:**  $Q^\\pi(s, a) = E_\\pi [R_{t+1} + \\gamma Q^\\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **贝尔曼方程**  就像一个 **“价值的秘密公式”**，它告诉你 **“现在” 的价值** 和 **“未来” 的价值** 之间有什么关系。  简单来说，**“现在” 的价值**  等于：\n",
    "\n",
    "1.  **“立刻能得到的好处”** (即时奖励)。\n",
    "2.  **加上 “打折后的 ‘未来’ 的价值”** (因为未来的奖励不如现在的奖励那么重要，所以要打个折扣 $\\gamma$)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝尔曼方程就像一个 “价值的计算器”，它可以帮你一步一步地算出每个状态或动作的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你今天的 “总收入” (价值) 可以看成是：  “今天上班赚到的工资” (即时奖励)  +  “未来几年工资增长的潜力 (但要打个折扣，因为未来的钱可能贬值)”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.  “最好的价值”：最优价值函数 (Optimal Value Functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **最优状态价值函数 $V^*(s)$ 公式:**  $V^*(s) = \\max_{\\pi} V^\\pi(s)$\n",
    "    *   $V^*(s)$:  最优状态价值函数 (optimal state value function)，表示所有策略下状态 $s$ 的最大价值\n",
    "    *   $\\max_{\\pi}$:  在所有可能的策略 $\\pi$ 中取最大值 (maximum over all possible policies $\\pi$)\n",
    "*   **最优动作价值函数 $Q^*(s, a)$ 公式:**  $Q^*(s, a) = \\max_{\\pi} Q^\\pi(s, a)$\n",
    "    *   $Q^*(s, a)$:  最优动作价值函数 (optimal action value function)，表示所有策略下在状态 $s$ 执行动作 $a$ 的最大价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **最优价值函数**  就是指 **“在所有可能的 ‘行动指南’ (策略) 中，能达到的 ‘最好’ 的价值”**。  就像 “所有菜谱里，能做出 ‘最好吃’ 的菜的那个菜谱”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **最优状态价值 $V^*(s)$**  就是指，如果你在状态 $s$，**“用尽所有 ‘最好’ 的方法”**，最终能获得的 **“最大”** 状态价值。\n",
    "*   **最优动作价值 $Q^*(s, a)$**  就是指，你在状态 $s$ 下执行动作 $a$ 后，**“之后都用 ‘最好’ 的方法”**，最终能获得的 **“最大”** 动作价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  玩游戏的目标是 “获得最高分” (最优价值)。  最优价值函数就是指，如果你 **“每一步都做出 ‘最聪明’ 的选择”**，最终能达到的 **“理论最高分”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.  “如何找到最好的价值？”：贝尔曼最优方程 (Bellman Optimality Equations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **状态价值贝尔曼最优方程公式:**  $V^*(s) = \\max_{a} \\sum_{s'} P_{ss'}^a [R(s, a, s') + \\gamma V^*(s')]$\n",
    "    *   $\\max_{a}$:  在所有可能的动作 $a$ 中取最大值 (maximum over all possible actions $a$)\n",
    "    *   $\\sum_{s'}$:  对所有可能的下一个状态 $s'$ 求和 (sum over all possible next states $s'$)\n",
    "*   **动作价值贝尔曼最优方程公式:**  $Q^*(s, a) = \\sum_{s'} P_{ss'}^a [R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a')]$\n",
    "    *   $\\max_{a'}$:  在所有可能的下一个动作 $a'$ 中取最大值 (maximum over all possible next actions $a'$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **贝尔曼最优方程**  是用来 **“找到 ‘最优价值’ 的秘密武器”**。  它告诉你，要让 **“现在” 的价值达到 “最好”**，你需要：\n",
    "\n",
    "1.  **“试遍所有可能的动作”** (在当前状态下)。\n",
    "2.  **“预测”**  如果做了某个动作，会 **“立刻得到多少好处”** (即时奖励)，以及 **“之后还能获得 ‘最好’ 的 ‘未来价值’”**。\n",
    "3.  **“选择那个能让你获得 ‘最大’  (即时奖励 + 未来最好价值) 的动作”**。  这样，你 “现在” 的价值就是 “最好” 的了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝尔曼最优方程就像一个 “最优策略的导航仪”，它指引你找到通往 “最优价值” 的最佳路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  你要在十字路口选择 “最快到达目的地” 的路线 (最优策略)。  你需要考虑：  每条路 “现在” 的路况 (即时情况)，以及 “如果走这条路，到达下一个路口后，再选择 ‘最快’ 的路，未来还能节省多少时间” (未来最好价值)。  然后选择那个 “总时间最短” 的路线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.  “通往成功的最佳指南”：最优策略 (Optimal Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **公式 (基于最优 Q 函数):**  $\\pi^*(s) = \\arg\\max_{a} Q^*(s, a)$\n",
    "    *   $\\pi^*(s)$:  最优策略 (optimal policy)，表示在每个状态 $s$ 下应该选择哪个动作才能获得最大价值\n",
    "    *   $\\arg\\max_{a}$:  返回使函数值最大的动作 $a$ (action $a$ that maximizes the function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大白话解释:**  **最优策略**  就是 **“能让你获得 ‘最优价值’ 的 ‘最佳行动指南’”**。  它告诉你，在 **“每个状态”** 下，**“应该选择哪个动作”**，才能最终获得 **“最多的总奖励”**，达到 “人生巅峰” (误)。  基于最优 Q 函数的最优策略，就是 **“在每个状态 $s$ 下，选择那个能让 ‘动作价值’ $Q^*(s, a)$ 达到最大的动作 $a$”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**生活例子:**  “最优菜谱” 就是能让你做出 “最好吃” 的菜的菜谱。  “最优策略” 就是能让你在游戏中 “获得最高分” 的 “完美游戏攻略”。  “人生最优策略” 就是指引你走向 “成功和幸福” 的 “人生指南” (虽然现实中可能没有绝对的最优策略，但在 MDP 框架下是存在的)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
