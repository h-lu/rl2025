---
title: "Week 12 - 学生练习"
subtitle: "Actor-Critic 方法"
---

# 练习目标

*   巩固对 Actor-Critic (AC) 框架的理解，包括 Actor 和 Critic 的角色分工。
*   掌握 AC 方法如何结合策略梯度和 TD 学习来降低方差。
*   理解使用 TD 误差作为优势函数估计来更新 Actor 的原理。
*   区分 A2C 和 A3C 的基本概念。
*   练习使用 Stable Baselines3 (SB3) 运行 A2C 算法。
*   通过实验对比 A2C 和 DQN 在离散动作任务上的表现。
*   理解 A2C 处理连续动作空间的能力。

# 练习内容

## 练习 1: Actor-Critic 概念

1.  **框架组成:** Actor-Critic (AC) 框架包含哪两个主要组成部分？它们各自的作用是什么？（提示：一个负责选动作，一个负责评估）
2.  **Critic 的作用:** Critic 网络（通常学习 $V$ 函数）是如何帮助 Actor 网络（策略网络）学习的？它提供了什么关键信息来指导 Actor 的更新？相比于 REINFORCE 使用的蒙特卡洛回报 $G_t$，这个信息有什么优势？
3.  **更新规则:**
    *   在基本的 Actor-Critic 方法中，Actor (策略参数 $\theta$) 的更新规则通常是什么样的？（写出包含 TD 误差 $\delta$ 的更新公式）
    *   Critic (价值参数 $w$) 的更新规则通常是什么样的？（写出包含 TD 误差 $\delta$ 的更新公式）
4.  **A2C vs. A3C:** A2C 和 A3C 的主要区别是什么？哪个是同步更新，哪个是异步更新？哪个在现代实践中更常用？

## 练习 2: 优势函数估计

1.  **优势函数 $A_{\pi}(s, a)$:** 回顾其定义，它衡量了什么？
2.  **TD 误差作为估计:** 为什么可以用 TD 误差 $\delta_t = R + \gamma V(S') - V(S)$ 来近似优势函数 $A_{\pi}(s, a)$？（提示：思考 $\delta_t$ 的期望与 $A_{\pi}$ 的关系）
3.  **为何有效:** 相比于直接使用 $Q_{\pi}(S, A)$ 或 $G_t$，使用优势函数（或其估计 $\delta_t$）来乘以 Score Function ($\nabla \log \pi$) 进行策略梯度更新，主要的好处是什么？

## 练习 3: A2C Lab 代码与结果分析 (基于 Lab 7)

本练习基于讲义/Lab 7 中提供的 CartPole 和 Pendulum 环境以及 SB3 A2C 实现代码。

1.  **代码理解 (CartPole):**
    *   在 A2C 的 SB3 实现中，`n_steps` 参数的作用是什么？它与 TD 学习有什么关系？
    *   `vf_coef` (值函数系数) 和 `ent_coef` (熵系数) 这两个参数分别调节了什么？它们对学习过程可能产生什么影响？
2.  **A2C vs. DQN (CartPole):**
    *   回顾你在 Lab 7 中运行 A2C 和 Lab 6 中运行 DQN 解决 CartPole 的结果。在你的实验中（或者根据预期），哪个算法收敛更快？哪个算法最终性能更好？哪个算法的训练曲线更平滑/更稳定？（简述观察到的现象即可）
    *   你认为造成这些差异的可能原因是什么？（提示：考虑 On-Policy vs Off-Policy, 经验回放 vs 并行环境等）
3.  **连续动作空间 (Pendulum):**
    *   为什么 A2C 能够处理像 Pendulum 这样的连续动作空间问题，而标准的 DQN 不能？（提示：思考 Actor 和 Critic 的输出以及 DQN 的 `max_a` 操作）
    *   (如果运行了 Pendulum 实验) A2C 在 Pendulum 任务上的表现如何？（例如，奖励曲线是否趋于上升？最终奖励大概是多少？）

# 提交要求

*   请将你的答案整理成一个文档（如 Word, PDF, 或 Markdown 文件）。
*   对于练习 1, 2, 3，请清晰地回答问题并阐述理由。
*   文件命名格式：`姓名_学号_Week12_Exercise.xxx`。
*   通过教学平台提交。