---
title: "Week 13 - 教师指导手册"
subtitle: "商业案例分析 1 - 动态定价与资源优化"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生能够将前面所学的 RL 概念和算法（MDP, Q-Learning, DQN, A2C 等）应用于分析具体的商业问题（动态定价/资源优化）。
    *   学生深入理解在实际商业场景中定义 MDP 各要素（特别是状态 S 和奖励 R）的复杂性和挑战性。
    *   学生认识到数据在 RL 应用中的关键作用以及相关的数据需求和挑战。
    *   学生了解模拟环境在 RL 开发和测试中的重要性。
    *   学生能够识别并将 RL 策略部署到现实世界中可能遇到的关键挑战（冷启动、Sim-to-Real、非平稳性、评估、安全等）。
*   **次要目标:**
    *   培养学生系统性地分析复杂商业问题的能力。
    *   加强学生对 RL 理论知识与商业实践之间联系的理解。
    *   引导学生批判性地思考 RL 应用的局限性和潜在风险。

# 重点概念 (Key Concepts)

*   将动态定价/资源优化问题形式化为 MDP
*   状态表示 (State Representation) 设计考量 (时空、供需、上下文、维度、连续性)
*   动作空间 (Action Space) 设计考量 (离散 vs. 连续)
*   奖励工程 (Reward Engineering) 挑战 (短期 vs. 长期指标, 多目标权衡, 奖励塑形陷阱)
*   数据需求与挑战 (量、质、偏差、日志策略、因果推断)
*   算法选择考量 (DQN vs. Actor-Critic, Offline RL, MARL)
*   模拟环境 (Simulation) 的作用与挑战 (保真度, Sim-to-Real Gap)
*   部署挑战 (冷启动, 非平稳性, 评估与安全, 可解释性与公平性)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 25 (约 90 分钟): 案例 MDP 定义与分析**
    *   回顾 DRL 算法 (DQN, A2C) (10 分钟)。
    *   引入动态定价/资源优化案例背景 (10 分钟): 介绍网约车、酒店、广告等场景。
    *   **深度案例分析：网约车动态定价 MDP 定义 (50-55 分钟):** **核心环节**。
        *   **引导讨论 S:** 逐步引导学生思考需要哪些信息，讨论维度、连续性、特征工程、近似马尔可夫性等挑战。
        *   **引导讨论 A:** 讨论离散与连续动作的优劣和适用算法。
        *   **引导讨论 R:** **重点深入讨论**。区分短期/长期指标，讨论多目标权衡，强调奖励设计对最终策略的影响和潜在陷阱。
        *   简述 P 和 γ。
    *   数据需求讨论 (10 分钟): 讨论训练模型需要哪些数据，以及可能遇到的数据问题。
    *   Q&A (5 分钟)
*   **Session 26 (约 90 分钟): 模拟、挑战与讨论**
    *   回顾案例 MDP 定义要点 (5 分钟)。
    *   可选算法讨论 (10 分钟): 根据 S 和 A 的特点，讨论 DQN、A2C 等算法的适用性，简单提及 Offline RL 和 MARL。
    *   模拟环境的作用与挑战 (15-20 分钟): 讲解为何需要模拟器，以及构建准确模拟器的困难和 Sim-to-Real Gap 问题。
    *   **讨论：参数调整与影响 (15-20 分钟):** 结合模拟场景，讨论探索率、学习率、折扣因子等关键参数如何影响学习结果。
    *   **讨论：实施挑战 (25-30 分钟):** **重点环节**。系统性地引导学生讨论冷启动、非平稳性、评估、安全、公平性等实际部署中会遇到的问题及可能的应对思路。
    *   总结与 Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 25

*   **案例引入:** 选择学生熟悉的场景（如打车软件高峰期加价）作为切入点。
*   **MDP 定义讨论 (核心):**
    *   **互动性:** 采用提问、分组讨论、头脑风暴等形式，让学生充分参与到 S, A, R 的定义过程中。
    *   **深入 R:** 这是最能体现商业理解的部分。
        *   提问：“只奖励平台收入会怎样？”（可能导致价格过高，乘客流失）。“只奖励订单完成率呢？”（可能导致价格过低，司机不愿接单）。“如何平衡？”
        *   提问：“如何衡量‘乘客满意度’或‘品牌形象’并放入奖励？”（引导思考代理指标、延迟奖励、多目标）。
    *   **强调权衡:** 在讨论 S, A, R 时，不断强调没有完美方案，都是在不同因素间做权衡（如状态的完备性 vs. 维度，奖励的准确性 vs. 可衡量性）。
*   **数据需求:** 结合 MDP 定义，反向思考需要哪些数据来支撑状态表示和奖励计算。

## Session 26

*   **模拟环境:**
    *   **必要性:** 强调在真实环境中试错成本太高（资金损失、用户流失）。
    *   **Sim-to-Real:** 用例子说明差距，如模拟器未考虑突发交通拥堵、用户对价格的非理性反应等。
*   **参数影响讨论:**
    *   **连接理论:** 将参数影响与之前学习的算法原理联系起来（如 ε 影响探索，α 影响收敛稳定性）。
*   **实施挑战讨论 (重点):**
    *   **结构化:** 可以按照讲义中列出的几个挑战逐一进行讨论。
    *   **开放性:** 鼓励学生结合自己的理解和常识，思考每个挑战的具体表现和可能的解决方案。
    *   **现实感:** 强调这些挑战是 RL 在工业界成功落地的真正瓶颈所在，光有好的算法是不够的。
    *   **公平性/伦理:** 再次强调技术应用需要考虑社会影响。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 状态 S 包含这么多信息，神经网络能处理得过来吗？**
    *   A: 这正是深度学习的优势所在。深度神经网络（尤其是结合了 Embedding, CNN, RNN 等结构的网络）能够处理高维、异构（包含连续和离散）的输入，并自动从中学习有效的特征表示。当然，网络结构的设计、特征的预处理和选择仍然很重要。
*   **Q: 奖励 R 这么难定，有没有通用的好方法？**
    *   A: 没有一劳永逸的方法，奖励工程是 RL 应用中最具艺术性和挑战性的部分之一。通常需要：1) 深入理解商业目标；2) 从可衡量的指标开始（如收入、点击率）；3) 思考这些指标与长期目标的关系，尝试加入能反映长期的代理指标或使用 LTV 估计；4) 通过模拟和 A/B 测试不断迭代和验证奖励函数的效果。有时也会用到从专家演示中学习奖励的逆强化学习 (IRL)。
*   **Q: Sim-to-Real Gap 听起来很难解决，那模拟还有用吗？**
    *   A: 模拟仍然非常有用，甚至是不可或缺的。虽然存在 Gap，但模拟提供了一个低成本、可控的环境来进行算法开发、初步训练和参数调优。关键在于：1) 尽可能提高模拟器的保真度；2) 使用能够增强策略鲁棒性的技术（如 Domain Randomization）；3) 认识到 Gap 的存在，在部署到真实环境时采取谨慎的策略（如 A/B 测试、在线微调）。
*   **Q: 这么多挑战，RL 在商业中真的能成功应用吗？**
    *   A: 是的，尽管挑战重重，RL 已经在许多商业领域取得了显著的成功，尤其是在互联网行业（如推荐、广告、游戏 AI）和一些优化问题（如数据中心冷却、资源调度）上。成功的关键往往在于：1) 选择合适的应用场景（问题边界清晰、数据可获取、允许一定试错）；2) 强大的工程能力（数据处理、模拟器构建、模型部署）；3) 算法、领域知识和业务理解的紧密结合；4) 持续的投入和迭代。

# 与后续课程的联系 (Connections to Future Topics)

*   本周对动态定价/资源优化问题的分析，为理解 RL 在运营管理、金融科技等领域的应用提供了基础。
*   对奖励工程、Sim-to-Real、评估等挑战的讨论，将在下周讨论推荐系统和总结实践挑战时进一步加深。
*   对算法选择（DQN vs. AC, Offline RL, MARL）的讨论，为后续可能的前沿方向介绍埋下伏笔。

# 教师准备建议 (Preparation Suggestions)

*   熟悉网约车动态定价等案例的背景知识。
*   准备好引导学生深入讨论 MDP 定义（特别是 S 和 R）的问题和角度。
*   梳理清楚 RL 落地实践中的主要挑战及其应对思路。
*   （可选）查找一些关于 RL 在动态定价或资源优化方面应用的公开资料或论文摘要，作为补充阅读或讨论素材。