\documentclass[zihao=5,noanswers]{BHCexam}
\usepackage{ctex}
\usepackage{multirow}

%\linespread{1.2}

% \usepackage{minted}
% \setminted{mathescape, style = black, linenos=true}


\begin{document}

\renewcommand{\O}{\mathcal{O}}


\biaoti{\kaishu \textbf{上海应用技术大学 2023--2024 学年第二学期}}
\kemu{\textbf{《{\kaishu 强化学习}》期末试卷 \quad (B)}}
\setxz{40}{20}{2}
\setpd{20}{10}{2}
\setjd{40}{4}
\xinxi{100}{90}

\maketitle

\begin{flushleft}
 \qquad\kaishu \textbf{课程代码}: \underline{\hspace{2ex}B4105300\hspace{2ex}} \quad\kaishu \textbf{学分}: \underline{\hspace{5ex}3\hspace{5ex}} \quad \kaishu \textbf{考试时间}: \underline{\hspace{3ex}90\hspace{3ex}}\kaishu \textbf{分钟}\\
 \qquad\kaishu \textbf{课程序号}: \underline{\hspace{25ex}2400273,2400274\hspace{25ex}}\\
 \qquad\vspace{1ex}
 \kaishu \textbf{班级:} \underline{\hspace{15ex}} \quad
 \kaishu \textbf{学号:} \underline{\hspace{15ex}} \quad
 \kaishu \textbf{姓名:} \underline{\hspace{15ex}} \quad
\end{flushleft}

 {\small \kaishu \textbf{我已阅读了有关的考试规定和纪律要求, 愿意在考试中遵守《考场规则》, 如有违反将愿接受相应的处理.}}
\begin{center}
   \setlength{\tabcolsep}{3mm}
   \begin{tabular}{*{12}{|c}|}
      \hline \textbf{题号} & 一 & 二 & 31 & 32 & 33 & 34 &   &   & &  总分 \\
      \hline \textbf{满分}& 40 & 20 & 10 & 10 & 10 & 10  &  &  &  & 100\\
      \hline \textbf{得分}&&&&&&&&&& \\
      \hline
   \end{tabular}
\end{center}

 {\small \textbf{试卷共~\numpages~页, 请先查看试卷有无缺页, 然后答题.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{questions}

\xuanze

\question 强化学习的目标是：
\choice{最小化损失函数}{最大化准确率}{最大化累积奖励}{最小化状态数量}

\question 在马尔可夫决策过程(MDP)中，状态转移概率函数P(s'|s,a)表示：
\choice{在状态s执行动作a获得的奖励}{从状态s执行动作a后到达状态s'的概率}{状态s的价值}{动作a在状态s下的价值}

\question 在强化学习中，最优动作值函数Q*(s,a)的定义是：
\choice{在状态s下执行动作a后获得的即时奖励}{在状态s下执行动作a后的最大期望回报}{状态s的最大期望回报}{遵循任意策略时在状态s执行动作a的期望回报}

\question 以下哪种算法属于无模型(Model-free)强化学习方法？
\choice{策略迭代}{值迭代}{Q-Learning}{动态规划}

\question 强化学习中的"探索与利用"权衡是指：
\choice{在训练与测试间的平衡}{在奖励与惩罚间的平衡}{在尝试新动作与选择已知最优动作间的平衡}{在在线学习与离线学习间的平衡}

\question 在时序差分(TD)学习中，与蒙特卡洛方法相比的主要优势是：
\choice{更低的偏差}{能够从不完整的回合中学习}{只使用实际奖励不使用估计值}{不需要探索策略}

\question 在深度强化学习中，目标网络(Target Network)的主要作用是：
\choice{加速梯度下降}{提高探索效率}{增加神经网络深度}{稳定训练过程}

\question 策略梯度方法与基于值的方法相比的主要优势是：
\choice{能够直接处理连续动作空间}{计算效率更高}{需要更少的训练样本}{不需要探索策略}

\question Actor-Critic方法的核心思想是：
\choice{结合蒙特卡洛和时序差分学习}{结合策略梯度和值函数逼近}{结合在线学习和离线学习}{结合监督学习和无监督学习}

\question 在强化学习中，什么是回报(Return)？
\choice{即时奖励}{所有状态的平均奖励}{从当前时刻开始的折扣累积奖励}{环境的状态转移函数}

\question 强化学习中的同策略(On-policy)学习和异策略(Off-policy)学习的主要区别是：
\choice{同策略使用神经网络，异策略使用表格}{同策略需要环境模型，异策略不需要}{同策略使用当前正在学习的策略采集数据，异策略可以使用不同策略采集数据}{同策略适用于离散状态，异策略适用于连续状态}

\question 以下哪种算法是异策略(Off-policy)强化学习方法？
\choice{SARSA}{Actor-Critic}{蒙特卡洛策略评估}{Q-Learning}

\question 深度Q网络(DQN)中的经验回放(Experience Replay)机制主要解决的问题是：
\choice{存储容量限制}{数据样本之间的相关性}{神经网络训练速度慢}{环境随机性大}

\question 在蒙特卡洛方法中，对状态价值的更新是基于：
\choice{后续一步的奖励与下一状态的估计值}{后续所有时间步的实际奖励}{仅当前状态的即时奖励}{环境模型预测的奖励}

\question 在策略梯度方法中，优势函数(Advantage Function)的作用是：
\choice{加速收敛}{减小方差}{增加探索}{降低训练复杂度}

\question 时序差分(TD)学习中的"自举(Bootstrapping)"是指：
\choice{使用多个不同的学习率}{使用当前估计更新当前估计}{使用多个网络参数副本}{在多个环境同时训练}

\question 在强化学习中，前交互学习(Off-line Learning)指的是：
\choice{在真实环境中学习}{在线上进行实时学习}{使用事先收集的数据集进行学习}{不更新模型参数的学习}

\question SARSA算法名称的由来是：
\choice{算法发明者的姓名缩写}{随机搜索与行动算法的缩写}{算法更新使用的数据元组(S,A,R,S',A')}{状态动作奖励状态动作的英文缩写}

\question 以下哪种方法不适合解决连续动作空间的问题？
\choice{策略梯度}{Actor-Critic}{原始形式的DQN}{DDPG}

\question 强化学习中的"信用分配问题"(Credit Assignment Problem)是指：
\choice{如何确定每个状态的奖励值}{如何分配计算资源}{如何确定哪些动作导致了最终的结果}{如何为智能体设计奖励函数}

\panduan

\question 强化学习中的奖励值一定是正数，负奖励表示惩罚。

\question 策略迭代(Policy Iteration)算法通常比值迭代(Value Iteration)算法需要更少的迭代次数就能收敛。

\question 在强化学习中，折扣因子γ越大，表示智能体越重视未来奖励。

\question 时序差分(TD)学习方法同时具有蒙特卡洛方法和动态规划方法的优点。

\question DQN算法主要用于解决连续动作空间问题。

\question 在强化学习中，价值函数(Value Function)用于评估状态或状态-动作对的好坏。

\question Actor-Critic方法中，Actor负责选择动作，Critic负责评估动作。

\question 异策略(Off-policy)学习方法比同策略(On-policy)学习方法通常有更低的样本效率。

\question 强化学习算法的收敛性严重依赖于探索策略的选择。

\question 蒙特卡洛方法不需要环境的马尔可夫性质，而时序差分方法依赖于此。

\jianda

\question[10] 探索与利用(Exploration & Exploitation)：
\begin{enumerate}
    \item 什么是强化学习中的"探索与利用"问题？为什么它在强化学习中很重要？（3分）
    \item 描述两种常用的探索策略，并分析它们的优缺点（4分）
    \item 如何在训练过程中平衡探索与利用？（3分）
\end{enumerate}

\question[10] 策略梯度方法(Policy Gradient Methods)：
\begin{enumerate}
    \item 简述策略梯度方法的基本思想（3分）
    \item 策略梯度方法与基于值的方法（如Q-Learning）相比有哪些优势和劣势？（4分）
    \item 为什么在策略梯度方法中通常需要使用基线(Baseline)技术？（3分）
\end{enumerate}

\question[10] Actor-Critic方法：
\begin{enumerate}
    \item 描述Actor-Critic方法的基本框架和工作原理（4分）
    \item Actor和Critic各自的作用是什么？它们如何协同工作？（3分）
    \item Actor-Critic方法如何结合策略梯度和时序差分学习的优点？（3分）
\end{enumerate}

\question[10] 强化学习应用场景分析：
\begin{enumerate}
    \item 强化学习在游戏、机器人控制和资源管理等领域的应用各有什么特点？（4分）
    \item 在面对不同类型的强化学习问题时，如何选择合适的算法？请给出方法和依据（3分）
    \item 在实际应用中，强化学习面临哪些常见挑战？如何应对这些挑战？（3分）
\end{enumerate}

\end{questions}



\ifprintanswers
\else
\newpage
\kemu{\textbf{《{\kaishu 强化学习}》期末试卷\quad (答题纸)}}
\maketitle


\begin{flushleft}
 \qquad\kaishu \textbf{课程代码}: \underline{\hspace{2ex}B4105300\hspace{2ex}} \quad\kaishu \textbf{学分}: \underline{\hspace{5ex}3\hspace{5ex}} \quad \kaishu \textbf{考试时间}: \underline{\hspace{3ex}90\hspace{3ex}}\kaishu \textbf{分钟}\\
 \qquad\kaishu \textbf{课程序号}: \underline{\hspace{25ex}2400273,2400274\hspace{25ex}}\\
 \qquad\vspace{1ex}
 \kaishu \textbf{班级:} \underline{\hspace{15ex}} \quad
 \kaishu \textbf{学号:} \underline{\hspace{15ex}} \quad
 \kaishu \textbf{姓名:} \underline{\hspace{15ex}} \quad
\end{flushleft}

 {\small \kaishu \textbf{我已阅读了有关的考试规定和纪律要求, 愿意在考试中遵守《考场规则》, 如有违反将愿接受相应的处理.}}
\begin{center}
   \setlength{\tabcolsep}{3mm}
   \begin{tabular}{*{12}{|c}|}
      \hline \textbf{题号} & 一 & 二 & 三 & 四 & 五 & 六 &   &   & &  总分 \\
      \hline \textbf{满分}& 40 & 20 & 10 & 10 & 10 & 10  &  &  &  & 100\\
      \hline \textbf{得分}&&&&&&&&&& \\
      \hline
   \end{tabular}
\end{center}

\vspace{0.5cm}

\end{document} 