# 强化学习期末复习

## 一、强化学习基础概念

### 1. 强化学习简介
- **定义**：强化学习是机器学习的一个分支，智能体通过与环境交互，学习如何做出一系列决策以最大化累积奖励。
- **与其他学习方法的区别**：
  - 监督学习：基于有标签的训练数据学习预测
  - 无监督学习：从无标签数据中发现结构
  - 强化学习：通过试错与环境交互，学习优化决策序列

### 2. 强化学习核心元素
- **智能体(Agent)**：做决策的学习实体
- **环境(Environment)**：智能体交互的外部系统
- **状态(State, S)**：环境和智能体的当前情况
- **动作(Action, A)**：智能体可以执行的操作
- **奖励(Reward, R)**：环境对智能体动作的反馈信号
- **策略(Policy, π)**：从状态到动作的映射，π(a|s)表示在状态s下选择动作a的概率
- **值函数(Value Function)**：衡量状态或状态-动作对的价值
  - 状态值函数 V(s)：从状态s开始，遵循当前策略的期望累积奖励
  - 动作值函数 Q(s,a)：在状态s采取动作a，之后遵循当前策略的期望累积奖励

### 3. 强化学习的目标
- 找到最优策略π*，使累积奖励最大化
- 关键挑战：探索与利用的权衡、延迟奖励、环境不确定性

## 二、马尔可夫决策过程(MDP)

### 1. MDP的定义与组成
- **马尔可夫性质**：未来状态只依赖于当前状态，与过去路径无关
- **MDP的五元组表示**：(S, A, P, R, γ)
  - S：状态空间
  - A：动作空间
  - P：状态转移概率函数，P(s'|s,a)表示从状态s采取动作a转移到状态s'的概率
  - R：奖励函数，R(s,a,s')表示从s经动作a到达s'获得的奖励
  - γ：折扣因子（0≤γ≤1），表示未来奖励的重要性

### 2. 价值函数
- **状态值函数(V函数)**：
  - V^π(s) = E_π[∑γ^t·R_{t+1}|S_0=s]
  - 表示从状态s开始，遵循策略π的期望累积折扣奖励
- **动作值函数(Q函数)**：
  - Q^π(s,a) = E_π[∑γ^t·R_{t+1}|S_0=s, A_0=a]
  - 表示在状态s采取动作a，之后遵循策略π的期望累积折扣奖励
- **最优值函数**：
  - V*(s) = max_π V^π(s)
  - Q*(s,a) = max_π Q^π(s,a)

### 3. 贝尔曼方程(Bellman Equation)
- **贝尔曼期望方程**：当前状态价值与后续状态期望价值的递归关系
  - V^π(s) = ∑_a π(a|s)∑_{s',r} p(s',r|s,a)[r + γV^π(s')]
  - Q^π(s,a) = ∑_{s',r} p(s',r|s,a)[r + γ∑_{a'} π(a'|s')Q^π(s',a')]
- **贝尔曼最优方程**：最优价值与后续最优状态价值的递归关系
  - V*(s) = max_a ∑_{s',r} p(s',r|s,a)[r + γV*(s')]
  - Q*(s,a) = ∑_{s',r} p(s',r|s,a)[r + γmax_{a'} Q*(s',a')]

## 三、动态规划(Dynamic Programming)方法

### 1. 策略评估(Policy Evaluation)
- **目标**：计算给定策略π的价值函数V^π
- **方法**：迭代更新价值函数，利用贝尔曼期望方程
- **算法流程**：
  1. 初始化V(s)为任意值
  2. 重复直到收敛：
     对每个状态s，更新V(s) = ∑_a π(a|s)∑_{s'} p(s'|s,a)[r(s,a,s') + γV(s')]

### 2. 策略改进(Policy Improvement)
- **目标**：根据当前价值函数改进策略
- **方法**：更新策略为对当前价值函数贪心的策略
- **策略改进定理**：如果 Q^π(s,π'(s)) ≥ V^π(s) 对所有状态s成立，则π'优于或等于π

### 3. 策略迭代(Policy Iteration)
- **目标**：找到最优策略
- **算法流程**：
  1. 初始化π为任意策略
  2. 策略评估：计算V^π
  3. 策略改进：更新π(s) = argmax_a ∑_{s'} p(s'|s,a)[r(s,a,s') + γV^π(s')]
  4. 如果策略改变，返回步骤2；否则，返回最终策略

### 4. 值迭代(Value Iteration)
- **目标**：直接计算最优值函数，然后得到最优策略
- **算法流程**：
  1. 初始化V(s)为任意值
  2. 重复直到收敛：
     对每个状态s，更新V(s) = max_a ∑_{s'} p(s'|s,a)[r(s,a,s') + γV(s')]
  3. 最优策略：π*(s) = argmax_a ∑_{s'} p(s'|s,a)[r(s,a,s') + γV*(s')]

## 四、蒙特卡洛(Monte Carlo)方法

### 1. 基本概念
- **特点**：基于完整回合的经验学习，不依赖环境模型
- **思想**：通过多次采样估计期望值
- **适用**：回合制问题（有明确终止状态）

### 2. MC预测：估计策略价值
- **算法流程**：
  1. 执行策略π，生成多个回合
  2. 对每个回合中出现的每个状态s，计算从该状态到回合结束的累积奖励G
  3. 更新该状态的价值：V(s) = V(s) + α(G - V(s)) 或取平均

### 3. MC控制：估计最优策略
- **策略改进**：基于贪心或ε-贪心策略
- **首次访问MC**：每个回合只考虑每个状态-动作对的首次出现
- **每次访问MC**：考虑每个回合中状态-动作对的所有出现
- **探索起始状态**：确保所有状态-动作对都有被访问的机会

### 4. MC方法的优缺点
- **优点**：
  - 不需要环境模型（转移概率和奖励）
  - 可以处理未知或复杂环境
  - 只需要样本回合
- **缺点**：
  - 需要完整回合，不适用于连续任务
  - 需要足够的样本减少方差
  - 更新延迟，等到回合结束才更新

## 五、时序差分(Temporal Difference, TD)学习

### 1. TD预测：估计策略价值
- **TD(0)算法**：
  - 更新规则：V(S_t) = V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]
  - TD目标：R_{t+1} + γV(S_{t+1})
  - TD误差：δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)
- **特点**：
  - 自举(Bootstrap)：使用当前估计更新当前估计
  - 增量更新：不需要等待回合结束
  - 适用于在线学习和持续任务

### 2. SARSA：同策略TD控制
- **名称**：来自算法使用的序列(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})
- **更新规则**：Q(S_t, A_t) = Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]
- **与环境交互**：
  1. 在S_t选择A_t (使用ε-贪心)
  2. 执行A_t，观察R_{t+1}, S_{t+1}
  3. 在S_{t+1}选择A_{t+1} (使用ε-贪心)
  4. 更新Q(S_t, A_t)
  5. S_t = S_{t+1}, A_t = A_{t+1}，返回步骤2
- **特点**：同策略，行为策略与目标策略相同

### 3. Q-Learning：异策略TD控制
- **更新规则**：Q(S_t, A_t) = Q(S_t, A_t) + α[R_{t+1} + γmax_a Q(S_{t+1}, a) - Q(S_t, A_t)]
- **与环境交互**：
  1. 在S_t选择A_t (使用ε-贪心)
  2. 执行A_t，观察R_{t+1}, S_{t+1}
  3. 更新Q(S_t, A_t)（使用max操作）
  4. S_t = S_{t+1}，返回步骤1
- **特点**：
  - 异策略：行为策略（如ε-贪心）与估计策略（贪心）不同
  - 可直接逼近最优动作值函数Q*
  - 通常比SARSA收敛更快，但在训练过程中可能更冒险

### 4. TD方法的比较
- **TD vs MC**：
  - TD不需要等待回合结束就可更新
  - TD利用马尔可夫性质，而MC不依赖于此
  - TD通常样本效率更高，但可能偏差更大
- **SARSA vs Q-Learning**：
  - SARSA是同策略，Q-Learning是异策略
  - SARSA考虑实际执行的下一动作，Q-Learning考虑理论上最优的下一动作
  - SARSA更安全，Q-Learning更积极（学习最优策略）

## 六、函数逼近与深度强化学习

### 1. 函数逼近的需求与方法
- **挑战**：状态空间过大或连续，难以使用表格表示
- **方法**：使用函数逼近器（如线性模型、神经网络）参数化值函数或策略
- **常见函数逼近器**：
  - 线性函数逼近：V(s) ≈ w^T·φ(s)，Q(s,a) ≈ w^T·φ(s,a)
  - 非线性函数逼近：神经网络、决策树等

### 2. 使用函数逼近的价值学习
- **梯度TD学习**：更新参数以最小化TD误差
- **线性TD(0)**：w = w + α·δ·∇_w V(s)
- **半梯度方法**：只考虑当前状态估计的梯度，忽略下一状态值的梯度

### 3. 深度Q网络(DQN)
- **核心思想**：使用深度神经网络逼近Q函数
- **主要挑战**：
  - 训练不稳定：样本之间相关性高，目标值随参数更新变化
- **两个关键技巧**：
  - 经验回放(Experience Replay)：存储和随机采样经验，打破数据相关性
  - 目标网络(Target Network)：使用较慢更新的网络计算目标，稳定训练
- **DQN算法流程**：
  1. 初始化经验回放缓冲区D，主网络参数w，目标网络参数w^-
  2. 循环：
     - 使用ε-贪心策略选择动作
     - 执行动作，观察奖励和下一状态
     - 存储经验到缓冲区D
     - 从D采样小批量经验
     - 计算目标y = R + γmax_a Q(S',a;w^-)
     - 更新参数w以减小(y - Q(S,A;w))^2
     - 每C步更新目标网络参数：w^- = w

### 4. 策略梯度方法(Policy Gradient)
- **基本思想**：直接参数化策略π(a|s;θ)，通过梯度上升优化参数θ
- **策略梯度定理**：∇_θ J(θ) = E_π[∇_θ log π(A|S;θ)·G_t]
- **REINFORCE算法**：
  1. 使用当前策略生成一个完整回合
  2. 对每个时间步t，计算回报G_t
  3. 更新参数：θ = θ + α·G_t·∇_θ log π(A_t|S_t;θ)
- **基线(Baseline)技术**：
  - 为减小方差，引入与动作无关的基线函数b(s)
  - 更新变为：θ = θ + α·(G_t - b(S_t))·∇_θ log π(A_t|S_t;θ)
  - 常用基线：状态值函数V(s)

### 5. Actor-Critic方法
- **基本框架**：结合策略梯度和值函数逼近
  - Actor：参数化策略π(a|s;θ)，负责选择动作
  - Critic：参数化值函数V(s;w)，评估策略好坏
- **学习过程**：
  - Actor利用TD误差更新策略：θ = θ + α·δ·∇_θ log π(A_t|S_t;θ)
  - Critic利用TD学习更新值函数：w = w + β·δ·∇_w V(S_t;w)
  - TD误差：δ = R_{t+1} + γV(S_{t+1};w) - V(S_t;w)
- **优势**：
  - 结合TD和策略梯度，兼具低方差和在线学习能力
  - 比纯策略梯度方法更稳定、效率更高
  - 可以自然处理连续动作空间

### 6. A2C/A3C算法
- **A2C(Advantage Actor-Critic)**：
  - 同步版本，使用多个并行环境收集经验
  - 使用优势函数A(s,a) = Q(s,a) - V(s)指导策略更新
- **A3C(Asynchronous Advantage Actor-Critic)**：
  - 异步版本，使用多个独立的worker并行学习
  - 各worker异步更新共享的全局网络
  - 不需要经验回放，依靠异步性打破数据相关

## 七、常见应用场景与实践注意事项

### 1. 强化学习的应用场景
- **游戏**：围棋、国际象棋、电子游戏
- **机器人控制**：运动控制、导航、抓取
- **资源管理**：电力系统、交通控制、网络路由
- **推荐系统**：内容推荐、广告投放
- **医疗**：治疗方案优化、药物开发

### 2. 算法选择建议
- **对于有模型的小规模MDP**：使用动态规划方法
- **对于回合制任务**：蒙特卡洛方法
- **对于需要实时更新的任务**：TD学习
- **对于离散小规模状态空间**：表格型方法
- **对于大规模或连续状态空间**：函数逼近方法
- **对于连续动作空间**：策略梯度或Actor-Critic方法

### 3. 实践中的常见挑战
- **奖励设计**：稀疏奖励、延迟奖励、奖励塑形
- **探索与利用的平衡**：ε-贪心、上置信界(UCB)、熵正则化
- **样本效率**：异策略方法、经验回放、模型学习
- **超参数调整**：学习率、折扣因子、网络结构
- **评估与调试**：可视化、记录关键指标、基线比较 