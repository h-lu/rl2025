---
title: "第三周：Q-Learning 算法详解与实践"
---

## 课程目标

- 理解 Q-Learning 算法的核心思想和原理
- 掌握 Q-Table 的概念和更新规则
- 学习使用 Q-Learning 算法解决迷宫寻宝 (Grid World) 问题
- 了解超参数 (学习率、折扣因子) 的作用，并进行简单调整
- 掌握 Q-Learning 算法的基本编程实现

## 第一次课：Q-Learning 算法详解

### 1.  Q-Learning 算法核心思想

::: {.callout-note}
Q-Learning 是一种**基于价值 (Value-based)** 的**离线 (Off-policy)** 强化学习算法，用于学习**最优 Q 函数**，从而得到**最优策略**。
:::

- **核心思想**:  通过不断**试错 (Trial-and-Error)** 和**更新 Q-Table**，逐步逼近最优 Q 函数。
- **离线 (Off-policy)**:  学习的策略 (Q 函数对应的策略) 与实际执行的策略 (探索策略，例如 $\epsilon$-greedy 策略) 可以不同。
- **最优 Q 函数 ($Q^*(s, a)$)**:  表示在状态 $s$ 下，执行动作 $a$，并**之后都采取最优策略**所能获得的**最大期望累积奖励**。
- **最优策略 ($\pi^*(s)$)**:  在每个状态 $s$ 下，选择能使 Q 函数值 $Q^*(s, a)$ 最大的动作 $a$。

### 2.  Q-Table (Q 值表)

::: {.callout-note}
Q-Table 是 Q-Learning 算法中用于**存储 Q 函数值**的表格。表格的**行**表示**状态 (State)**，**列**表示**动作 (Action)**，**单元格**存储的是 **Q 值** $Q(s, a)$。
:::

- **表格结构**:

    | 状态 (State) | 动作 1 (Action 1) | 动作 2 (Action 2) | ... | 动作 n (Action n) |
    |------------|--------------------|--------------------|-----|--------------------|
    | 状态 1 (S1) | $Q(S1, A1)$       | $Q(S1, A2)$       | ... | $Q(S1, An)$       |
    | 状态 2 (S2) | $Q(S2, A1)$       | $Q(S2, A2)$       | ... | $Q(S2, An)$       |
    | ...        | ...                | ...                | ... | ...                |
    | 状态 m (Sm) | $Q(Sm, A1)$       | $Q(Sm, A2)$       | ... | $Q(Sm, An)$       |

- **初始化**:  Q-Table 通常**初始化为 0** 或者**小的随机值**。
- **更新**:  在智能体与环境交互的过程中，**不断更新 Q-Table 中的 Q 值**，使其逐渐逼近真实的最优 Q 函数值。
- **查询**:  在决策时，根据当前状态 $s$，**查询 Q-Table 中对应行的 Q 值**，选择 Q 值最大的动作 (或者根据探索策略选择动作)。

### 3.  Q-Learning 更新规则 (时序差分学习)

::: {.callout-note}
Q-Learning 使用**时序差分 (Temporal Difference, TD)** 学习方法来更新 Q 值。TD 学习是一种**无模型 (Model-free)** 的强化学习方法，**无需事先知道环境的转移概率和奖励函数**，通过**采样**和**迭代**的方式进行学习。
:::

- **更新公式 (无需数学公式，侧重直观理解)**:

    ```
    新的 Q(s, a)  <-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))
    ```

    - **学习率 ($\alpha$)**:  控制每次更新的幅度，取值范围通常为 $[0, 1]$。
        -  **$\alpha$ 较大**:  更新幅度大，学习速度快，但容易**不稳定**，可能**震荡**。
        -  **$\alpha$ 较小**:  更新幅度小，学习速度慢，但**稳定**，收敛性好。
    - **TD 目标 (TD Target)**:  表示我们**期望**的 Q 值，是**对未来累积奖励的估计**。
        -  **TD 目标 =  即时奖励 (Reward)  +  折扣因子 ($\gamma$) *  未来最优 Q 值 (下一状态的最大 Q 值)**
        -  **未来最优 Q 值**:  在下一个状态 $s'$ 下，所有可能动作 $a'$ 中，Q 值最大的那个值，即 $\max_{a'} Q(s', a')$。
    - **折扣因子 ($\gamma$)**:  控制未来奖励的重要性，取值范围通常为 $[0, 1]$。
        -  **$\gamma$ 接近 0**:  更关注**即时奖励**，**短视**。
        -  **$\gamma$ 接近 1**:  更关注**未来奖励**，**有远见**。

- **更新过程**:

    1.  智能体在状态 $s$ 下，根据策略 (例如 $\epsilon$-greedy 策略) 选择动作 $a$。
    2.  智能体执行动作 $a$，环境转移到下一个状态 $s'$，并返回奖励 $r$。
    3.  根据 **Q-Learning 更新规则**，更新 Q-Table 中 $Q(s, a)$ 的值。
    4.  将当前状态更新为 $s'$，重复步骤 1-3，直到 episode 结束。

- **Q-Learning 更新规则图示**:

```{mermaid}
graph LR
    S["状态 s"] --> A["动作 a"]
    A --> E["环境"]
    E --> SP["新状态 s'"]
    E --> R["奖励 r"]
    SP --> QP["最大Q值"]
    R --> TD["TD目标"]
    QP --> TD
    TD --> U["更新Q表"]
```

### 4.  Q-Learning 算法步骤流程

::: {.callout-note}
Q-Learning 算法的步骤流程可以总结为：**初始化 Q-Table**，**循环迭代 episodes**，**在每个 episode 中，循环迭代 steps**，**选择动作**，**执行动作**，**更新 Q-Table**。
:::

1.  **初始化 Q-Table**:  创建一个 Q-Table，**行数为状态空间大小**，**列数为动作空间大小**，**所有 Q 值初始化为 0** (或其他小值)。
2.  **循环迭代 Episodes**:  进行**多次 episodes** 训练，让智能体不断与环境交互，学习优化策略。
    -  **For each episode**:
        a.  **初始化环境**:  重置环境到初始状态 $s$。
        b.  **循环迭代 Steps**:  在每个 episode 中，进行**多步交互**，直到 episode 结束 (例如到达目标状态或达到最大步数)。
            -  **For each step**:
                i.  **选择动作**:  根据当前状态 $s$，使用**探索策略** (例如 $\epsilon$-greedy 策略) 从 Q-Table 中选择一个动作 $a$。
                ii. **执行动作**:  智能体在环境中执行动作 $a$，环境返回**下一个状态 $s'$ 和奖励 $r$**。
                iii. **更新 Q-Table**:  使用 **Q-Learning 更新规则**，根据 $(s, a, r, s')$ 更新 Q-Table 中 $Q(s, a)$ 的值。
                iv. **更新状态**:  将当前状态更新为 $s'，s \leftarrow s'$。
                v.  **判断 Episode 结束**:  判断是否到达终止状态 (terminated) 或截断状态 (truncated)，如果 episode 结束，则跳出 step 循环。
3.  **训练结束**:  当 Q-Table 收敛 (Q 值变化很小) 或者达到预设的训练 episodes 数量时，训练结束。
4.  **策略提取**:  训练结束后，可以从 Q-Table 中提取最优策略。对于每个状态 $s$，最优策略 $\pi^*(s)$ 是选择能使 Q 值 $Q^*(s, a)$ 最大的动作 $a$。

### 5.  动态定价案例 (结合商业案例，演示 Q-Learning 应用)

::: {.callout-note}
动态定价是一种**根据市场供需变化**，**实时调整商品或服务价格**的定价策略。强化学习可以用于**学习最优的动态定价策略**，以最大化收益或利润。
:::

#### 5.1  动态定价场景简化

- **场景**:  在线零售平台，销售**单一商品** (例如：某品牌的热门手机)。
- **状态 (State)**:  **商品库存水平** (例如：高库存、中库存、低库存)。
- **动作 (Action)**:  **价格调整** (例如：涨价、降价、维持原价)。
- **奖励 (Reward)**:  **销售利润** (例如：销售额 - 成本)。
- **目标**:  **最大化**一段时间内的**累积利润**。

#### 5.2  Q-Learning 动态定价步骤 (简化版)

1.  **状态空间**:  假设商品库存水平分为 3 个状态：`高库存 (High)`，`中库存 (Medium)`，`低库存 (Low)`。
2.  **动作空间**:  假设价格调整分为 3 个动作：`涨价 (Increase)`，`降价 (Decrease)`，`维持原价 (Maintain)`。
3.  **Q-Table**:  创建一个 $3 \times 3$ 的 Q-Table，初始化为 0。
4.  **奖励函数**:  假设奖励函数如下 (简化示例)：

    | 状态 (库存) | 动作 (价格调整) | 奖励 (利润) |
    |-------------|-----------------|-------------|
    | 高库存      | 降价            | +5 (销量增加) |
    | 高库存      | 维持原价        | +2 (正常销量) |
    | 高库存      | 涨价            | -1 (销量减少) |
    | 中库存      | 降价            | +3 (销量略增) |
    | 中库存      | 维持原价        | +4 (正常销量) |
    | 中库存      | 涨价            | +1 (销量略减) |
    | 低库存      | 降价            | -2 (缺货风险) |
    | 低库存      | 维持原价        | +6 (高利润率) |
    | 低库存      | 涨价            | +8 (更高利润率) |

    **注意**:  这只是一个简化的奖励函数示例，实际应用中奖励函数会更复杂，需要根据具体业务场景进行设计。

5.  **Q-Learning 训练**:  进行 episodes 训练，使用 $\epsilon$-greedy 策略选择动作，并根据奖励函数和 Q-Learning 更新规则更新 Q-Table。
6.  **最优策略**:  训练结束后，Q-Table 会学习到最优的动态定价策略。例如，可能学习到：

    -  **高库存**:  应该 **降价** 以快速清理库存。
    -  **中库存**:  应该 **维持原价** 以获得稳定利润。
    -  **低库存**:  可以 **涨价** 以提高利润率 (但需注意缺货风险)。

#### 5.3  AI 辅助编程演示 (使用 Python 和 AI 工具，演示动态定价代码)

-  **可以使用 Python 模拟动态定价环境** (状态、动作、奖励函数等)。
-  **使用 AI 辅助编程工具 (例如 GitHub Copilot)**，辅助编写 Q-Learning 算法代码，应用于动态定价问题。
-  **演示代码** (伪代码示例，仅供参考)：

    ```python
    # 初始化 Q-Table (字典或 NumPy 数组)
    q_table = {} #  状态为 (库存状态)，动作为 (价格调整动作)

    # 定义状态空间和动作空间 (例如使用枚举类型)
    states = ["High", "Medium", "Low"]
    actions = ["Increase", "Maintain", "Decrease"]

    # 定义奖励函数 (根据状态和动作返回奖励值)
    def get_reward(state, action):
        # ... (根据奖励函数表格返回奖励值)
        pass

    # Q-Learning 算法训练循环
    episodes = 1000
    learning_rate = 0.1
    discount_factor = 0.9
    epsilon = 0.1 #  epsilon-greedy 策略的探索率

    for episode in range(episodes):
        # 初始化状态 (随机选择初始库存状态)
        current_state = random.choice(states)

        for step in range(steps_per_episode): #  每个 episode 的步数
            # 使用 epsilon-greedy 策略选择动作
            if random.uniform(0, 1) < epsilon:
                action = random.choice(actions) #  探索：随机选择动作
            else:
                # 利用：选择 Q 值最大的动作
                action = max(actions, key=lambda a: q_table.get((current_state, a), 0))

            # 执行动作，获取下一个状态和奖励 (模拟环境交互)
            next_state = ... #  根据当前状态和动作，模拟状态转移
            reward = get_reward(current_state, action)

            # Q-Learning 更新规则
            old_q_value = q_table.get((current_state, action), 0)
            next_max_q = max([q_table.get((next_state, a), 0) for a in actions]) #  下一个状态的最大 Q 值
            new_q_value = old_q_value + learning_rate * (reward + discount_factor * next_max_q - old_q_value)
            q_table[(current_state, action)] = new_q_value #  更新 Q-Table

            # 更新状态
            current_state = next_state

    # 训练结束，输出学习到的 Q-Table 和最优策略
    print("Q-Table:")
    print(q_table)

    #  提取最优策略 (对于每个状态，选择 Q 值最大的动作)
    optimal_policy = {}
    for state in states:
        optimal_action = max(actions, key=lambda a: q_table.get((state, a), 0))
        optimal_policy[state] = optimal_action
    print("\nOptimal Policy:")
    print(optimal_policy)
    ```

    **注意**:  这只是一个非常简化的动态定价示例，用于演示 Q-Learning 的基本应用思路。实际商业场景中的动态定价问题会更加复杂，需要考虑更多因素 (例如：竞争对手价格、季节性因素、促销活动等)，并使用更复杂的强化学习算法。

## 第二次课：小组项目一：Q-Learning 算法编程实践

### 1.  小组项目一：Q-Learning 算法编程实践 (迷宫寻宝 Grid World)

-  **项目目标**:
    -  以小组为单位，**使用 Python 和 AI 工具**，**编写 Q-Learning 算法代码**，应用于**迷宫寻宝 (Grid World) 项目**。
    -  **实现 Q-Learning 智能体**，使其能够在迷宫环境中**自主探索**，并**学习找到宝藏的最优路径**。
    -  **可视化智能体在迷宫中的探索过程** (例如：绘制智能体路径、Q-Table 热力图等，**可选**)。

-  **代码框架**:
    -  可以使用**第一次课提供的 Grid World 环境代码** (或者小组自行搭建的 Grid World 环境)。
    -  小组需要**自行编写 Q-Learning 算法代码**，并**与 Grid World 环境进行集成**。

-  **AI 辅助工具**:
    -  **鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具**，提高开发效率。
    -  **但强调**:  AI 工具是辅助手段，学生需要理解 Q-Learning 算法原理和代码逻辑，**不能完全依赖** AI 工具生成代码，而忽略算法理解和调试。

-  **项目提交**:
    -  小组**提交**完整的 Q-Learning 算法代码 (Python 文件)，以及**修改后的 Grid World 环境代码** (如果环境代码有修改)。
    -  **无需提交**项目报告。

### 2.  超参数讲解与调整 (学习率、折扣因子)

::: {.callout-note}
超参数 (Hyperparameters) 是强化学习算法中需要**手动设置的参数**，例如学习率 ($\alpha$)、折扣因子 ($\gamma$)、探索率 ($\epsilon$) 等。**超参数的选择会直接影响算法的性能和收敛速度**。
:::

- **常用超参数**:
    - **学习率 ($\alpha$)**:  已在第一次课中讲解。
    - **折扣因子 ($\gamma$)**:  已在第一次课中讲解。
    - **探索率 ($\epsilon$)**:  $\epsilon$-greedy 策略中的探索概率，已在第二次课中讲解。

- **超参数调整**:
    -  **经验调整**:  根据经验和直觉进行调整 (trial-and-error)。
    -  **网格搜索 (Grid Search)**:  在超参数空间中，**预先定义一组候选值**，**遍历所有可能的组合**，**训练模型并评估性能**，选择性能最佳的超参数组合。
    -  **随机搜索 (Random Search)**:  在超参数空间中，**随机采样**一定数量的超参数组合，**训练模型并评估性能**，选择性能最佳的超参数组合。
    -  **自动化超参数优化方法**:  例如 Bayesian Optimization, Hyperband 等 (更高级的方法，本课程不深入讲解)。

- **超参数调整建议 (针对 Q-Learning 和 Grid World)**:
    -  **学习率 ($\alpha$)**:  可以尝试  `0.1, 0.3, 0.5, 0.7`  等值。
    -  **折扣因子 ($\gamma$)**:  可以尝试  `0.8, 0.9, 0.95, 0.99`  等值。
    -  **探索率 ($\epsilon$)**:  初始值可以设置为 `1.0` (完全探索)，然后**逐渐衰减**到较小的值 (例如 `0.1` 或 `0.01`)。衰减方式可以是**线性衰减**、**指数衰减**等。

- **超参数调整实践**:
    -  **在 Q-Learning 代码中，将超参数设置为可调节的变量**。
    -  **尝试不同的超参数组合**，观察智能体在 Grid World 环境中的表现 (例如：是否能更快找到宝藏，是否能避免陷阱，平均 episode 奖励等)。
    -  **记录实验结果**，分析超参数对算法性能的影响。

### 3.  小组项目一检查点：Q-Learning 智能体探索

-  **检查点目标**:  **确保学生小组能够运行基本的 Q-Learning 智能体**，**在迷宫环境 (Grid World) 中进行探索**。
-  **检查内容**:
    -  **Q-Learning 算法代码是否能够正常运行**，**没有明显的 bug**。
    -  **智能体是否能够在迷宫环境中移动**，**并与环境进行交互**。
    -  **Q-Table 是否能够正常更新** (可以通过打印 Q-Table 或 Q 值变化来观察)。
    -  **智能体是否能够进行初步的探索** (例如：在迷宫中随机移动，尝试不同的路径)。
    -  **不需要智能体达到最优性能**，**重点是代码能够跑起来，并且智能体能够进行基本的探索**。

-  **检查方式**:
    -  **小组演示**:  每个小组**简单演示** Q-Learning 智能体在 Grid World 环境中的运行情况。
    -  **代码 review (可选)**:  教师可以**抽查部分小组的代码**，进行简单的代码 review，**指出潜在问题和改进方向**。
    -  **答疑**:  解答学生在 Q-Learning 算法编程和环境集成过程中遇到的问题。

### 课后作业

1.  **完成小组项目一：Q-Learning 算法编程实践**，实现 Q-Learning 智能体在迷宫环境中寻宝。
2.  **调整 Q-Learning 算法的超参数** (学习率、折扣因子、探索率等)，观察超参数对智能体性能的影响，并尝试找到一组较好的超参数组合。
3.  **思考题**:
    -  Q-Learning 算法有什么优点和缺点？
    -  Q-Learning 算法适用于什么类型的问题？有什么局限性？
    -  如何改进 Q-Learning 算法，以提高其性能和适用范围？ (例如：Double Q-Learning, Prioritized Experience Replay 等，可以查阅资料了解)

### 预习资料

1.  **阅读材料**:
    -  Q-Learning 算法的改进版本：Double Q-Learning, Dueling Q-Network 等 (初步了解思想)。
    -  ε-greedy 退火策略、UCB 算法等更高级的探索策略。
    -  动态定价的更多商业应用案例。

2.  **视频资源**:
    -  Q-Learning 算法代码实现详解 (Python)。
    -  超参数调整技巧和经验分享。
    -  强化学习算法的探索策略进阶。

3.  **下周预习重点**:
    -  Q-Learning 算法的优化和改进方向。
    -  探索策略的进一步探讨 (例如 ε-greedy 退火策略)。
    -  Q-Table 初始化、奖励函数设计等实用技巧。
    -  小组项目一提交和优秀项目讲解准备。

---

**请注意**:

-   **代码框架**:  可以使用第二次课提供的 Grid World 环境代码，或者小组自行搭建的环境。重点是**自行编写 Q-Learning 算法代码**，并与环境集成。
-   **AI 辅助工具**:  鼓励使用 AI 工具，但务必强调理解算法原理和代码逻辑的重要性，避免过度依赖 AI 工具。
-   **小组项目**:  小组项目一旨在让学生**实践 Q-Learning 算法**，**解决迷宫寻宝问题**，并**初步了解超参数调整**。为后续更复杂的强化学习算法和项目打下基础。
-   **检查点**:  本次课设有小组项目一检查点，旨在**及时发现学生在编程实践中遇到的问题**，并提供**指导和帮助**，确保所有小组都能顺利进行后续的学习和项目。
