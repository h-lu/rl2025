---
title: "第三周：Q-Learning 算法详解与实践"
---

::: {.callout-tip appearance="simple"}
## 本周学习目标
- 理解 Q-Learning 算法的核心思想和原理
- 掌握 Q-Table 的概念和更新规则
- 学习使用 Q-Learning 算法解决迷宫寻宝问题
- 了解超参数的作用，并进行简单调整
- 掌握 Q-Learning 算法的基本编程实现
:::

## 第一次课：Q-Learning 算法详解

::: {.callout-important}
## Q-Learning 算法简介
Q-Learning 是一种基于价值的离线强化学习算法，用于学习最优 Q 函数，从而得到最优策略。

### 核心特点
- **核心思想**: 通过不断试错和更新 Q-Table，逐步逼近最优 Q 函数
- **离线特性**: 学习的策略与实际执行的策略可以不同
- **最优 Q 函数**: 表示在状态 $s$ 下，执行动作 $a$ 后的最大期望累积奖励
- **最优策略**: 在每个状态下选择 Q 函数值最大的动作
:::

::: {.callout-note}
## Q-Table 详解
Q-Table 是 Q-Learning 算法中用于存储 Q 函数值的表格。

### 表格结构
| 状态 (State) | 动作 1 (Action 1) | 动作 2 (Action 2) | ... | 动作 n (Action n) |
|------------|--------------------|--------------------|-----|--------------------|
| 状态 1 (S1) | $Q(S1, A1)$       | $Q(S1, A2)$       | ... | $Q(S1, An)$       |
| 状态 2 (S2) | $Q(S2, A1)$       | $Q(S2, A2)$       | ... | $Q(S2, An)$       |
| ...        | ...                | ...                | ... | ...                |
| 状态 m (Sm) | $Q(Sm, A1)$       | $Q(Sm, A2)$       | ... | $Q(Sm, An)$       |

### 关键操作
- **初始化**: 通常初始化为 0 或小的随机值
- **更新**: 在交互过程中不断更新 Q 值
- **查询**: 根据当前状态查询对应的 Q 值
:::

::: {.callout-warning}
## Q-Learning 更新规则
Q-Learning 使用时序差分学习方法来更新 Q 值，是一种无模型的强化学习方法。

### 更新公式
```
新的 Q(s, a)  <-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))
```

### 关键参数
- **学习率 ($\alpha$)**: 控制更新幅度 [0, 1]
  - $\alpha$ 较大：学习快但不稳定
  - $\alpha$ 较小：学习慢但稳定

- **折扣因子 ($\gamma$)**: 控制未来奖励重要性 [0, 1]
  - $\gamma$ 接近 0：重视即时奖励
  - $\gamma$ 接近 1：重视未来奖励
:::

::: {.callout-tip}
## 算法流程图
```{mermaid}
graph LR
    S["状态 s"] --> A["动作 a"]
    A --> E["环境"]
    E --> SP["新状态 s'"]
    E --> R["奖励 r"]
    SP --> QP["最大Q值"]
    R --> TD["TD目标"]
    QP --> TD
    TD --> U["更新Q表"]
```
:::

::: {.callout-important}
## 算法步骤详解

1. **初始化 Q-Table**
   - 创建状态-动作表格
   - 初始化所有 Q 值为 0

2. **循环训练 Episodes**
   - 重置环境到初始状态
   - 在每个 episode 中进行多步交互
   - 直到达到目标或最大步数

3. **每步操作**
   - 选择动作（探索策略）
   - 执行动作获取反馈
   - 更新 Q-Table
   - 更新当前状态

4. **训练完成**
   - Q-Table 收敛或达到预设次数
   - 提取最优策略
:::

::: {.callout-note}
## 动态定价案例

### 场景设定
- **商品**: 单一商品销售
- **状态**: 库存水平（高/中/低）
- **动作**: 价格调整（涨/跌/维持）
- **目标**: 最大化累积利润

### 奖励设计
| 状态 (库存) | 动作 (价格调整) | 奖励 (利润) |
|-------------|-----------------|-------------|
| 高库存      | 降价            | +5 (销量增加) |
| 高库存      | 维持原价        | +2 (正常销量) |
| 高库存      | 涨价            | -1 (销量减少) |
| 中库存      | 降价            | +3 (销量略增) |
| 中库存      | 维持原价        | +4 (正常销量) |
| 中库存      | 涨价            | +1 (销量略减) |
| 低库存      | 降价            | -2 (缺货风险) |
| 低库存      | 维持原价        | +6 (高利润率) |
| 低库存      | 涨价            | +8 (更高利润率) |
:::

::: {.callout-tip}
## 动态定价代码示例
```python
# 初始化 Q-Table
q_table = {}

# 定义状态空间和动作空间
states = ["High", "Medium", "Low"]
actions = ["Increase", "Maintain", "Decrease"]

# Q-Learning 算法训练循环
episodes = 1000
learning_rate = 0.1
discount_factor = 0.9
epsilon = 0.1

for episode in range(episodes):
    current_state = random.choice(states)
    
    for step in range(steps_per_episode):
        # epsilon-greedy 策略选择动作
        if random.uniform(0, 1) < epsilon:
            action = random.choice(actions)
        else:
            action = max(actions, key=lambda a: q_table.get((current_state, a), 0))
            
        # 获取奖励和下一状态
        next_state = ...  # 状态转移
        reward = get_reward(current_state, action)
        
        # 更新 Q 值
        old_q = q_table.get((current_state, action), 0)
        next_max_q = max([q_table.get((next_state, a), 0) for a in actions])
        new_q = old_q + learning_rate * (reward + discount_factor * next_max_q - old_q)
        q_table[(current_state, action)] = new_q
        
        current_state = next_state
```
:::

## 第二次课：小组项目一：Q-Learning 算法编程实践

::: {.callout-important}
## 项目要求

### 目标
- 使用 Python 和 AI 工具编写 Q-Learning 算法
- 应用于迷宫寻宝 (Grid World) 项目
- 实现智能体自主探索和学习

### 提交内容
- 完整的 Q-Learning 算法代码
- 修改后的 Grid World 环境代码（如有）
- 可视化结果（可选）
:::

::: {.callout-warning}
## 超参数调整指南

### 关键超参数
- 学习率 ($\alpha$)
- 折扣因子 ($\gamma$)
- 探索率 ($\epsilon$)

### 调整方法
1. **经验调整**
   - 基于经验和直觉
   - 快速但可能不是最优

2. **网格搜索**
   - 遍历预定义参数组合
   - 全面但耗时

3. **随机搜索**
   - 随机采样参数组合
   - 平衡效率和效果
:::

::: {.callout-note}
## 课后作业
1. 完成 Q-Learning 算法实现
2. 调整超参数并记录效果
3. 分析不同参数对学习效果的影响
4. 尝试改进算法性能
:::

::: {.callout-tip}
## 预习资料
1. 阅读材料
   - Sutton & Barto 第6章
   - Q-Learning 相关论文
   
2. 视频资源
   - David Silver RL Course
   - Q-Learning 实现教程

3. 下周预习重点
   - DQN 算法原理
   - 神经网络基础
   - PyTorch 入门
:::
