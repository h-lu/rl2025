{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二次课：强化学习框架与基本概念 (Jupyter Notebook 版 - 详细版)\n",
    "\n",
    "**课程目标:**\n",
    "\n",
    "*   **深入理解**马尔可夫决策过程 (MDP) 的核心思想，并能将其应用于 CartPole 环境的分析。\n",
    "*   **掌握**策略 (Policy) 的概念，理解策略在强化学习中的作用。\n",
    "*   **学习并应用 epsilon 贪婪策略**，平衡探索与利用，并理解其背后的原理。\n",
    "*   **初步了解价值函数 (Value Function) 的概念**，并使用价值函数的思想评估策略的性能。\n",
    "*   **学习利用 AI 工具 (如 ChatGPT) 辅助设计更复杂的 CartPole 控制策略**。\n",
    "*   **通过实验比较不同策略的性能**，并分析实验结果。\n",
    "\n",
    "**实验环境:**\n",
    "\n",
    "本节课将继续使用 `gymnasium` 库的 `CartPole-v1` 环境。  请确保您已经安装了 `gymnasium` 和 `numpy` 库。\n",
    "\n",
    "```bash\n",
    "pip install gymnasium numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 强化学习的“游戏规则”：马尔可夫决策过程 (MDP) - 详细讲解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 什么是马尔可夫决策过程 (MDP)？ (更详细的解释)\n",
    "\n",
    "**想象一下你在玩一个游戏**，比如超级马里奥。\n",
    "\n",
    "*   **游戏世界** 就是 **环境 (Environment)**。\n",
    "*   **你 (玩家)** 就是 **智能体 (Agent)**。\n",
    "*   **游戏画面** 就是 **状态 (State)**，它告诉你当前游戏的进展情况，例如马里奥的位置、敌人的位置、剩余时间等等。\n",
    "*   **你可以按的按钮 (跳跃、奔跑、射击)** 就是 **动作 (Action)**。  你的每一个动作都会改变游戏的状态。\n",
    "*   **游戏得分** 就是 **奖励 (Reward)**。  你击败敌人、吃到金币、到达终点都会获得奖励，奖励会告诉你你的行为是好还是坏。\n",
    "\n",
    "**强化学习就像训练一个游戏 AI**，让 AI 像你一样玩游戏，但 AI 的目标是**尽可能获得更高的游戏得分 (累积奖励)**。\n",
    "\n",
    "**马尔可夫决策过程 (Markov Decision Process, MDP) 就是用来描述这种“玩游戏”过程的数学框架。**  它提供了一套精确的语言和规则，让我们能够形式化地描述强化学习问题，并设计算法来解决这些问题。\n",
    "\n",
    "**MDP 的核心思想:**  智能体与环境进行一系列的**交互**，在每一步交互中，智能体：\n",
    "\n",
    "1.  **观察 (Observation):**  观察环境的当前状态。\n",
    "2.  **选择动作 (Action):**  根据当前状态，选择一个动作。\n",
    "3.  **获得反馈 (Feedback):**  执行动作后，环境会转移到新的状态，并返回一个奖励给智能体。\n",
    "\n",
    "智能体的目标是学习一个**策略 (Policy)**，这个策略就像一本“行动指南”，告诉智能体在**每个状态下应该选择哪个动作**，从而**最大化整个游戏过程中的累积奖励**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 马尔可夫性质 (Markov Property) - 核心假设 (详细解释)\n",
    "\n",
    "**马尔可夫性质是 MDP 框架中一个非常重要的假设。**  它指的是：\n",
    "\n",
    "**“未来只取决于现在，与过去无关。”**\n",
    "\n",
    "**更具体地说：**  在任何一个时间点，环境的 **状态** 包含了所有对 **未来** 发展 *有影响* 的信息。  也就是说，要预测环境的下一个状态，或者智能体能获得的未来奖励，我们只需要知道 **当前的状态** 就足够了，不需要回顾过去发生了什么。\n",
    "\n",
    "**再打个比方：**  假设你在玩扑克牌。  你当前手里的牌，以及牌桌上已经亮出的公共牌，就构成了当前的状态。  你做决策 (例如，是否跟注、加注) 时，只需要考虑 *当前* 的牌面情况，而不需要记住之前发了哪些牌，或者之前你做过什么操作。  因为所有 *相关* 的历史信息都 *已经* 反映在 *当前* 的牌面状态中了。\n",
    "\n",
    "**马尔可夫性质在强化学习中有什么用？ (核心作用)**\n",
    "\n",
    "马尔可夫性质最大的作用是 **简化问题**。  它让我们可以把复杂的强化学习问题分解成一个个独立的、只与当前状态相关的决策步骤。  这极大地简化了策略的设计和算法的开发。  具体来说，马尔可夫性质有以下几个关键作用：\n",
    "\n",
    "*   **简化状态表示：**  有了马尔可夫性质，我们只需要关注能够完整描述当前环境状态的 **最少信息**。  例如，在 CartPole 中，我们只需要知道小车的位置、速度、杆的角度和角速度这四个量就足够了，因为这四个量已经包含了所有影响未来状态的信息。  我们不需要记录过去多少秒施加了多大的力，或者杆子之前摆动了多少次等等。  这大大降低了状态空间的维度，使得问题更容易处理。\n",
    "\n",
    "*   **简化策略设计：**  策略 (Policy) 是智能体的“行动指南”，它告诉智能体在 *每个状态* 下应该选择哪个动作。  由于马尔可夫性质，策略的设计可以变得非常简洁：策略只需要是一个 **从状态到动作的映射**。  也就是说，策略只需要考虑 *当前状态*，然后根据当前状态来决定动作。  我们不需要设计复杂的策略，让它去记忆过去的历史，或者根据历史来调整当前的动作。\n",
    "\n",
    "*   **简化算法开发：**  很多强化学习算法 (例如，Q-Learning, SARSA, Policy Gradient 等) 都 **依赖于马尔可夫性质**。  这些算法的核心思想是基于 **动态规划** 或 **时序差分学习**，而这些方法都要求环境具有马尔可夫性质才能保证算法的收敛性和有效性。  如果环境不满足马尔可夫性质，这些算法可能就无法直接应用，或者需要进行复杂的修改才能使用。\n",
    "\n",
    "*   **价值函数和贝尔曼方程：**  马尔可夫性质是 **价值函数** 和 **贝尔曼方程** 成立的基础。  价值函数用于评估状态或动作的“好坏”，而贝尔曼方程则描述了价值函数在不同状态之间的递推关系。  这些都是强化学习理论和算法的核心工具，它们都依赖于马尔可夫性质。\n",
    "\n",
    "**如果没有马尔可夫性质会怎样？ (对比，突出重要性)**\n",
    "\n",
    "想象一下，如果 CartPole 环境 **不满足马尔可夫性质**，会发生什么？  例如，假设环境的下一个状态不仅取决于当前的状态和动作，还取决于 **过去一段时间内施加的力的历史**。\n",
    "\n",
    "*   **状态变得非常复杂：**  为了预测未来，智能体不仅需要知道当前的小车位置、速度、杆的角度和角速度，还需要记住过去一段时间内施加的力的序列。  状态空间会变得非常庞大，甚至可能是无限维的。\n",
    "\n",
    "*   **策略设计变得极其困难：**  策略不能仅仅是一个从当前状态到动作的映射，而需要是一个 **从历史状态序列到动作的映射**。  策略的设计会变得非常复杂，难以表示和学习。\n",
    "\n",
    "*   **算法开发面临挑战：**  很多基于动态规划和时序差分学习的算法可能不再适用，或者需要进行非常复杂的修改。  我们需要开发新的算法来处理非马尔可夫环境下的强化学习问题。\n",
    "\n",
    "**CartPole 环境的马尔可夫性质**\n",
    "\n",
    "在 CartPole 环境中，我们通常 **假设它是满足马尔可夫性质的**。  也就是说，我们认为，只要我们知道当前的小车位置、速度、杆的角度和角速度，就足以预测 CartPole 未来的状态和奖励，而不需要回顾过去的历史。  这个假设在一定程度上是合理的，也使得我们可以使用各种基于 MDP 的强化学习算法来解决 CartPole 问题。\n",
    "\n",
    "**总结：马尔可夫性质的重要性**\n",
    "\n",
    "*   **简化问题：**  马尔可夫性质让我们可以把复杂的强化学习问题分解成一系列独立的、只与当前状态相关的决策步骤，极大地简化了问题。\n",
    "*   **支撑理论和算法：**  马尔可夫性质是强化学习理论 (价值函数、贝尔曼方程) 和算法 (Q-Learning, Policy Gradient 等) 的基础。\n",
    "*   **降低复杂性：**  马尔可夫性质降低了状态表示、策略设计和算法开发的复杂性，使得强化学习问题更容易处理和求解。\n",
    "\n",
    "---\n",
    "\n",
    "**哪些游戏满足马尔可夫性质？哪些不满足？ (例子分析)**\n",
    "\n",
    "为了更好地区分哪些环境满足或不满足马尔可夫性质，我们来看一些具体的例子：\n",
    "\n",
    "**1. 扑克牌 (例如德州扑克):**\n",
    "\n",
    "**在一定程度上，扑克牌可以被近似看作是马尔可夫的，但严格来说，它并不完全满足马尔可夫性质。**  这取决于我们如何定义 \"状态\"。\n",
    "\n",
    "*   **如果我们将 \"状态\" 定义为：**  **当前牌局的信息，包括你手里的牌、公共牌、当前的下注轮数、已经下注的金额、以及其他玩家的公开行为 (例如，是否弃牌、加注等)。**  在这种情况下，在 *同一局牌* 中，未来的发展很大程度上取决于 *当前* 的牌面和局势。  你不需要记住之前几轮发了什么牌 (除非是为了统计发牌概率，但这通常是高级策略)。  **在单局牌的范围内，可以近似认为是马尔可夫的。**\n",
    "\n",
    "*   **但是，如果我们将 \"状态\" 定义得更狭隘，例如只包括你手里的牌和公共牌，而忽略了下注历史和对手的行为，那么马尔可夫性质就不成立了。**  因为对手之前的下注模式、激进程度、诈唬频率等信息，会影响你对他们 *当前* 行为的判断，以及你 *未来* 的决策。  此外，如果你考虑 **多局牌** 的情况，对手可能会根据你 *过去* 的打法来调整策略，这时历史信息就变得非常重要了。\n",
    "\n",
    "**总结：**  对于扑克牌，特别是德州扑克，**在单局牌的范围内，如果我们把足够多的当前信息 (包括牌面和当前的下注状态) 纳入 \"状态\" 的定义，可以近似地看作是马尔可夫的。**  但如果考虑更长期的游戏，或者更精细的策略，历史信息 (对手的行为模式) 就变得重要，这时就不再严格满足马尔可夫性质。  高级扑克 AI 通常会使用记忆机制来追踪对手的行为模式，这实际上是在一定程度上克服马尔可夫性质的局限性。\n",
    "\n",
    "---\n",
    "\n",
    "**2. 俄罗斯方块:**\n",
    "\n",
    "**俄罗斯方块在很大程度上可以被认为是满足马尔可夫性质的。**\n",
    "\n",
    "*   **状态：**  俄罗斯方块的状态主要由 **当前的游戏面板 (已经堆积的方块)** 和 **即将掉落的方块形状** 决定。\n",
    "\n",
    "*   **动作：**  玩家可以做的动作是旋转和水平移动当前掉落的方块。\n",
    "\n",
    "*   **马尔可夫性质：**  你为了决定 *当前* 方块的最佳放置位置，主要依赖于 **当前的面板布局** 和 **即将到来的方块形状**。  你不需要知道 *之前* 掉落了哪些方块序列。  虽然 *之前* 的方块堆积形成了 *当前* 的面板布局，但关键是 *当前* 的面板布局已经包含了所有与 *未来* 决策相关的信息。  即将到来的方块形状也是独立的，只取决于随机生成器，与之前的方块序列无关 (通常是这样设计的)。\n",
    "\n",
    "**总结：**  俄罗斯方块是一个很好的例子，它在很大程度上满足马尔可夫性质。  你的决策只需要基于当前的游戏状态和即将到来的信息。\n",
    "\n",
    "---\n",
    "\n",
    "**3. 星际争霸 (StarCraft) 和 英雄联盟 (League of Legends):**\n",
    "\n",
    "**星际争霸和英雄联盟都是典型的 不满足马尔可夫性质 的复杂游戏。**  它们是实时战略 (RTS) 和多人在线竞技 (MOBA) 游戏，具有以下特点，导致马尔可夫性质不成立：\n",
    "\n",
    "*   **信息不完全 (Fog of War):**  玩家无法看到地图上的所有区域，敌方单位的位置和行动是隐藏的。  你需要通过侦察等手段来获取信息。  **过去的侦察信息和经验会影响你对当前局势的判断和未来的决策。**  例如，如果你之前侦察到敌方正在建造大量兵营，你现在即使没有直接看到敌方单位，也需要考虑到敌方可能即将发动进攻。\n",
    "\n",
    "*   **长期战略和规划:**  这些游戏不仅仅是即时反应，还需要制定长期的战略和规划，例如经济发展、科技升级、兵种组合、地图控制等等。  **你当前的决策会受到你过去战略选择的影响，也会影响你未来的发展。**  例如，你早期选择了快速攀科技，那么你现在的兵种结构和经济状况就与这个早期决策有关。\n",
    "\n",
    "*   **对手适应性:**  在多人游戏中，对手会根据你的打法和策略进行学习和调整。  **你过去的行动会影响对手未来的行为，从而间接地影响你未来的状态。**  例如，如果你连续几局都使用 Rush 战术，对手可能会在下一局采取更保守的防御策略。\n",
    "\n",
    "*   **隐藏状态:**  除了信息不完全，还存在一些隐藏状态，例如对手的意图、心理状态、以及一些无法直接观察到的内部参数。  这些隐藏状态会受到 *过去* 事件的影响。\n",
    "\n",
    "**总结：**  星际争霸和英雄联盟这类复杂游戏，由于信息不完全、长期战略、对手适应性等因素， **严重违反了马尔可夫性质。**  为了在这些游戏中取得好的表现，AI 需要具备记忆能力，能够追踪历史信息，并进行长期规划和对手建模。  强化学习在这些游戏中面临的挑战也更大，需要更复杂的算法和模型来处理非马尔可夫性。\n",
    "\n",
    "---\n",
    "\n",
    "**4. 足球和篮球 (真实体育运动):**\n",
    "\n",
    "**真实的足球和篮球比赛，也 严格来说不满足马尔可夫性质。**  原因与星际争霸和英雄联盟有些类似，但更复杂：\n",
    "\n",
    "*   **连续时间:**  体育运动是连续时间的，而 MDP 通常是离散时间的。  虽然可以进行离散化近似，但会损失一些信息。\n",
    "\n",
    "*   **物理因素和惯性:**  球员的运动、球的轨迹都受到物理规律的约束，具有惯性。  **当前的状态不仅取决于上一时刻的状态和动作，还受到更早之前的状态和动作的影响。**  例如，球员的奔跑速度和方向，不可能瞬间改变，会受到之前的运动状态的影响。\n",
    "\n",
    "*   **球员状态 (疲劳、士气、心理):**  球员的状态 (例如，疲劳程度、士气高低、心理状态) 会随着比赛的进行而变化，并受到 *过去* 比赛事件的影响。  例如，连续失误可能会导致球员士气下降，影响后续表现。\n",
    "\n",
    "*   **战略和战术调整:**  教练和球员会根据比赛的进程和对手的表现，不断调整战略和战术。  **当前的战术选择会受到 *过去* 比赛进程和对手行为的影响。**  例如，如果上半场进攻效果不佳，下半场可能会调整进攻策略。\n",
    "\n",
    "*   **随机性和偶然性:**  体育比赛中存在很多随机性和偶然性因素，例如天气、场地条件、裁判判罚、球员临场发挥等等。  这些因素可能受到一些 *历史* 因素的微弱影响，但更多的是不可预测的。\n",
    "\n",
    "**总结：**  真实的足球和篮球比赛，由于连续时间、物理因素、球员状态、战略调整、随机性等复杂因素， **严格来说不满足马尔可夫性质。**  虽然在某些简化模型中，为了分析或模拟方便，可能会进行马尔可夫近似，但在真实比赛中，历史信息和长期依赖性是不可忽视的。  体育运动的 AI 研究也面临着处理非马尔可夫性的挑战。\n",
    "\n",
    "---\n",
    "\n",
    "**总而言之：**\n",
    "\n",
    "*   **理想的 MDP 环境 (例如 CartPole, 迷宫寻宝):**  通常是人为设计的，为了简化问题，会刻意使其满足或近似满足马尔可夫性质。\n",
    "*   **真实世界和复杂游戏:**  大多数真实世界的问题和复杂游戏 (例如，扑克牌、RTS/MOBA 游戏、体育运动)  **本质上都是非马尔可夫的，或者只是近似马尔可夫的。**  在这些情况下，我们需要使用更高级的强化学习方法，或者结合其他技术 (例如，记忆机制、模型预测、分层强化学习等) 来处理非马尔可夫性带来的挑战。\n",
    "\n",
    "理解马尔可夫性质的局限性，以及哪些场景下马尔可夫性质成立或不成立，对于选择合适的强化学习方法和解决实际问题至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 MDP 的关键要素 (回顾与总结)\n",
    "\n",
    "现在我们已经理解了 MDP 的核心思想和马尔可夫性质。  让我们再次回顾一下 MDP 的三个关键要素，并结合 CartPole 例子来加深理解：\n",
    "\n",
    "*   **状态 (State) -  环境的“当前情况”**\n",
    "    *   **定义:**  状态 $s$ 是对环境当前情况的描述，它包含了智能体做出决策所需的所有信息 (在满足马尔可夫性质的假设下)。\n",
    "    *   **例子 (CartPole):**  CartPole 环境的状态通常由 4 个变量组成：\n",
    "        *   小车位置 (cart position)\n",
    "        *   小车速度 (cart velocity)\n",
    "        *   杆的角度 (pole angle)\n",
    "        *   杆的角速度 (pole angular velocity)\n",
    "    *   **状态空间 (State Space):**  所有可能状态的集合。  CartPole 的状态空间是连续的 (因为位置、速度、角度、角速度都是连续值)。\n",
    "    *   **重要性:**  状态是智能体感知环境的基础，也是智能体做出决策的依据。\n",
    "\n",
    "*   **动作 (Action) -  智能体可以“做”的事情**\n",
    "    *   **定义:**  动作 $a$ 是智能体在环境中可以执行的操作。\n",
    "    *   **例子 (CartPole):**  CartPole 环境的动作空间是离散的，智能体只有两个动作可以选择：\n",
    "        *   向左推动小车 (action 0)\n",
    "        *   向右推动小车 (action 1)\n",
    "    *   **动作空间 (Action Space):**  所有可能动作的集合。  CartPole 的动作空间是离散的，且大小为 2。\n",
    "    *   **重要性:**  动作是智能体与环境交互的手段，通过执行动作，智能体可以改变环境的状态，并获得奖励。\n",
    "\n",
    "*   **奖励 (Reward) -  环境对智能体行为的“评价”**\n",
    "    *   **定义:**  奖励 $r$ 是环境在智能体执行某个动作后，返回给智能体的反馈信号，用于评价该动作的好坏。\n",
    "    *   **例子 (CartPole):**  CartPole 环境的奖励函数非常简单：\n",
    "        *   每保持杆子不倒立一个时间步，奖励 +1\n",
    "        *   如果杆子倒立 (超出一定角度) 或小车超出边界，则 episode 结束，没有额外奖励。\n",
    "    *   **奖励函数 (Reward Function):**  定义了在每个状态-动作-下一个状态转移过程中，智能体获得的奖励。\n",
    "    *   **重要性:**  奖励是强化学习的目标驱动力。  智能体的目标是学习一个策略，使其在与环境交互的过程中，获得的累积奖励最大化。\n",
    "\n",
    "---\n",
    "\n",
    "**总结:**  MDP 提供了一个描述强化学习问题的通用框架。  理解 MDP 的核心思想、马尔可夫性质和关键要素，是学习强化学习的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 策略 (Policy) - 智能体的“行动指南”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 什么是策略 (Policy)？\n",
    "\n",
    "**策略 (Policy) 是强化学习中智能体的“行动指南”，它定义了智能体在 *每个状态* 下应该采取的 *动作*。**  简单来说，策略就是一个 **从状态到动作的映射**。\n",
    "\n",
    "**策略的表示形式:**\n",
    "\n",
    "*   **确定性策略 (Deterministic Policy):**  对于每个状态 $s$，策略 $\\pi(s)$  **唯一确定** 一个动作 $a = \\pi(s)$。\n",
    "*   **随机性策略 (Stochastic Policy):**  对于每个状态 $s$，策略 $\\pi(a|s)$  给出一个 **动作的概率分布**，表示在状态 $s$ 下，选择每个动作 $a$ 的概率 $\\pi(a|s) = P(A=a|S=s)$。  智能体会根据这个概率分布来 **随机选择** 动作。\n",
    "\n",
    "**在本课程中，我们主要关注随机性策略，因为它更通用，也更适合探索。**\n",
    "\n",
    "**策略的作用:**\n",
    "\n",
    "*   **控制智能体的行为:**  策略直接决定了智能体在环境中的行为方式。  不同的策略会导致智能体采取不同的动作序列，从而产生不同的累积奖励。\n",
    "*   **强化学习的目标:**  强化学习的最终目标就是 **学习一个最优策略**，使得智能体在与环境交互的过程中，能够获得最大的期望累积奖励。\n",
    "\n",
    "**如何设计策略？**\n",
    "\n",
    "策略的设计是强化学习的核心问题之一。  策略的设计方法有很多种，从简单的规则策略到复杂的深度神经网络策略，都可以用来表示和学习策略。  在本节课中，我们将学习一些简单的策略设计方法，并在后续课程中逐步学习更高级的策略学习方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 简单规则策略 (Rule-based Policy) -  基于人类经验的策略\n",
    "\n",
    "**简单规则策略** 是最直接、最容易理解的一种策略。  它基于人类对环境的理解和经验，人为地设计一些规则来指导智能体的行为。\n",
    "\n",
    "**对于 CartPole 环境，一个简单的规则策略可以是：**\n",
    "\n",
    "“**如果杆子向左倾斜，则向左推小车；如果杆子向右倾斜，则向右推小车。**”\n",
    "\n",
    "这个策略的依据是：  为了保持杆子平衡，我们需要抵消杆子倾斜的趋势。  如果杆子向左倾斜，说明杆子的重心偏左，我们需要向左推动小车，使小车向左运动，从而将杆子的重心拉回中间位置。\n",
    "\n",
    "**用 Python 代码实现这个简单规则策略：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def simple_rule_policy(observation):\n",
    "    pole_angle = observation[2] # 杆的角度是状态的第 3 个元素\n",
    "    if pole_angle < 0:\n",
    "        action = 0 # 向左推\n",
    "    else:\n",
    "        action = 1 # 向右推\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**测试简单规则策略:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均 Episode 步数: 23.17\n",
      "成功率 (>= 200 步): 0.00\n"
     ]
    }
   ],
   "source": [
    "def evaluate_policy(policy, env, num_episodes=100):\n",
    "    episode_lengths = []\n",
    "    success_episodes = 0\n",
    "    success_threshold = 200 # 定义成功步数阈值 (CartPole-v1 认为超过 200 步就成功)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        episode_length = 0\n",
    "        for t in range(500): # CartPole-v1 最大 episode 长度是 500\n",
    "            action = policy(observation)\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_length += 1\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        episode_lengths.append(episode_length)\n",
    "        if episode_length >= success_threshold:\n",
    "            success_episodes += 1\n",
    "\n",
    "    average_length = np.mean(episode_lengths)\n",
    "    success_rate = success_episodes / num_episodes\n",
    "\n",
    "    print(f\"平均 Episode 步数: {average_length:.2f}\")\n",
    "    print(f\"成功率 (>= {success_threshold} 步): {success_rate:.2f}\")\n",
    "    return average_length, success_rate\n",
    "\n",
    "# 评估简单规则策略\n",
    "print(\"\\n评估 简单规则策略:\")\n",
    "average_length_rule_based, success_rate_rule_based = evaluate_policy(simple_rule_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验结果分析:**  简单规则策略的平均 Episode 步数只有 20 步左右，成功率几乎为 0。  这表明简单规则策略虽然能够让 CartPole 保持平衡一段时间，但性能并不理想，无法长时间保持平衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Epsilon 贪婪策略 (Epsilon-Greedy Policy) - 平衡探索与利用\n",
    "\n",
    "**简单规则策略虽然有效，但它可能不是最优的。**  因为它只利用了我们已知的规则，而没有尝试去**探索**其他可能的动作。\n",
    "\n",
    "**在强化学习中，“探索 (Exploration)” 和 “利用 (Exploitation)” 是一个非常重要的权衡。**\n",
    "\n",
    "*   **探索 (Exploration):**  尝试新的、未知的动作，去发现环境中可能存在的更好的策略。\n",
    "*   **利用 (Exploitation):**  利用当前已知的最好策略，尽可能获得更多的奖励。\n",
    "\n",
    "**Epsilon 贪婪策略** 就是一种常用的平衡探索与利用的方法。\n",
    "\n",
    "**Epsilon 贪婪策略的原理:**\n",
    "\n",
    "*   以 **概率 ε (epsilon)**  进行 **探索**：  随机选择一个动作 (在 CartPole 中就是随机选择 0 或 1)。\n",
    "*   以 **概率 1-ε** 进行 **利用**：  根据当前已知的最佳策略 (例如，我们的简单规则策略) 选择动作。\n",
    "\n",
    "**ε 的取值范围通常在 0 到 1 之间。**\n",
    "\n",
    "*   **ε 越大，探索的程度越高，利用的程度越低。**  智能体更倾向于尝试新的动作，更有可能发现更好的策略，但也可能因为探索而错失一些眼前的奖励。\n",
    "*   **ε 越小，探索的程度越低，利用的程度越高。**  智能体更倾向于利用已知的策略，可以更快地获得较高的奖励，但也可能陷入局部最优，错过全局最优策略。\n",
    "\n",
    "**通常情况下，我们会从一个较大的 ε 值开始 (例如 0.1 或 0.2)，然后随着训练的进行，逐渐减小 ε 的值。**  这样在训练初期，智能体可以充分探索环境，发现有潜力的策略；在训练后期，智能体可以更多地利用已知的策略，稳定地获得高奖励。\n",
    "\n",
    "**用 Python 代码实现 Epsilon 贪婪策略 (结合简单规则策略):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(observation, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        # 探索：随机选择动作\n",
    "        action = env.action_space.sample() # 随机选择 0 或 1\n",
    "    else:\n",
    "        # 利用：使用简单规则策略选择动作\n",
    "        action = simple_rule_policy(observation)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**测试 Epsilon 贪婪策略 (epsilon=0.1):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估 Epsilon 贪婪策略 (epsilon=0.1):\n",
      "平均 Episode 步数: 135.88\n",
      "成功率 (>= 200 步): 0.32\n"
     ]
    }
   ],
   "source": [
    "# 评估 epsilon 贪婪策略 (epsilon=0.1)\n",
    "print(\"\\n评估 Epsilon 贪婪策略 (epsilon=0.1):\")\n",
    "average_length_epsilon_01, success_rate_epsilon_01 = evaluate_policy(lambda obs: epsilon_greedy_policy(obs, epsilon=0.1), env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验结果分析:**  Epsilon 贪婪策略 (epsilon=0.1) 的平均 Episode 步数显著提高到了 130 步左右，成功率也达到了 30% 以上。  这表明 Epsilon 贪婪策略比简单规则策略表现更好。  **原因在于 Epsilon 贪婪策略在利用简单规则策略的基础上，加入了适度的探索，尝试了随机动作，从而有可能发现比简单规则策略更好的动作，最终提高了策略的性能。**\n",
    "\n",
    "**尝试不同的 epsilon 值:**  可以尝试修改 `epsilon_greedy_policy` 函数中的 `epsilon` 参数，例如设置为 0.01, 0.05, 0.2, 0.5 等，观察 epsilon 值对策略性能的影响。  思考 epsilon 值如何影响探索和利用的平衡？  如何选择合适的 epsilon 值？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 AI 辅助策略设计 - 尝试更复杂的策略 (进阶)\n",
    "\n",
    "**简单规则策略和 epsilon 贪婪策略都是人工设计的策略。**  对于更复杂的问题，人工设计策略可能会非常困难，甚至不可能。\n",
    "\n",
    "**这时，我们可以借助 AI 工具来辅助我们设计策略。**  例如，我们可以使用 **ChatGPT** 这样的对话式 AI，让它帮我们生成 CartPole 的控制策略。\n",
    "\n",
    "**如何使用 ChatGPT 辅助设计 CartPole 策略？ (Prompt 工程)**\n",
    "\n",
    "**Prompt (提示语) 的设计非常重要，好的 Prompt 可以引导 AI 生成更有用的策略。**  以下是一些 Prompt 的示例：\n",
    "\n",
    "**示例 Prompt 1 (简单):**\n",
    "```\n",
    "请你帮我设计一个控制 CartPole 平衡的策略。\n",
    "```\n",
    "\n",
    "**示例 Prompt 2 (更具体):**\n",
    "```\n",
    "请你帮我设计一个控制 CartPole 平衡的策略，策略需要根据杆子的角度和角速度来决定向左还是向右推动小车。\n",
    "```\n",
    "\n",
    "**示例 Prompt 3 (更详细，指定策略类型):**\n",
    "```\n",
    "请你帮我设计一个基于 PID 控制的 CartPole 平衡策略。  策略需要根据杆子的角度偏差、角速度和角加速度来计算控制力，并转换为向左或向右的动作。\n",
    "```\n",
    "\n",
    "**你可以尝试使用不同的 Prompt，看看 ChatGPT 能生成什么样的策略。**  生成的策略可能是伪代码、Python 代码，或者是自然语言描述的规则。\n",
    "\n",
    "**拿到 AI 生成的策略后，你需要做的是：**\n",
    "\n",
    "1.  **理解策略:**  仔细阅读 AI 生成的策略，理解策略的思路和原理。\n",
    "2.  **评估策略:**  思考 AI 生成的策略是否合理，是否考虑了 CartPole 的物理特性。  例如，策略是否会过度反应？是否会忽略某些重要的状态信息？\n",
    "3.  **改进策略:**  根据你的理解和评估，对 AI 生成的策略进行修改和完善。  例如，可以调整策略的参数，或者添加一些额外的规则。\n",
    "4.  **实现策略:**  将最终的策略用 Python 代码实现，并进行实验验证。\n",
    "\n",
    "**任务:**\n",
    "\n",
    "1.  **尝试使用上述 Prompt 或你自己的 Prompt，让 ChatGPT 生成 CartPole 的控制策略。**\n",
    "2.  **选择一个你认为比较有潜力的 AI 策略，用 Python 代码实现它。**  (可以先从简单的策略开始)\n",
    "3.  **将你实现的 AI 策略与之前的简单规则策略和 epsilon 贪婪策略进行比较，看看 AI 策略的性能如何。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 策略评估：价值函数的思想 (初步介绍)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 价值函数 (Value Function) - 策略好坏的“评分标准” (初步理解)\n",
    "\n",
    "**价值函数 (Value Function) 是强化学习中一个非常重要的概念。**  它可以用来**评估一个状态或者一个策略的“好坏”**。\n",
    "\n",
    "**价值函数的直观理解:**\n",
    "\n",
    "*   **状态价值函数 (State Value Function) V(s):**  表示从**状态 s** 出发，**遵循策略 π**，**未来能够获得的期望累积奖励**。  可以理解为：**当前状态 s 的 “价值” 有多大**，价值越大，代表从这个状态出发，未来能获得的奖励越多。\n",
    "*   **动作价值函数 (Action Value Function) Q(s, a):**  表示从**状态 s** 出发，**执行动作 a**，然后**遵循策略 π**，**未来能够获得的期望累积奖励**。  可以理解为：**在状态 s 下，执行动作 a 的 “价值” 有多大**，价值越大，代表在状态 s 下执行动作 a，未来能获得的奖励越多。\n",
    "\n",
    "**价值函数就像一个“评分标准”，它告诉我们：**\n",
    "\n",
    "*   **对于某个状态 s，它的价值 V(s) 越高，表示从这个状态出发，未来能够获得的奖励越多，这个状态就越“好”。**  例如，在 CartPole 中，如果一个状态是杆子接近垂直，小车速度很慢，那么这个状态的价值就比较高，因为它更容易保持平衡，获得更多奖励。\n",
    "*   **对于某个状态-动作对 (s, a)，它的价值 Q(s, a) 越高，表示在状态 s 下执行动作 a，未来能够获得的奖励越多，这个动作就越“好”。**  例如，在杆子稍微向左倾斜的状态下，向左推小车的动作可能比向右推小车的动作价值更高，因为它更有助于恢复平衡。\n",
    "*   **对于一个策略 π，我们可以通过计算它在不同状态下的价值函数，来评估这个策略的整体性能。**  如果一个策略在大多数状态下都能获得较高的价值，那么这个策略就比较“好”。\n",
    "\n",
    "**在本节课，我们不深入学习价值函数的计算方法，而是使用价值函数的思想来评估我们设计的策略。**  我们会通过实验来**估计策略的价值**，并比较不同策略的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 策略评估方法 - 实验指标 (基于价值函数的思想)\n",
    "\n",
    "**我们使用以下指标来评估策略的性能 (这些指标都体现了价值函数的思想):**\n",
    "\n",
    "*   **平均 Episode 步数 (Average Episode Length):**  策略在每个 episode 中能够保持 CartPole 平衡的平均步数。  **平均步数越高，表示策略越好，因为策略能够让 CartPole 坚持更长时间，获得更多的奖励。**  这可以看作是对策略价值的一个粗略估计。\n",
    "\n",
    "*   **成功率 (Success Rate):**  在一定数量的 episodes 中，策略能够成功保持 CartPole 平衡超过一定步数阈值的比例。  **成功率越高，表示策略的鲁棒性越好，更可靠。**  成功率也可以反映策略的价值。\n",
    "\n",
    "**我们已经定义了 `evaluate_policy` 函数来运行策略并计算这些指标 (见 2.2 节)。**  可以直接使用该函数来评估不同的策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 实验与结果分析\n",
    "\n",
    "**运行以下代码，评估不同策略的性能：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估 Epsilon 贪婪策略 (epsilon=0.1):\n",
      "平均 Episode 步数: 134.38\n",
      "成功率 (>= 200 步): 0.29\n",
      "\n",
      "评估 简单规则策略:\n",
      "平均 Episode 步数: 23.53\n",
      "成功率 (>= 200 步): 0.00\n"
     ]
    }
   ],
   "source": [
    "# 评估 epsilon 贪婪策略 (epsilon=0.1)\n",
    "print(\"\\n评估 Epsilon 贪婪策略 (epsilon=0.1):\")\n",
    "average_length_epsilon_01, success_rate_epsilon_01 = evaluate_policy(lambda obs: epsilon_greedy_policy(obs, epsilon=0.1), env)\n",
    "\n",
    "# 评估简单规则策略 (作为对比)\n",
    "print(\"\\n评估 简单规则策略:\")\n",
    "average_length_rule_based, success_rate_rule_based = evaluate_policy(simple_rule_policy, env)\n",
    "\n",
    "# [可选] 评估你实现的 AI 辅助策略 (如果实现了的话)\n",
    "# print(\"\\n评估 AI 辅助策略:\")\n",
    "# average_length_ai_policy, success_rate_ai_policy = evaluate_policy(your_ai_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实验结果分析:**\n",
    "\n",
    "运行上面的代码后，观察不同策略的评估结果，比较它们的平均 Episode 步数和成功率。\n",
    "\n",
    "*   **比较 epsilon 贪婪策略和简单规则策略的性能。**  epsilon 贪婪策略是否比简单规则策略表现更好？  为什么？\n",
    "*   **如果你实现了 AI 辅助策略，比较 AI 策略与 epsilon 贪婪策略和简单规则策略的性能。**  AI 策略是否表现更优？  AI 策略的优势和劣势是什么？\n",
    "*   **尝试调整 epsilon 的值 (例如，0.01, 0.05, 0.2, 0.5)，观察 epsilon 值对 epsilon 贪婪策略性能的影响。**  ε 值如何影响探索和利用的平衡？  如何选择合适的 ε 值？\n",
    "*   **思考：**  除了平均 Episode 步数和成功率，还有没有其他可以用来评估策略性能的指标？  例如，可以考虑 episode 步数的分布情况，或者策略的稳定性等等。\n",
    "*   **更深入地思考价值函数的概念：**  实验结果如何体现价值函数的思想？  平均 Episode 步数和成功率是否可以看作是对策略价值的估计？  如何更精确地计算价值函数？ (这个问题我们会在后续课程中学习)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  总结与课后作业\n",
    "\n",
    "**本节课总结:**\n",
    "\n",
    "*   **深入学习了强化学习的核心框架 马尔可夫决策过程 (MDP)**，理解了状态、动作、奖励、策略和马尔可夫性质等关键概念。\n",
    "*   **掌握了策略 (Policy) 的概念**，理解了策略在强化学习中的作用，并学习了如何设计策略。\n",
    "*   **学习并应用了 epsilon 贪婪策略**，理解了探索与利用的权衡，以及 epsilon 贪婪策略如何平衡两者。\n",
    "*   **初步了解了价值函数 (Value Function) 的概念**，并使用价值函数的思想，通过实验评估了策略的性能。\n",
    "*   **学习了如何利用 AI 工具 (如 ChatGPT) 辅助设计更复杂的 CartPole 控制策略**，并鼓励大家尝试。\n",
    "*   **通过实验比较了不同策略的性能**，并进行了结果分析。\n",
    "\n",
    "**课后作业:**\n",
    "\n",
    "1.  **调整 epsilon 值实验:**  修改 `epsilon_greedy_policy` 函数中的 `epsilon` 参数，尝试不同的 epsilon 值 (例如 0, 0.01, 0.05, 0.1, 0.2, 0.5)，运行实验，观察 epsilon 值对策略性能的影响，并绘制 epsilon 值 vs. 平均 Episode 步数的折线图。\n",
    "2.  **实现并评估 AI 辅助策略:**  选择一个你认为有潜力的 AI 生成的 CartPole 策略，用 Python 代码实现它，并使用 `evaluate_policy` 函数评估其性能，与简单规则策略和 epsilon 贪婪策略进行比较。\n",
    "3.  **改进策略 (可选):**  尝试改进简单规则策略、epsilon 贪婪策略或 AI 辅助策略，例如，可以考虑杆子的角速度、角加速度等更多状态信息，或者设计更复杂的规则或策略结构。  比较改进前后策略的性能。\n",
    "4.  **思考题:**\n",
    "    *   除了平均 Episode 步数和成功率，你还能想到其他的策略评估指标吗？  例如，可以考虑 episode 步数的方差、最大步数等等。\n",
    "    *   在实际的强化学习问题中，如何选择合适的探索策略？  除了 epsilon 贪婪策略，还有没有其他的探索策略？  它们的优缺点是什么？ (可以查阅资料了解更多探索策略，例如 Boltzmann Exploration, Upper Confidence Bound (UCB) 等)\n",
    "    *   价值函数在强化学习中有什么作用？  为什么说价值函数是评估策略好坏的“评分标准”？  价值函数和策略之间有什么关系？ (为后续学习价值函数方法打下基础)\n",
    "\n",
    "**下节课预告:**  下节课，我们将正式开始学习第一个基于价值函数的强化学习算法：**Q-Learning**，并用 Q-Learning 算法来解决迷宫寻宝 (Grid World) 问题！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
