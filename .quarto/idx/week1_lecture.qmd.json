{"title":"Week 1: 商业决策智能化与强化学习概览","markdown":{"yaml":{"title":"Week 1: 商业决策智能化与强化学习概览"},"headingText":"课程介绍与商业决策的挑战","containsRefs":false,"markdown":"\n\n\n## 欢迎与课程概览\n\n欢迎来到《商业决策的智能优化：强化学习方法与应用》！\n\n*   **课程目标:** (回顾大纲中的课程目标 1-7)\n*   **面向对象:** 经济管理学院大三学生\n*   **先修要求:** 概率统计、经济/管理基础、基本 Python 了解 (AI 辅助可用)\n*   **教学方式:** 理论讲授 (40%), 编程实验 (40%), 案例讨论 (20%)\n*   **评估方式:** (回顾大纲中的评估方式)\n    *   编程实验与 Lab 报告 (35-40%)\n    *   案例分析与讨论参与 (10-15%)\n    *   中期测试 (15-20%)\n    *   期末项目 (30-35%)\n\n::: {.callout-note title=\"评分标准\"}\n详细的评分细则将在后续说明。请注意 Lab 报告的要求，即使使用 AI 辅助，也需要体现独立思考和理解。\n:::\n\n## 商业决策的复杂性\n\n传统的商业决策方法往往面临挑战：\n\n*   **动态性 (Dynamics):** 市场环境、客户偏好、竞争对手策略不断变化。今天的最优决策明天可能不再适用。\n*   **不确定性 (Uncertainty):** 决策结果往往受到随机因素的影响（如供应链中断、突发事件、消费者情绪波动）。\n*   **延迟反馈 (Delayed Feedback):** 很多决策（如长期投资、品牌建设）的效果需要很长时间才能显现，难以快速评估和调整。\n*   **大规模与高维度 (Large Scale & High Dimension):** 现代商业涉及海量数据和众多决策变量（如管理数千种商品的库存、对百万级用户进行个性化营销）。\n\n::: {.callout-tip title=\"思考\"}\n你能想到哪些具体的商业决策场景，同时具备动态性、不确定性和延迟反馈的特点？\n:::\n\n## 人工智能与商业智能 (AI & BI)\n\n*   **商业智能 (BI):** 侧重于**描述性分析 (Descriptive Analytics)** 和**诊断性分析 (Diagnostic Analytics)**。利用历史数据理解发生了什么 (What happened?) 以及为什么发生 (Why did it happen?)。常用工具包括报表、仪表盘、数据可视化。\n*   **人工智能 (AI):** 涵盖更广泛的技术，包括**预测性分析 (Predictive Analytics)** (预测未来会发生什么 - What will happen?) 和**处方性分析 (Prescriptive Analytics)** (应该采取什么行动 - What should we do?)。机器学习是 AI 的核心组成部分。\n\n## 为何需要强化学习 (RL)？\n\n监督学习 (Supervised Learning) 在许多领域取得了巨大成功（如图像识别、语音识别），它依赖于带有明确标签的数据 (输入 -> 正确输出)。\n\n然而，许多商业决策问题缺乏明确的“正确答案”标签：\n\n*   **没有唯一的“最优”定价:** 最优价格取决于市场反应、竞争对手行为等动态因素。\n*   **没有完美的营销策略:** 效果依赖于用户反馈和长期影响。\n*   **序贯决策 (Sequential Decisions):** 决策不是一次性的，而是一系列相互影响的决策。当前决策不仅影响即时收益，更影响未来的状态和可选动作。\n\n**强化学习 (Reinforcement Learning, RL)** 提供了一种不同的范式：\n\n*   **通过与环境交互学习:** 智能体 (Agent) 在环境 (Environment) 中采取行动 (Action)，观察结果 (State) 和奖励 (Reward)，并据此调整策略 (Policy) 以最大化长期累积奖励。\n*   **关注长期目标:** RL 不仅仅追求即时奖励，而是学习能够带来最大化未来总回报的策略。\n*   **试错学习 (Trial-and-Error):** 智能体通过尝试不同的行动来发现哪些行动能带来好的结果。\n\n::: {.callout-important title=\"RL vs. 监督学习\"}\n*   **监督学习:** 从“老师”提供的标签中学习 (Learn from labels)。\n*   **强化学习:** 从与环境交互的经验中学习 (Learn from experience/interaction)。\n:::\n\n## RL 成功案例简介\n\n*   **游戏 AI:** AlphaGo (围棋), AlphaStar (星际争霸), OpenAI Five (Dota 2) - 超越人类水平。\n*   **机器人控制:** 学习复杂的抓取、行走任务。\n*   **推荐系统:** 优化长期用户参与度和满意度，而不仅仅是短期点击率。\n*   **动态定价:** 根据供需关系实时调整价格（网约车、酒店）。\n*   **资源优化:** 数据中心能源优化、网络流量调度。\n*   **金融交易:** (虽然挑战重重) 尝试制定交易策略。\n\n# 强化学习核心要素\n\n理解 RL 的基本构成模块至关重要。\n\n*   **智能体 (Agent):** 学习者和决策者。它可以是你的定价算法、库存管理系统、推荐引擎等。\n*   **环境 (Environment):** 智能体交互的外部世界。它包含了除智能体之外的一切。例如，市场、客户群体、供应链系统。\n*   **状态 (State, $S$):** 对环境当前状况的描述。智能体根据状态来决定下一步行动。\n    *   *例子 (定价):* 当前库存水平、竞争对手价格、近期销售趋势、时间（如季节、节假日）。\n    *   *例子 (库存):* 当前各种商品的库存量、预测的需求、在途库存。\n*   **动作 (Action, $A$):** 智能体可以采取的操作。\n    *   *例子 (定价):* 提高价格 5%，降低价格 10%，保持不变。\n    *   *例子 (库存):* 订购 100 单位 A 商品，订购 50 单位 B 商品，不订购。\n*   **奖励 (Reward, $R$):** 环境对智能体在某个状态下采取某个动作后给出的即时反馈信号。它定义了智能体的目标。奖励可以是正面的（收益、利润、用户满意度）或负面的（成本、损失、客户流失）。\n    *   *例子 (定价):* 该动作带来的即时销售额或利润。\n    *   *例子 (库存):* 满足需求的收益 - 库存持有成本 - 缺货损失。\n*   **策略 (Policy, $\\pi$):** 智能体的行为方式，即从状态到动作的映射。它定义了智能体在特定状态下会选择哪个（或哪些）动作。\n    *   *确定性策略 (Deterministic):* $\\pi(s) = a$ (在状态 $s$ 下，总是选择动作 $a$)\n    *   *随机性策略 (Stochastic):* $\\pi(a|s) = P(A=a | S=s)$ (在状态 $s$ 下，选择动作 $a$ 的概率)\n*   **值函数 (Value Function, $V$):** 评估一个状态（或状态-动作对）有多好。它表示从该状态开始，遵循特定策略 $\\pi$，预期未来能获得的累积奖励。\n    *   *状态值函数 $V_{\\pi}(s)$:* 从状态 $s$ 开始，遵循策略 $\\pi$ 的预期总回报。\n*   **动作值函数 (Action-Value Function, $Q$):** 也称为 Q 函数。\n    *   *$Q_{\\pi}(s, a)$:* 在状态 $s$ 下，采取动作 $a$，然后继续遵循策略 $\\pi$ 的预期总回报。$Q$ 函数直接关联了具体动作的好坏，对于决策至关重要。\n\n::: {.callout-note title=\"核心循环\"}\n1.  智能体观察当前状态 $S_t$。\n2.  智能体根据策略 $\\pi$ 选择动作 $A_t$。\n3.  环境接收动作 $A_t$，转移到新状态 $S_{t+1}$，并给出奖励 $R_{t+1}$。\n4.  智能体利用 $(S_t, A_t, R_{t+1}, S_{t+1})$ 这个经验来学习和改进策略 $\\pi$。\n:::\n\n## 互动练习：分解商业场景\n\n请尝试将以下商业场景分解为 RL 的核心要素 ($S$, $A$, $R$)。思考可能的策略 $\\pi$ 和值函数 $V$/$Q$ 的含义。\n\n1.  **动态定价 (Dynamic Pricing):** 单一易腐烂商品（如机票、酒店房间），需要在到期前售出。\n    *   $S$: ? (e.g., 剩余时间，剩余库存，近期预订速率...)\n    *   $A$: ? (e.g., 设定具体价格，提价/降价幅度...)\n    *   $R$: ? (e.g., 即时销售收入，一天结束时的总收入...)\n    *   $\\pi$: ? (e.g., 如果剩余时间少且库存多，则大幅降价...)\n    *   $V$/$Q$: ? (e.g., V(t天, k库存) = 从现在开始到售罄/过期的预期总收入)\n\n2.  **库存管理 (Inventory Management):** 单一商品，需要决定每天订购多少。\n    *   $S$: ? (e.g., 当前库存水平，预测的未来几天需求...)\n    *   $A$: ? (e.g., 订购数量...)\n    *   $R$: ? (e.g., 销售收入 - 订购成本 - 库存持有成本 - 缺货惩罚...)\n    *   $\\pi$: ? (e.g., 如果库存低于阈值，则订购一定量...)\n    *   $V$/$Q$: ? (e.g., Q(k库存, d订购量) = 采取订购动作后的长期预期净利润)\n\n3.  **个性化营销 (Personalized Marketing):** 向网站访客推送优惠券。\n    *   $S$: ? (e.g., 用户画像 [浏览历史、购买记录、人口统计学信息], 当前访问页面...)\n    *   A: ? (e.g., 推送 A 类优惠券, 推送 B 类优惠券, 不推送...)\n    *   $R$: ? (e.g., 用户是否点击/使用优惠券, 购买转化金额, 长期用户价值 LTV 的变化...)\n    *   $\\pi$: ? (e.g., 对高价值历史用户推送高折扣券...)\n    *   $V$/$Q$: ? (e.g., Q(用户u, 优惠券c) = 向用户 u 推送优惠券 c 的预期长期价值贡献)\n\n4.  **智能客服路由 (Intelligent Customer Service Routing):** 将来电分配给最合适的客服代表。\n    *   $S$: ? (e.g., 客户类型, 问题类型, 可用客服代表的技能/状态...)\n    *   $A$: ? (e.g., 分配给代表 1, 分配给代表 2...)\n    *   $R$: ? (e.g., 问题解决时长, 客户满意度评分, 是否需要二次呼入...)\n    *   $\\pi$: ? (e.g., 技术问题分配给技术专家...)\n    *   $V$/$Q$: ? (e.g., V(客户类型c, 问题类型p) = 该类电话的最佳处理方式下的预期服务质量指标)\n\n## 探索 (Exploration) vs. 利用 (Exploitation)\n\n这是 RL 中的一个核心权衡：\n\n*   **利用 (Exploitation):** 根据当前已知的最优策略采取行动，以获得当前看来最好的回报。\n*   **探索 (Exploration):** 尝试新的、未知的行动，即使它们当前看起来不是最优的，目的是为了收集更多信息，发现可能更好的策略。\n\n**商业实例:**\n\n*   **餐厅:**\n    *   *利用:* 只做最受欢迎的招牌菜。\n    *   *探索:* 尝试推出新菜品，可能发现新的爆款，但也可能不受欢迎。\n*   **广告投放:**\n    *   *利用:* 将预算集中投放在已知效果最好的渠道和人群。\n    *   *探索:* 分配一部分预算尝试新的广告平台、创意或目标受众。\n*   **产品推荐:**\n    *   *利用:* 总是推荐用户过去喜欢或购买过的同类商品。\n    *   *探索:* 推荐一些用户可能感兴趣但从未接触过的新品类。\n\n::: {.callout-warning title=\"权衡的重要性\"}\n*   **过度利用:** 可能陷入局部最优，错失发现更好策略的机会。\n*   **过度探索:** 可能浪费过多资源在次优的行动上，导致整体性能不佳。\nRL 算法需要有效地平衡探索与利用。\n:::\n\n---\n\n**下周预告:** 序贯决策建模 - 马尔可夫决策过程 (MDP)","srcMarkdownNoYaml":"\n\n# 课程介绍与商业决策的挑战\n\n## 欢迎与课程概览\n\n欢迎来到《商业决策的智能优化：强化学习方法与应用》！\n\n*   **课程目标:** (回顾大纲中的课程目标 1-7)\n*   **面向对象:** 经济管理学院大三学生\n*   **先修要求:** 概率统计、经济/管理基础、基本 Python 了解 (AI 辅助可用)\n*   **教学方式:** 理论讲授 (40%), 编程实验 (40%), 案例讨论 (20%)\n*   **评估方式:** (回顾大纲中的评估方式)\n    *   编程实验与 Lab 报告 (35-40%)\n    *   案例分析与讨论参与 (10-15%)\n    *   中期测试 (15-20%)\n    *   期末项目 (30-35%)\n\n::: {.callout-note title=\"评分标准\"}\n详细的评分细则将在后续说明。请注意 Lab 报告的要求，即使使用 AI 辅助，也需要体现独立思考和理解。\n:::\n\n## 商业决策的复杂性\n\n传统的商业决策方法往往面临挑战：\n\n*   **动态性 (Dynamics):** 市场环境、客户偏好、竞争对手策略不断变化。今天的最优决策明天可能不再适用。\n*   **不确定性 (Uncertainty):** 决策结果往往受到随机因素的影响（如供应链中断、突发事件、消费者情绪波动）。\n*   **延迟反馈 (Delayed Feedback):** 很多决策（如长期投资、品牌建设）的效果需要很长时间才能显现，难以快速评估和调整。\n*   **大规模与高维度 (Large Scale & High Dimension):** 现代商业涉及海量数据和众多决策变量（如管理数千种商品的库存、对百万级用户进行个性化营销）。\n\n::: {.callout-tip title=\"思考\"}\n你能想到哪些具体的商业决策场景，同时具备动态性、不确定性和延迟反馈的特点？\n:::\n\n## 人工智能与商业智能 (AI & BI)\n\n*   **商业智能 (BI):** 侧重于**描述性分析 (Descriptive Analytics)** 和**诊断性分析 (Diagnostic Analytics)**。利用历史数据理解发生了什么 (What happened?) 以及为什么发生 (Why did it happen?)。常用工具包括报表、仪表盘、数据可视化。\n*   **人工智能 (AI):** 涵盖更广泛的技术，包括**预测性分析 (Predictive Analytics)** (预测未来会发生什么 - What will happen?) 和**处方性分析 (Prescriptive Analytics)** (应该采取什么行动 - What should we do?)。机器学习是 AI 的核心组成部分。\n\n## 为何需要强化学习 (RL)？\n\n监督学习 (Supervised Learning) 在许多领域取得了巨大成功（如图像识别、语音识别），它依赖于带有明确标签的数据 (输入 -> 正确输出)。\n\n然而，许多商业决策问题缺乏明确的“正确答案”标签：\n\n*   **没有唯一的“最优”定价:** 最优价格取决于市场反应、竞争对手行为等动态因素。\n*   **没有完美的营销策略:** 效果依赖于用户反馈和长期影响。\n*   **序贯决策 (Sequential Decisions):** 决策不是一次性的，而是一系列相互影响的决策。当前决策不仅影响即时收益，更影响未来的状态和可选动作。\n\n**强化学习 (Reinforcement Learning, RL)** 提供了一种不同的范式：\n\n*   **通过与环境交互学习:** 智能体 (Agent) 在环境 (Environment) 中采取行动 (Action)，观察结果 (State) 和奖励 (Reward)，并据此调整策略 (Policy) 以最大化长期累积奖励。\n*   **关注长期目标:** RL 不仅仅追求即时奖励，而是学习能够带来最大化未来总回报的策略。\n*   **试错学习 (Trial-and-Error):** 智能体通过尝试不同的行动来发现哪些行动能带来好的结果。\n\n::: {.callout-important title=\"RL vs. 监督学习\"}\n*   **监督学习:** 从“老师”提供的标签中学习 (Learn from labels)。\n*   **强化学习:** 从与环境交互的经验中学习 (Learn from experience/interaction)。\n:::\n\n## RL 成功案例简介\n\n*   **游戏 AI:** AlphaGo (围棋), AlphaStar (星际争霸), OpenAI Five (Dota 2) - 超越人类水平。\n*   **机器人控制:** 学习复杂的抓取、行走任务。\n*   **推荐系统:** 优化长期用户参与度和满意度，而不仅仅是短期点击率。\n*   **动态定价:** 根据供需关系实时调整价格（网约车、酒店）。\n*   **资源优化:** 数据中心能源优化、网络流量调度。\n*   **金融交易:** (虽然挑战重重) 尝试制定交易策略。\n\n# 强化学习核心要素\n\n理解 RL 的基本构成模块至关重要。\n\n*   **智能体 (Agent):** 学习者和决策者。它可以是你的定价算法、库存管理系统、推荐引擎等。\n*   **环境 (Environment):** 智能体交互的外部世界。它包含了除智能体之外的一切。例如，市场、客户群体、供应链系统。\n*   **状态 (State, $S$):** 对环境当前状况的描述。智能体根据状态来决定下一步行动。\n    *   *例子 (定价):* 当前库存水平、竞争对手价格、近期销售趋势、时间（如季节、节假日）。\n    *   *例子 (库存):* 当前各种商品的库存量、预测的需求、在途库存。\n*   **动作 (Action, $A$):** 智能体可以采取的操作。\n    *   *例子 (定价):* 提高价格 5%，降低价格 10%，保持不变。\n    *   *例子 (库存):* 订购 100 单位 A 商品，订购 50 单位 B 商品，不订购。\n*   **奖励 (Reward, $R$):** 环境对智能体在某个状态下采取某个动作后给出的即时反馈信号。它定义了智能体的目标。奖励可以是正面的（收益、利润、用户满意度）或负面的（成本、损失、客户流失）。\n    *   *例子 (定价):* 该动作带来的即时销售额或利润。\n    *   *例子 (库存):* 满足需求的收益 - 库存持有成本 - 缺货损失。\n*   **策略 (Policy, $\\pi$):** 智能体的行为方式，即从状态到动作的映射。它定义了智能体在特定状态下会选择哪个（或哪些）动作。\n    *   *确定性策略 (Deterministic):* $\\pi(s) = a$ (在状态 $s$ 下，总是选择动作 $a$)\n    *   *随机性策略 (Stochastic):* $\\pi(a|s) = P(A=a | S=s)$ (在状态 $s$ 下，选择动作 $a$ 的概率)\n*   **值函数 (Value Function, $V$):** 评估一个状态（或状态-动作对）有多好。它表示从该状态开始，遵循特定策略 $\\pi$，预期未来能获得的累积奖励。\n    *   *状态值函数 $V_{\\pi}(s)$:* 从状态 $s$ 开始，遵循策略 $\\pi$ 的预期总回报。\n*   **动作值函数 (Action-Value Function, $Q$):** 也称为 Q 函数。\n    *   *$Q_{\\pi}(s, a)$:* 在状态 $s$ 下，采取动作 $a$，然后继续遵循策略 $\\pi$ 的预期总回报。$Q$ 函数直接关联了具体动作的好坏，对于决策至关重要。\n\n::: {.callout-note title=\"核心循环\"}\n1.  智能体观察当前状态 $S_t$。\n2.  智能体根据策略 $\\pi$ 选择动作 $A_t$。\n3.  环境接收动作 $A_t$，转移到新状态 $S_{t+1}$，并给出奖励 $R_{t+1}$。\n4.  智能体利用 $(S_t, A_t, R_{t+1}, S_{t+1})$ 这个经验来学习和改进策略 $\\pi$。\n:::\n\n## 互动练习：分解商业场景\n\n请尝试将以下商业场景分解为 RL 的核心要素 ($S$, $A$, $R$)。思考可能的策略 $\\pi$ 和值函数 $V$/$Q$ 的含义。\n\n1.  **动态定价 (Dynamic Pricing):** 单一易腐烂商品（如机票、酒店房间），需要在到期前售出。\n    *   $S$: ? (e.g., 剩余时间，剩余库存，近期预订速率...)\n    *   $A$: ? (e.g., 设定具体价格，提价/降价幅度...)\n    *   $R$: ? (e.g., 即时销售收入，一天结束时的总收入...)\n    *   $\\pi$: ? (e.g., 如果剩余时间少且库存多，则大幅降价...)\n    *   $V$/$Q$: ? (e.g., V(t天, k库存) = 从现在开始到售罄/过期的预期总收入)\n\n2.  **库存管理 (Inventory Management):** 单一商品，需要决定每天订购多少。\n    *   $S$: ? (e.g., 当前库存水平，预测的未来几天需求...)\n    *   $A$: ? (e.g., 订购数量...)\n    *   $R$: ? (e.g., 销售收入 - 订购成本 - 库存持有成本 - 缺货惩罚...)\n    *   $\\pi$: ? (e.g., 如果库存低于阈值，则订购一定量...)\n    *   $V$/$Q$: ? (e.g., Q(k库存, d订购量) = 采取订购动作后的长期预期净利润)\n\n3.  **个性化营销 (Personalized Marketing):** 向网站访客推送优惠券。\n    *   $S$: ? (e.g., 用户画像 [浏览历史、购买记录、人口统计学信息], 当前访问页面...)\n    *   A: ? (e.g., 推送 A 类优惠券, 推送 B 类优惠券, 不推送...)\n    *   $R$: ? (e.g., 用户是否点击/使用优惠券, 购买转化金额, 长期用户价值 LTV 的变化...)\n    *   $\\pi$: ? (e.g., 对高价值历史用户推送高折扣券...)\n    *   $V$/$Q$: ? (e.g., Q(用户u, 优惠券c) = 向用户 u 推送优惠券 c 的预期长期价值贡献)\n\n4.  **智能客服路由 (Intelligent Customer Service Routing):** 将来电分配给最合适的客服代表。\n    *   $S$: ? (e.g., 客户类型, 问题类型, 可用客服代表的技能/状态...)\n    *   $A$: ? (e.g., 分配给代表 1, 分配给代表 2...)\n    *   $R$: ? (e.g., 问题解决时长, 客户满意度评分, 是否需要二次呼入...)\n    *   $\\pi$: ? (e.g., 技术问题分配给技术专家...)\n    *   $V$/$Q$: ? (e.g., V(客户类型c, 问题类型p) = 该类电话的最佳处理方式下的预期服务质量指标)\n\n## 探索 (Exploration) vs. 利用 (Exploitation)\n\n这是 RL 中的一个核心权衡：\n\n*   **利用 (Exploitation):** 根据当前已知的最优策略采取行动，以获得当前看来最好的回报。\n*   **探索 (Exploration):** 尝试新的、未知的行动，即使它们当前看起来不是最优的，目的是为了收集更多信息，发现可能更好的策略。\n\n**商业实例:**\n\n*   **餐厅:**\n    *   *利用:* 只做最受欢迎的招牌菜。\n    *   *探索:* 尝试推出新菜品，可能发现新的爆款，但也可能不受欢迎。\n*   **广告投放:**\n    *   *利用:* 将预算集中投放在已知效果最好的渠道和人群。\n    *   *探索:* 分配一部分预算尝试新的广告平台、创意或目标受众。\n*   **产品推荐:**\n    *   *利用:* 总是推荐用户过去喜欢或购买过的同类商品。\n    *   *探索:* 推荐一些用户可能感兴趣但从未接触过的新品类。\n\n::: {.callout-warning title=\"权衡的重要性\"}\n*   **过度利用:** 可能陷入局部最优，错失发现更好策略的机会。\n*   **过度探索:** 可能浪费过多资源在次优的行动上，导致整体性能不佳。\nRL 算法需要有效地平衡探索与利用。\n:::\n\n---\n\n**下周预告:** 序贯决策建模 - 马尔可夫决策过程 (MDP)"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week1_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 1: 商业决策智能化与强化学习概览"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}