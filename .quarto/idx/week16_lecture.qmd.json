{"title":"Week 16: 课程总结与未来展望","markdown":{"yaml":{"title":"Week 16: 课程总结与未来展望"},"headingText":"课程回顾：从基础到应用","containsRefs":false,"markdown":"\n\n\n欢迎来到我们课程的最后一周！在过去的十五周里，我们一起探索了强化学习 (RL) 这个激动人心的领域，特别是它在商业决策智能化方面的应用潜力。我们的旅程涵盖了从基本概念到前沿算法，再到实际应用中的挑战与考量。\n\n**我们的学习路径：**\n\n1.  **基础奠基 (Weeks 1-3):**\n    *   理解商业决策的复杂性以及 RL 的价值。\n    *   掌握 RL 核心要素 (S, A, R, π, V, Q)。\n    *   学习使用马尔可夫决策过程 (MDP) 对序贯决策问题进行建模。\n    *   理解 Bellman 方程（期望与最优）作为价值函数分析的核心工具。\n    *   熟悉 Gym/Gymnasium 实验环境。\n\n2.  **核心无模型算法 (Weeks 4-8):**\n    *   学习在**模型未知**情况下进行**策略评估**：\n        *   蒙特卡洛 (MC) 方法：基于完整回合回报，无偏但高方差。\n        *   时序差分 (TD) 学习：基于单步经验和自举，低方差但有偏，可在线学习。\n    *   学习在**模型未知**情况下进行**策略控制**（寻找最优策略）：\n        *   SARSA (同策略 TD 控制)：学习包含探索的行为策略价值。\n        *   Q-Learning (异策略 TD 控制)：直接学习最优动作值函数，可利用历史数据。\n    *   理解同策略与异策略的区别及其影响。\n\n3.  **应对复杂性 - 函数逼近与 DRL (Weeks 9-12):**\n    *   认识到表格型方法的局限性（维度灾难、连续空间）。\n    *   引入**函数逼近**思想：用带参数的函数（特别是神经网络）近似价值函数或策略。\n    *   学习**深度 Q 网络 (DQN):** 结合 Q-Learning 与神经网络，利用经验回放和目标网络提高稳定性（处理离散动作）。\n    *   学习**策略梯度 (PG) 方法:** 直接优化参数化策略 π(a|s, θ)。\n    *   理解**策略梯度定理**和 REINFORCE 算法（高方差问题）。\n    *   学习**Actor-Critic 方法 (A2C):** 结合策略学习 (Actor) 和价值学习 (Critic)，降低方差，提高稳定性，可处理连续动作。\n    *   掌握使用 **Stable Baselines3** 库运行 DQN 和 A2C 实验。\n\n4.  **商业应用、挑战与伦理 (Weeks 13-15):**\n    *   深入分析 RL 在**动态定价/资源优化**和**个性化推荐/营销**中的应用。\n    *   探讨 MDP 定义、奖励设计、数据需求、算法选择等关键环节。\n    *   总结 RL 落地实践中的**共性挑战**：数据、模拟、奖励、安全、部署、维护等。\n    *   强调**负责任 AI** 的重要性，讨论公平性、透明度、隐私等**伦理规范**。\n    *   进行期末项目选题指导。\n\n# 核心概念再梳理\n\n让我们再次巩固一些贯穿始终的核心概念：\n\n*   **MDP:** 描述问题的框架，理解 S, A, R, P, γ 至关重要。商业问题的关键在于如何**定义**这些元素。\n*   **价值函数 (V/Q):** 衡量“好坏”的标准，是许多算法的核心。理解 Vπ, Qπ, V\\*, Q\\* 的区别。\n*   **Bellman 方程:** 连接当前价值与未来价值的桥梁，是理解 TD 学习和动态规划的基础。\n*   **无模型学习:** 现实中模型往往未知，MC 和 TD 是两大基石。\n*   **偏差-方差权衡:** MC (无偏, 高方差) vs. TD (有偏, 低方差)。\n*   **预测 vs. 控制:** 评估现有策略 vs. 寻找最优策略。\n*   **同策略 vs. 异策略:** 学习的策略是否与收集数据的策略相同？(SARSA vs. Q-Learning)。\n*   **函数逼近:** 应对大规模/连续问题的关键，泛化能力是核心优势。\n*   **DRL 技巧:** 经验回放、目标网络 (DQN)，Actor-Critic 结构 (A2C) 是为了解决函数逼近带来的稳定性问题。\n*   **奖励工程:** 设计能够准确反映长期商业目标的奖励函数是 RL 应用成功的关键，也是最具挑战性的环节之一。\n*   **探索与利用:** RL 永恒的主题，需要在收集信息和最大化当前回报之间取得平衡。\n\n# RL 与其他 AI/数据科学技术的结合\n\n强化学习并非孤立存在，它经常与其他 AI 和数据科学技术结合使用，以发挥更大威力：\n\n1.  **结合监督学习 (Supervised Learning, SL):**\n    *   **特征提取:** 使用 SL 模型（如 CNN 处理图像，RNN 处理序列）从原始输入（如用户评论、市场新闻、传感器读数）中提取有意义的状态特征，供 RL 智能体使用。\n    *   **模型构建 (Model-Based RL):** 使用 SL 学习环境模型（预测 P(s'|s, a) 和 R(s, a, s')），然后基于学习到的模型进行规划或生成模拟数据供 Model-Free RL 使用。\n    *   **行为克隆 (Behavioral Cloning):** 使用 SL 模仿专家演示数据，为 RL 提供一个良好的初始策略（预训练）。\n    *   **奖励函数学习 (Inverse Reinforcement Learning, IRL):** 从专家演示中反向推断奖励函数，然后用 RL 优化该奖励函数。\n2.  **结合优化 (Optimization):**\n    *   **RL 作为优化器:** RL 本身可以看作是一种优化方法，用于寻找最大化累积回报的策略（参数）。\n    *   **传统优化方法辅助 RL:**\n        *   **超参数优化:** 使用贝叶斯优化、网格搜索等方法寻找 RL 算法的最佳超参数。\n        *   **约束优化:** 在 RL 框架中加入约束条件（如预算约束、安全约束），使用约束优化技术求解。\n3.  **结合因果推断 (Causal Inference):**\n    *   **理解策略效果:** 在商业场景中，理解 RL 策略改变对业务指标的**因果**影响至关重要（而不仅仅是相关性）。\n    *   **Off-Policy Evaluation:** 使用因果推断的技术（如重要性采样、双重机器学习）更准确地评估一个新策略在历史数据上的表现。\n    *   **处理混淆变量:** 识别和处理影响决策和结果的混淆变量。\n4.  **结合其他数据科学技术:**\n    *   **数据挖掘/分析:** 用于理解数据、发现模式、进行特征工程。\n    *   **A/B 测试:** 用于在线评估和比较不同 RL 策略的效果。\n\n::: {.callout-note title=\"融合是趋势\"}\n未来的智能决策系统很可能是多种 AI 和数据科学技术的融合体，RL 在其中扮演着处理序贯决策和长期优化的关键角色。\n:::\n\n# 前沿方向与商业潜力展望\n\n强化学习仍然是一个快速发展的领域，一些前沿方向在商业应用中展现出巨大潜力：\n\n1.  **离线强化学习 (Offline RL / Batch RL):**\n    *   **背景:** 许多商业场景拥有大量的历史日志数据，但在线交互成本高或风险大。\n    *   **目标:** **仅**使用固定的历史数据集来学习最优策略，而**无需**与环境进行新的交互。\n    *   **挑战:** 分布偏移 (Distribution Shift) - 历史数据的策略与正在学习的策略不同，可能导致价值估计不准或策略表现糟糕。\n    *   **技术:** 通过引入保守主义（如限制策略学习范围、悲观价值估计）来缓解分布偏移问题。\n    *   **商业潜力:** 在推荐、广告、医疗、金融等拥有大量历史数据的领域应用前景广阔，可以更安全、低成本地利用数据进行策略优化。\n2.  **多智能体强化学习 (Multi-Agent RL, MARL):**\n    *   **背景:** 许多商业环境涉及多个相互影响的决策主体（智能体），如市场中的多个竞争公司、共享出行平台上的司机和乘客、协作机器人团队。\n    *   **目标:** 学习能够在这种多智能体环境中有效协作或竞争的策略。\n    *   **挑战:** 环境非平稳性（其他智能体的策略在变）、信用分配（奖励如何在团队中分配）、智能体之间的协调与沟通。\n    *   **商业潜力:** 优化竞争策略、设计协作机制（如供应链协同、车队管理）、理解复杂市场动态。\n3.  **基于模型的强化学习 (Model-Based RL):**\n    *   **思路:** 学习一个环境模型，然后利用这个模型进行规划（如 MCTS）或生成模拟数据训练无模型策略。\n    *   **优点:** 理论上可以提高样本效率（如果模型学得准）。\n    *   **挑战:** 学习准确的环境模型本身就很难，模型误差可能导致策略表现不佳。\n    *   **商业潜力:** 在需要规划能力或样本效率要求极高的场景（如机器人、复杂供应链优化）中有应用价值。\n4.  **表示学习与 RL (Representation Learning):**\n    *   **目标:** 从高维原始输入（如图像、文本）中自动学习有效的低维状态表示，供 RL 算法使用。\n    *   **技术:** 结合自编码器、对比学习、Transformer 等深度学习表示技术。\n    *   **商业潜力:** 使 RL 能够应用于更广泛的、具有非结构化输入的商业问题。\n5.  **可解释性、安全性与公平性:**\n    *   随着 RL 应用日益广泛，如何确保其决策过程透明、结果可信、行为安全、影响公平，是越来越重要的研究方向，也是商业落地必须解决的问题。\n\n# Session 32: 期末项目展示 / 期末考试\n\n根据课程安排，下一次课（Session 32）将用于：\n\n*   **期末项目展示 (Final Project Presentations):** 选择做项目的同学将进行简短展示，分享你们的研究成果、方案设计或文献分析。请准备好 PPT 或演示文稿。\n*   **或 期末考试 (Final Exam):** 如果采用考试形式，将考察整个学期所学的核心概念、算法原理、优缺点比较以及对商业应用的理解。\n\n具体形式和要求将另行通知。\n\n**感谢大家一学期的投入与参与！希望这门课程能够帮助大家打开一扇通往智能决策优化的大门，并为你们未来的学习和职业生涯提供有价值的知识和技能。**","srcMarkdownNoYaml":"\n\n# 课程回顾：从基础到应用\n\n欢迎来到我们课程的最后一周！在过去的十五周里，我们一起探索了强化学习 (RL) 这个激动人心的领域，特别是它在商业决策智能化方面的应用潜力。我们的旅程涵盖了从基本概念到前沿算法，再到实际应用中的挑战与考量。\n\n**我们的学习路径：**\n\n1.  **基础奠基 (Weeks 1-3):**\n    *   理解商业决策的复杂性以及 RL 的价值。\n    *   掌握 RL 核心要素 (S, A, R, π, V, Q)。\n    *   学习使用马尔可夫决策过程 (MDP) 对序贯决策问题进行建模。\n    *   理解 Bellman 方程（期望与最优）作为价值函数分析的核心工具。\n    *   熟悉 Gym/Gymnasium 实验环境。\n\n2.  **核心无模型算法 (Weeks 4-8):**\n    *   学习在**模型未知**情况下进行**策略评估**：\n        *   蒙特卡洛 (MC) 方法：基于完整回合回报，无偏但高方差。\n        *   时序差分 (TD) 学习：基于单步经验和自举，低方差但有偏，可在线学习。\n    *   学习在**模型未知**情况下进行**策略控制**（寻找最优策略）：\n        *   SARSA (同策略 TD 控制)：学习包含探索的行为策略价值。\n        *   Q-Learning (异策略 TD 控制)：直接学习最优动作值函数，可利用历史数据。\n    *   理解同策略与异策略的区别及其影响。\n\n3.  **应对复杂性 - 函数逼近与 DRL (Weeks 9-12):**\n    *   认识到表格型方法的局限性（维度灾难、连续空间）。\n    *   引入**函数逼近**思想：用带参数的函数（特别是神经网络）近似价值函数或策略。\n    *   学习**深度 Q 网络 (DQN):** 结合 Q-Learning 与神经网络，利用经验回放和目标网络提高稳定性（处理离散动作）。\n    *   学习**策略梯度 (PG) 方法:** 直接优化参数化策略 π(a|s, θ)。\n    *   理解**策略梯度定理**和 REINFORCE 算法（高方差问题）。\n    *   学习**Actor-Critic 方法 (A2C):** 结合策略学习 (Actor) 和价值学习 (Critic)，降低方差，提高稳定性，可处理连续动作。\n    *   掌握使用 **Stable Baselines3** 库运行 DQN 和 A2C 实验。\n\n4.  **商业应用、挑战与伦理 (Weeks 13-15):**\n    *   深入分析 RL 在**动态定价/资源优化**和**个性化推荐/营销**中的应用。\n    *   探讨 MDP 定义、奖励设计、数据需求、算法选择等关键环节。\n    *   总结 RL 落地实践中的**共性挑战**：数据、模拟、奖励、安全、部署、维护等。\n    *   强调**负责任 AI** 的重要性，讨论公平性、透明度、隐私等**伦理规范**。\n    *   进行期末项目选题指导。\n\n# 核心概念再梳理\n\n让我们再次巩固一些贯穿始终的核心概念：\n\n*   **MDP:** 描述问题的框架，理解 S, A, R, P, γ 至关重要。商业问题的关键在于如何**定义**这些元素。\n*   **价值函数 (V/Q):** 衡量“好坏”的标准，是许多算法的核心。理解 Vπ, Qπ, V\\*, Q\\* 的区别。\n*   **Bellman 方程:** 连接当前价值与未来价值的桥梁，是理解 TD 学习和动态规划的基础。\n*   **无模型学习:** 现实中模型往往未知，MC 和 TD 是两大基石。\n*   **偏差-方差权衡:** MC (无偏, 高方差) vs. TD (有偏, 低方差)。\n*   **预测 vs. 控制:** 评估现有策略 vs. 寻找最优策略。\n*   **同策略 vs. 异策略:** 学习的策略是否与收集数据的策略相同？(SARSA vs. Q-Learning)。\n*   **函数逼近:** 应对大规模/连续问题的关键，泛化能力是核心优势。\n*   **DRL 技巧:** 经验回放、目标网络 (DQN)，Actor-Critic 结构 (A2C) 是为了解决函数逼近带来的稳定性问题。\n*   **奖励工程:** 设计能够准确反映长期商业目标的奖励函数是 RL 应用成功的关键，也是最具挑战性的环节之一。\n*   **探索与利用:** RL 永恒的主题，需要在收集信息和最大化当前回报之间取得平衡。\n\n# RL 与其他 AI/数据科学技术的结合\n\n强化学习并非孤立存在，它经常与其他 AI 和数据科学技术结合使用，以发挥更大威力：\n\n1.  **结合监督学习 (Supervised Learning, SL):**\n    *   **特征提取:** 使用 SL 模型（如 CNN 处理图像，RNN 处理序列）从原始输入（如用户评论、市场新闻、传感器读数）中提取有意义的状态特征，供 RL 智能体使用。\n    *   **模型构建 (Model-Based RL):** 使用 SL 学习环境模型（预测 P(s'|s, a) 和 R(s, a, s')），然后基于学习到的模型进行规划或生成模拟数据供 Model-Free RL 使用。\n    *   **行为克隆 (Behavioral Cloning):** 使用 SL 模仿专家演示数据，为 RL 提供一个良好的初始策略（预训练）。\n    *   **奖励函数学习 (Inverse Reinforcement Learning, IRL):** 从专家演示中反向推断奖励函数，然后用 RL 优化该奖励函数。\n2.  **结合优化 (Optimization):**\n    *   **RL 作为优化器:** RL 本身可以看作是一种优化方法，用于寻找最大化累积回报的策略（参数）。\n    *   **传统优化方法辅助 RL:**\n        *   **超参数优化:** 使用贝叶斯优化、网格搜索等方法寻找 RL 算法的最佳超参数。\n        *   **约束优化:** 在 RL 框架中加入约束条件（如预算约束、安全约束），使用约束优化技术求解。\n3.  **结合因果推断 (Causal Inference):**\n    *   **理解策略效果:** 在商业场景中，理解 RL 策略改变对业务指标的**因果**影响至关重要（而不仅仅是相关性）。\n    *   **Off-Policy Evaluation:** 使用因果推断的技术（如重要性采样、双重机器学习）更准确地评估一个新策略在历史数据上的表现。\n    *   **处理混淆变量:** 识别和处理影响决策和结果的混淆变量。\n4.  **结合其他数据科学技术:**\n    *   **数据挖掘/分析:** 用于理解数据、发现模式、进行特征工程。\n    *   **A/B 测试:** 用于在线评估和比较不同 RL 策略的效果。\n\n::: {.callout-note title=\"融合是趋势\"}\n未来的智能决策系统很可能是多种 AI 和数据科学技术的融合体，RL 在其中扮演着处理序贯决策和长期优化的关键角色。\n:::\n\n# 前沿方向与商业潜力展望\n\n强化学习仍然是一个快速发展的领域，一些前沿方向在商业应用中展现出巨大潜力：\n\n1.  **离线强化学习 (Offline RL / Batch RL):**\n    *   **背景:** 许多商业场景拥有大量的历史日志数据，但在线交互成本高或风险大。\n    *   **目标:** **仅**使用固定的历史数据集来学习最优策略，而**无需**与环境进行新的交互。\n    *   **挑战:** 分布偏移 (Distribution Shift) - 历史数据的策略与正在学习的策略不同，可能导致价值估计不准或策略表现糟糕。\n    *   **技术:** 通过引入保守主义（如限制策略学习范围、悲观价值估计）来缓解分布偏移问题。\n    *   **商业潜力:** 在推荐、广告、医疗、金融等拥有大量历史数据的领域应用前景广阔，可以更安全、低成本地利用数据进行策略优化。\n2.  **多智能体强化学习 (Multi-Agent RL, MARL):**\n    *   **背景:** 许多商业环境涉及多个相互影响的决策主体（智能体），如市场中的多个竞争公司、共享出行平台上的司机和乘客、协作机器人团队。\n    *   **目标:** 学习能够在这种多智能体环境中有效协作或竞争的策略。\n    *   **挑战:** 环境非平稳性（其他智能体的策略在变）、信用分配（奖励如何在团队中分配）、智能体之间的协调与沟通。\n    *   **商业潜力:** 优化竞争策略、设计协作机制（如供应链协同、车队管理）、理解复杂市场动态。\n3.  **基于模型的强化学习 (Model-Based RL):**\n    *   **思路:** 学习一个环境模型，然后利用这个模型进行规划（如 MCTS）或生成模拟数据训练无模型策略。\n    *   **优点:** 理论上可以提高样本效率（如果模型学得准）。\n    *   **挑战:** 学习准确的环境模型本身就很难，模型误差可能导致策略表现不佳。\n    *   **商业潜力:** 在需要规划能力或样本效率要求极高的场景（如机器人、复杂供应链优化）中有应用价值。\n4.  **表示学习与 RL (Representation Learning):**\n    *   **目标:** 从高维原始输入（如图像、文本）中自动学习有效的低维状态表示，供 RL 算法使用。\n    *   **技术:** 结合自编码器、对比学习、Transformer 等深度学习表示技术。\n    *   **商业潜力:** 使 RL 能够应用于更广泛的、具有非结构化输入的商业问题。\n5.  **可解释性、安全性与公平性:**\n    *   随着 RL 应用日益广泛，如何确保其决策过程透明、结果可信、行为安全、影响公平，是越来越重要的研究方向，也是商业落地必须解决的问题。\n\n# Session 32: 期末项目展示 / 期末考试\n\n根据课程安排，下一次课（Session 32）将用于：\n\n*   **期末项目展示 (Final Project Presentations):** 选择做项目的同学将进行简短展示，分享你们的研究成果、方案设计或文献分析。请准备好 PPT 或演示文稿。\n*   **或 期末考试 (Final Exam):** 如果采用考试形式，将考察整个学期所学的核心概念、算法原理、优缺点比较以及对商业应用的理解。\n\n具体形式和要求将另行通知。\n\n**感谢大家一学期的投入与参与！希望这门课程能够帮助大家打开一扇通往智能决策优化的大门，并为你们未来的学习和职业生涯提供有价值的知识和技能。**"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week16_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 16: 课程总结与未来展望"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}