{"title":"Week 12: Actor-Critic 方法","markdown":{"yaml":{"title":"Week 12: Actor-Critic 方法"},"headingText":"回顾：策略梯度 (Policy Gradient) 与 REINFORCE","containsRefs":false,"markdown":"\n\n\n上周我们学习了策略梯度 (PG) 方法：\n\n*   **核心思想:** 直接参数化策略 π(a|s, **θ**) 并优化参数 **θ** 以最大化预期回报 J(**θ**)。\n*   **优化方式:** 梯度上升 **θ** ← **θ** + α ∇J(**θ**)。\n*   **策略梯度定理:** ∇J(**θ**) = E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * Qπ(S_t, A_t) ] (或使用 G_t)。\n*   **REINFORCE 算法:** 使用蒙特卡洛方法估计 Qπ (即使用完整回报 G_t)。\n    *   ∇J(**θ**) ≈ E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * G_t ]\n    *   **缺点:** 高方差，收敛慢，需要完整回合。\n*   **基线 (Baseline):** 为了减小方差，从回报中减去一个与动作无关的基线 b(S_t)。\n    *   ∇J(**θ**) ≈ E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * (G_t - b(S_t)) ]\n    *   常用的基线是状态值函数 Vπ(S_t)。\n    *   **优势函数 (Advantage Function):** Aπ(S_t, A_t) = Qπ(S_t, A_t) - Vπ(S_t)。\n    *   梯度变为：∇J(**θ**) = E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * Aπ(S_t, A_t) ]\n\n**问题:** 如何在不知道 Qπ 和 Vπ 的情况下，有效地估计优势函数 Aπ 并进行策略更新？\n\n# Actor-Critic 框架\n\nActor-Critic (AC) 方法提供了一个优雅的解决方案，它结合了**策略梯度**和**TD学习**的思想。\n\n**核心思想:** 维护两个参数化的模型（通常是神经网络）：\n\n1.  **Actor (行动者):**\n    *   参数化的**策略** π(a|s, **θ**)。\n    *   负责根据当前状态 s **选择动作** a。\n    *   目标是优化参数 **θ** 以改进策略。\n2.  **Critic (评论家):**\n    *   参数化的**价值函数**（通常是状态值函数 V(s, **w**) 或动作值函数 Q(s, a, **w**)）。\n    *   负责**评估** Actor 选择的动作有多好。\n    *   目标是学习准确的价值估计，参数为 **w**。\n\n**交互流程:**\n\n1.  Actor 根据当前状态 S_t 和策略 π(·|·, **θ**) 选择动作 A_t。\n2.  执行动作 A_t，观察到奖励 R_{t+1} 和下一个状态 S_{t+1}。\n3.  Critic 利用这个转移 (S_t, A_t, R_{t+1}, S_{t+1}) 来**评估**动作 A_t 的好坏，并**更新**其价值函数参数 **w**。\n4.  Actor 利用 Critic 的评估信息来**更新**其策略参数 **θ**。\n\n![Actor-Critic Architecture](https://spinningup.openai.com/en/latest/_images/actor_critic.svg)\n*(图片来源: OpenAI Spinning Up)*\n\n**Critic 如何帮助 Actor？**\n\nCritic 的主要作用是提供一个比蒙特卡洛回报 G_t **方差更低**的信号来指导 Actor 的学习。这通常通过以下方式实现：\n\n*   **计算 TD 误差:** 如果 Critic 学习的是状态值函数 V(s, **w**)，它可以计算 TD 误差：\n    *   δ_t = R_{t+1} + γ V(S_{t+1}, **w**) - V(S_t, **w**)\n    *   这个 TD 误差 δ_t 可以作为优势函数 Aπ(S_t, A_t) 的一个（有偏但低方差的）**估计**。\n*   **更新 Actor:** Actor 使用这个 TD 误差来更新策略参数 **θ**：\n    *   **θ** ← **θ** + α * ∇ log π(A_t|S_t, **θ**) * δ_t\n    *   **直观理解:**\n        *   如果 δ_t > 0 (实际回报 R_{t+1} + 未来预期 γV(S') 比当前预期 V(S) 要好)，说明动作 A_t 是个好动作，增加其概率。\n        *   如果 δ_t < 0 (实际回报比预期差)，说明动作 A_t 是个坏动作，减小其概率。\n*   **更新 Critic:** Critic 也需要学习，通常使用 TD 学习来更新其参数 **w**，目标是最小化 TD 误差（使其对 Vπ 的估计更准确）：\n    *   **w** ← **w** + β * δ_t * ∇ V(S_t, **w**) (β 是 Critic 的学习率)\n\n**优势:**\n\n*   **结合了 PG 和 TD:** 利用 TD 学习的低方差和在线学习能力来改进策略梯度的高方差问题。\n*   **更稳定高效:** 通常比纯粹的 REINFORCE 算法收敛更快、更稳定。\n\n# 优势函数估计 (Advantage Function Estimation)\n\n在 Actor-Critic 中，我们希望使用优势函数 Aπ(S_t, A_t) = Qπ(S_t, A_t) - Vπ(S_t) 来更新 Actor。但我们通常只有状态值函数 V 的估计。\n\n如何估计 Aπ(S_t, A_t)？\n\n回顾 TD 误差：\nδ_t = R_{t+1} + γ V(S_{t+1}, **w**) - V(S_t, **w**)\n\n可以证明，TD 误差 δ_t 是优势函数 Aπ(S_t, A_t) 的一个（有偏）估计：\nE[δ_t | S_t, A_t] ≈ Aπ(S_t, A_t)\n\n因此，许多 Actor-Critic 算法直接使用 TD 误差 δ_t 作为优势函数的估计来更新 Actor：\n\n**Actor 更新:** **θ** ← **θ** + α * ∇ log π(A_t|S_t, **θ**) * δ_t\n**Critic 更新:** **w** ← **w** + β * δ_t * ∇ V(S_t, **w**)\n\n# A2C / A3C 算法概念\n\n**A2C (Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**同步 (Synchronous)**、**确定性 (Deterministic)** 版本。\n*   通常使用**多个并行的环境**实例来收集经验数据。\n*   智能体在所有环境中执行一步，收集一批 (S, A, R, S') 数据。\n*   使用这批数据计算 TD 误差 δ 和梯度，然后**一次性**更新 Actor 和 Critic 的参数。\n*   由于是同步更新，实现相对简单。Stable Baselines3 中的 `A2C` 实现的就是这种思想。\n\n**A3C (Asynchronous Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**异步 (Asynchronous)** 版本，是早期 DRL 的一个里程碑式算法。\n*   **核心思想:** 创建多个并行的 Actor-Learner 线程，每个线程都有自己的环境副本和模型参数副本。\n*   每个线程独立地与环境交互，计算梯度（Actor 和 Critic 的梯度）。\n*   **异步更新:** 各个线程**独立地、异步地**将计算出的梯度应用到**全局共享**的模型参数上。\n*   **优点:** 不需要经验回放缓冲区（异步性本身提供了数据去相关性）；通过并行化提高了训练速度。\n*   **缺点:** 实现相对复杂；异步更新可能导致某些线程使用过时的参数进行计算。\n\n::: {.callout-note title=\"A2C vs. A3C\"}\n实践中发现，A2C（使用并行环境的同步版本）通常能达到与 A3C 相当甚至更好的性能，并且实现更简单、更容易复现。因此，现代框架（如 Stable Baselines3）通常提供 A2C 的实现。\n:::\n\n# Lab 7: 使用 Stable Baselines3 运行 A2C\n\n## 目标\n\n1.  使用 Stable Baselines3 (SB3) 运行 A2C 算法。\n2.  在 CartPole (离散动作) 或 Pendulum (连续动作) 环境上进行实验。\n3.  对比 A2C 和 DQN (在 CartPole 上) 的训练过程和结果。\n4.  理解 Actor-Critic 方法相对于 DQN 的优势（尤其是在处理连续动作空间方面）。\n\n## 环境选择\n\n*   **CartPole-v1:** 离散动作空间。可以与上周的 DQN 进行直接比较。\n*   **Pendulum-v1:** **连续动作空间**。\n    *   目标: 通过施加力矩，将倒立摆摆动到最高点并保持稳定。\n    *   状态: [cos(杆角度), sin(杆角度), 杆角速度] (连续)。\n    *   动作: 施加的力矩 (连续值，通常在 [-2.0, 2.0] 之间)。\n    *   奖励: 与杆子角度和角速度有关，目标是最大化奖励（最小化“成本”）。\n    *   **注意:** DQN 无法直接处理 Pendulum 的连续动作空间，而 A2C 可以。\n\n## 示例代码 (SB3 A2C on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym_a2c/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境 (A2C 通常需要向量化环境)\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=8) # A2C 通常使用更多并行环境\n\n# 2. 定义 A2C 模型\n# A2C 使用 \"MlpPolicy\" 或 \"CnnPolicy\"\n# 关键超参数:\n# n_steps: 每个环境在更新前运行多少步 (影响 TD 估计的长度)\n# vf_coef: 值函数损失的系数 (Critic loss weight)\n# ent_coef: 熵正则化系数 (鼓励探索)\nmodel = A2C(\"MlpPolicy\", vec_env, verbose=1,\n            gamma=0.99,             # 折扣因子\n            n_steps=5,              # 每个环境更新前运行 5 步\n            vf_coef=0.5,            # 值函数损失系数\n            ent_coef=0.0,           # 熵正则化系数 (CartPole 通常不需要太多探索)\n            learning_rate=7e-4,     # 学习率 (A2C 通常用稍高一点的学习率)\n            tensorboard_log=log_dir\n           )\n\n# 3. 训练模型\nprint(\"Starting A2C training on CartPole...\")\nstart_time = time.time()\nmodel.learn(total_timesteps=100000, log_interval=50) # 训练步数与 DQN 保持一致\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"a2c_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained A2C model...\")\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results (A2C): Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\nvec_env.close()\neval_env.close()\n\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n\n# --- (可选) 运行 A2C on Pendulum-v1 ---\n# print(\"\\nStarting A2C training on Pendulum...\")\n# log_dir_pendulum = \"/tmp/gym_a2c_pendulum/\"\n# os.makedirs(log_dir_pendulum, exist_ok=True)\n# vec_env_pendulum = make_vec_env(\"Pendulum-v1\", n_envs=8)\n# model_pendulum = A2C(\"MlpPolicy\", vec_env_pendulum, verbose=1,\n#                      gamma=0.99,\n#                      n_steps=5,\n#                      vf_coef=0.5,\n#                      ent_coef=0.0, # Pendulum 可能需要一点熵正则化\n#                      learning_rate=7e-4,\n#                      tensorboard_log=log_dir_pendulum\n#                     )\n# start_time = time.time()\n# model_pendulum.learn(total_timesteps=200000, log_interval=50) # Pendulum 可能需要更多步数\n# end_time = time.time()\n# print(f\"Pendulum training finished in {end_time - start_time:.2f} seconds.\")\n# model_path_pendulum = os.path.join(log_dir_pendulum, \"a2c_pendulum_sb3\")\n# model_pendulum.save(model_path_pendulum)\n# print(f\"Pendulum model saved to {model_path_pendulum}.zip\")\n\n# print(\"Evaluating trained A2C model on Pendulum...\")\n# eval_env_pendulum = gym.make(\"Pendulum-v1\")\n# mean_reward_p, std_reward_p = evaluate_policy(model_pendulum, eval_env_pendulum, n_eval_episodes=10, deterministic=True)\n# print(f\"Evaluation results (A2C on Pendulum): Mean reward = {mean_reward_p:.2f} +/- {std_reward_p:.2f}\")\n# vec_env_pendulum.close()\n# eval_env_pendulum.close()\n# print(f\"To view Pendulum training logs, run: tensorboard --logdir {log_dir_pendulum}\")\n```\n\n## 任务与思考\n\n1.  **运行 A2C on CartPole:** 运行代码的前半部分（CartPole）。使用 TensorBoard 观察训练曲线 (`rollout/ep_rew_mean`)。查看最终的评估结果。\n2.  **对比 A2C 与 DQN (CartPole):**\n    *   比较 A2C 和上周 DQN 在 CartPole 上的**收敛速度**（达到相似性能所需的步数）和**最终性能**（评估奖励）。哪个表现更好或更快？（注意：超参数可能需要调整才能公平比较）。\n    *   考虑两种算法的**样本效率**。哪个算法似乎需要更少的交互步数来学习？（提示：DQN 使用经验回放，A2C 通常是 On-Policy）。\n3.  **(可选) 运行 A2C on Pendulum:** 取消注释代码的后半部分，运行 A2C 解决 Pendulum-v1 问题。观察训练曲线和评估结果。思考为什么 DQN 无法直接用于此任务，而 A2C 可以？\n4.  **分析 Actor-Critic:**\n    *   解释 Actor-Critic 框架如何结合策略学习和价值学习。\n    *   Critic 在 Actor-Critic 中扮演什么角色？它如何帮助 Actor 学习？\n    *   什么是优势函数？为什么在策略梯度更新中使用优势函数估计（如 TD 误差）通常比使用原始回报更好？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 A2C 代码 (至少包含 CartPole 部分)。\n*   提交 A2C 在 CartPole 上的 TensorBoard 训练曲线截图。\n*   提交 A2C 在 CartPole 上的评估结果。\n*   提交一份简短的分析报告，讨论：\n    *   A2C 与 DQN 在 CartPole 上的性能对比（收敛速度、最终性能、可能的样本效率差异）。\n    *   (如果运行了 Pendulum) 解释为什么 A2C 适用于连续动作空间而 DQN 不适用。\n    *   Actor-Critic 框架的基本原理以及 Critic 的作用。\n    *   优势函数及其在降低方差方面的作用。\n\n---\n\n**下周预告:** 商业案例分析 1 - 动态定价/资源优化。我们将深入探讨如何将前面学到的 RL 概念（MDP 定义、价值函数、Q-Learning、DQN、A2C 等）应用于更具体的商业场景，并讨论其中的挑战。","srcMarkdownNoYaml":"\n\n# 回顾：策略梯度 (Policy Gradient) 与 REINFORCE\n\n上周我们学习了策略梯度 (PG) 方法：\n\n*   **核心思想:** 直接参数化策略 π(a|s, **θ**) 并优化参数 **θ** 以最大化预期回报 J(**θ**)。\n*   **优化方式:** 梯度上升 **θ** ← **θ** + α ∇J(**θ**)。\n*   **策略梯度定理:** ∇J(**θ**) = E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * Qπ(S_t, A_t) ] (或使用 G_t)。\n*   **REINFORCE 算法:** 使用蒙特卡洛方法估计 Qπ (即使用完整回报 G_t)。\n    *   ∇J(**θ**) ≈ E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * G_t ]\n    *   **缺点:** 高方差，收敛慢，需要完整回合。\n*   **基线 (Baseline):** 为了减小方差，从回报中减去一个与动作无关的基线 b(S_t)。\n    *   ∇J(**θ**) ≈ E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * (G_t - b(S_t)) ]\n    *   常用的基线是状态值函数 Vπ(S_t)。\n    *   **优势函数 (Advantage Function):** Aπ(S_t, A_t) = Qπ(S_t, A_t) - Vπ(S_t)。\n    *   梯度变为：∇J(**θ**) = E_{π_θ} [ ∇ log π(A_t|S_t, **θ**) * Aπ(S_t, A_t) ]\n\n**问题:** 如何在不知道 Qπ 和 Vπ 的情况下，有效地估计优势函数 Aπ 并进行策略更新？\n\n# Actor-Critic 框架\n\nActor-Critic (AC) 方法提供了一个优雅的解决方案，它结合了**策略梯度**和**TD学习**的思想。\n\n**核心思想:** 维护两个参数化的模型（通常是神经网络）：\n\n1.  **Actor (行动者):**\n    *   参数化的**策略** π(a|s, **θ**)。\n    *   负责根据当前状态 s **选择动作** a。\n    *   目标是优化参数 **θ** 以改进策略。\n2.  **Critic (评论家):**\n    *   参数化的**价值函数**（通常是状态值函数 V(s, **w**) 或动作值函数 Q(s, a, **w**)）。\n    *   负责**评估** Actor 选择的动作有多好。\n    *   目标是学习准确的价值估计，参数为 **w**。\n\n**交互流程:**\n\n1.  Actor 根据当前状态 S_t 和策略 π(·|·, **θ**) 选择动作 A_t。\n2.  执行动作 A_t，观察到奖励 R_{t+1} 和下一个状态 S_{t+1}。\n3.  Critic 利用这个转移 (S_t, A_t, R_{t+1}, S_{t+1}) 来**评估**动作 A_t 的好坏，并**更新**其价值函数参数 **w**。\n4.  Actor 利用 Critic 的评估信息来**更新**其策略参数 **θ**。\n\n![Actor-Critic Architecture](https://spinningup.openai.com/en/latest/_images/actor_critic.svg)\n*(图片来源: OpenAI Spinning Up)*\n\n**Critic 如何帮助 Actor？**\n\nCritic 的主要作用是提供一个比蒙特卡洛回报 G_t **方差更低**的信号来指导 Actor 的学习。这通常通过以下方式实现：\n\n*   **计算 TD 误差:** 如果 Critic 学习的是状态值函数 V(s, **w**)，它可以计算 TD 误差：\n    *   δ_t = R_{t+1} + γ V(S_{t+1}, **w**) - V(S_t, **w**)\n    *   这个 TD 误差 δ_t 可以作为优势函数 Aπ(S_t, A_t) 的一个（有偏但低方差的）**估计**。\n*   **更新 Actor:** Actor 使用这个 TD 误差来更新策略参数 **θ**：\n    *   **θ** ← **θ** + α * ∇ log π(A_t|S_t, **θ**) * δ_t\n    *   **直观理解:**\n        *   如果 δ_t > 0 (实际回报 R_{t+1} + 未来预期 γV(S') 比当前预期 V(S) 要好)，说明动作 A_t 是个好动作，增加其概率。\n        *   如果 δ_t < 0 (实际回报比预期差)，说明动作 A_t 是个坏动作，减小其概率。\n*   **更新 Critic:** Critic 也需要学习，通常使用 TD 学习来更新其参数 **w**，目标是最小化 TD 误差（使其对 Vπ 的估计更准确）：\n    *   **w** ← **w** + β * δ_t * ∇ V(S_t, **w**) (β 是 Critic 的学习率)\n\n**优势:**\n\n*   **结合了 PG 和 TD:** 利用 TD 学习的低方差和在线学习能力来改进策略梯度的高方差问题。\n*   **更稳定高效:** 通常比纯粹的 REINFORCE 算法收敛更快、更稳定。\n\n# 优势函数估计 (Advantage Function Estimation)\n\n在 Actor-Critic 中，我们希望使用优势函数 Aπ(S_t, A_t) = Qπ(S_t, A_t) - Vπ(S_t) 来更新 Actor。但我们通常只有状态值函数 V 的估计。\n\n如何估计 Aπ(S_t, A_t)？\n\n回顾 TD 误差：\nδ_t = R_{t+1} + γ V(S_{t+1}, **w**) - V(S_t, **w**)\n\n可以证明，TD 误差 δ_t 是优势函数 Aπ(S_t, A_t) 的一个（有偏）估计：\nE[δ_t | S_t, A_t] ≈ Aπ(S_t, A_t)\n\n因此，许多 Actor-Critic 算法直接使用 TD 误差 δ_t 作为优势函数的估计来更新 Actor：\n\n**Actor 更新:** **θ** ← **θ** + α * ∇ log π(A_t|S_t, **θ**) * δ_t\n**Critic 更新:** **w** ← **w** + β * δ_t * ∇ V(S_t, **w**)\n\n# A2C / A3C 算法概念\n\n**A2C (Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**同步 (Synchronous)**、**确定性 (Deterministic)** 版本。\n*   通常使用**多个并行的环境**实例来收集经验数据。\n*   智能体在所有环境中执行一步，收集一批 (S, A, R, S') 数据。\n*   使用这批数据计算 TD 误差 δ 和梯度，然后**一次性**更新 Actor 和 Critic 的参数。\n*   由于是同步更新，实现相对简单。Stable Baselines3 中的 `A2C` 实现的就是这种思想。\n\n**A3C (Asynchronous Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**异步 (Asynchronous)** 版本，是早期 DRL 的一个里程碑式算法。\n*   **核心思想:** 创建多个并行的 Actor-Learner 线程，每个线程都有自己的环境副本和模型参数副本。\n*   每个线程独立地与环境交互，计算梯度（Actor 和 Critic 的梯度）。\n*   **异步更新:** 各个线程**独立地、异步地**将计算出的梯度应用到**全局共享**的模型参数上。\n*   **优点:** 不需要经验回放缓冲区（异步性本身提供了数据去相关性）；通过并行化提高了训练速度。\n*   **缺点:** 实现相对复杂；异步更新可能导致某些线程使用过时的参数进行计算。\n\n::: {.callout-note title=\"A2C vs. A3C\"}\n实践中发现，A2C（使用并行环境的同步版本）通常能达到与 A3C 相当甚至更好的性能，并且实现更简单、更容易复现。因此，现代框架（如 Stable Baselines3）通常提供 A2C 的实现。\n:::\n\n# Lab 7: 使用 Stable Baselines3 运行 A2C\n\n## 目标\n\n1.  使用 Stable Baselines3 (SB3) 运行 A2C 算法。\n2.  在 CartPole (离散动作) 或 Pendulum (连续动作) 环境上进行实验。\n3.  对比 A2C 和 DQN (在 CartPole 上) 的训练过程和结果。\n4.  理解 Actor-Critic 方法相对于 DQN 的优势（尤其是在处理连续动作空间方面）。\n\n## 环境选择\n\n*   **CartPole-v1:** 离散动作空间。可以与上周的 DQN 进行直接比较。\n*   **Pendulum-v1:** **连续动作空间**。\n    *   目标: 通过施加力矩，将倒立摆摆动到最高点并保持稳定。\n    *   状态: [cos(杆角度), sin(杆角度), 杆角速度] (连续)。\n    *   动作: 施加的力矩 (连续值，通常在 [-2.0, 2.0] 之间)。\n    *   奖励: 与杆子角度和角速度有关，目标是最大化奖励（最小化“成本”）。\n    *   **注意:** DQN 无法直接处理 Pendulum 的连续动作空间，而 A2C 可以。\n\n## 示例代码 (SB3 A2C on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym_a2c/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境 (A2C 通常需要向量化环境)\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=8) # A2C 通常使用更多并行环境\n\n# 2. 定义 A2C 模型\n# A2C 使用 \"MlpPolicy\" 或 \"CnnPolicy\"\n# 关键超参数:\n# n_steps: 每个环境在更新前运行多少步 (影响 TD 估计的长度)\n# vf_coef: 值函数损失的系数 (Critic loss weight)\n# ent_coef: 熵正则化系数 (鼓励探索)\nmodel = A2C(\"MlpPolicy\", vec_env, verbose=1,\n            gamma=0.99,             # 折扣因子\n            n_steps=5,              # 每个环境更新前运行 5 步\n            vf_coef=0.5,            # 值函数损失系数\n            ent_coef=0.0,           # 熵正则化系数 (CartPole 通常不需要太多探索)\n            learning_rate=7e-4,     # 学习率 (A2C 通常用稍高一点的学习率)\n            tensorboard_log=log_dir\n           )\n\n# 3. 训练模型\nprint(\"Starting A2C training on CartPole...\")\nstart_time = time.time()\nmodel.learn(total_timesteps=100000, log_interval=50) # 训练步数与 DQN 保持一致\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"a2c_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained A2C model...\")\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results (A2C): Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\nvec_env.close()\neval_env.close()\n\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n\n# --- (可选) 运行 A2C on Pendulum-v1 ---\n# print(\"\\nStarting A2C training on Pendulum...\")\n# log_dir_pendulum = \"/tmp/gym_a2c_pendulum/\"\n# os.makedirs(log_dir_pendulum, exist_ok=True)\n# vec_env_pendulum = make_vec_env(\"Pendulum-v1\", n_envs=8)\n# model_pendulum = A2C(\"MlpPolicy\", vec_env_pendulum, verbose=1,\n#                      gamma=0.99,\n#                      n_steps=5,\n#                      vf_coef=0.5,\n#                      ent_coef=0.0, # Pendulum 可能需要一点熵正则化\n#                      learning_rate=7e-4,\n#                      tensorboard_log=log_dir_pendulum\n#                     )\n# start_time = time.time()\n# model_pendulum.learn(total_timesteps=200000, log_interval=50) # Pendulum 可能需要更多步数\n# end_time = time.time()\n# print(f\"Pendulum training finished in {end_time - start_time:.2f} seconds.\")\n# model_path_pendulum = os.path.join(log_dir_pendulum, \"a2c_pendulum_sb3\")\n# model_pendulum.save(model_path_pendulum)\n# print(f\"Pendulum model saved to {model_path_pendulum}.zip\")\n\n# print(\"Evaluating trained A2C model on Pendulum...\")\n# eval_env_pendulum = gym.make(\"Pendulum-v1\")\n# mean_reward_p, std_reward_p = evaluate_policy(model_pendulum, eval_env_pendulum, n_eval_episodes=10, deterministic=True)\n# print(f\"Evaluation results (A2C on Pendulum): Mean reward = {mean_reward_p:.2f} +/- {std_reward_p:.2f}\")\n# vec_env_pendulum.close()\n# eval_env_pendulum.close()\n# print(f\"To view Pendulum training logs, run: tensorboard --logdir {log_dir_pendulum}\")\n```\n\n## 任务与思考\n\n1.  **运行 A2C on CartPole:** 运行代码的前半部分（CartPole）。使用 TensorBoard 观察训练曲线 (`rollout/ep_rew_mean`)。查看最终的评估结果。\n2.  **对比 A2C 与 DQN (CartPole):**\n    *   比较 A2C 和上周 DQN 在 CartPole 上的**收敛速度**（达到相似性能所需的步数）和**最终性能**（评估奖励）。哪个表现更好或更快？（注意：超参数可能需要调整才能公平比较）。\n    *   考虑两种算法的**样本效率**。哪个算法似乎需要更少的交互步数来学习？（提示：DQN 使用经验回放，A2C 通常是 On-Policy）。\n3.  **(可选) 运行 A2C on Pendulum:** 取消注释代码的后半部分，运行 A2C 解决 Pendulum-v1 问题。观察训练曲线和评估结果。思考为什么 DQN 无法直接用于此任务，而 A2C 可以？\n4.  **分析 Actor-Critic:**\n    *   解释 Actor-Critic 框架如何结合策略学习和价值学习。\n    *   Critic 在 Actor-Critic 中扮演什么角色？它如何帮助 Actor 学习？\n    *   什么是优势函数？为什么在策略梯度更新中使用优势函数估计（如 TD 误差）通常比使用原始回报更好？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 A2C 代码 (至少包含 CartPole 部分)。\n*   提交 A2C 在 CartPole 上的 TensorBoard 训练曲线截图。\n*   提交 A2C 在 CartPole 上的评估结果。\n*   提交一份简短的分析报告，讨论：\n    *   A2C 与 DQN 在 CartPole 上的性能对比（收敛速度、最终性能、可能的样本效率差异）。\n    *   (如果运行了 Pendulum) 解释为什么 A2C 适用于连续动作空间而 DQN 不适用。\n    *   Actor-Critic 框架的基本原理以及 Critic 的作用。\n    *   优势函数及其在降低方差方面的作用。\n\n---\n\n**下周预告:** 商业案例分析 1 - 动态定价/资源优化。我们将深入探讨如何将前面学到的 RL 概念（MDP 定义、价值函数、Q-Learning、DQN、A2C 等）应用于更具体的商业场景，并讨论其中的挑战。"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week12_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 12: Actor-Critic 方法"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}