{"title":"Week 12: Actor-Critic 方法","markdown":{"yaml":{"title":"Week 12: Actor-Critic 方法"},"headingText":"回顾：策略梯度 (Policy Gradient) 与 REINFORCE","containsRefs":false,"markdown":"\n\n\n上周我们学习了策略梯度 (PG) 方法：\n\n*   **核心思想:** 直接参数化策略 $\\pi(a|s, \\theta)$ 并优化参数 $\\theta$ 以最大化预期回报 $J(\\theta)$。\n*   **优化方式:** 梯度上升 $\\theta \\leftarrow \\theta + \\alpha \\nabla J(\\theta)$。\n*   **策略梯度定理:** $\\nabla J(\\theta) = E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * Q_{\\pi}(S_t, A_t) ]$ (或使用 $G_t$)。\n*   **REINFORCE 算法:** 使用蒙特卡洛方法估计 $Q_{\\pi}$ (即使用完整回报 $G_t$)。\n    *   $\\nabla J(\\theta) \\approx E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * G_t ]$\n    *   **缺点:** 高方差，收敛慢，需要完整回合。\n*   **基线 (Baseline):** 为了减小方差，从回报中减去一个与动作无关的基线 $b(S_t)$。\n    *   $\\nabla J(\\theta) \\approx E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * (G_t - b(S_t)) ]$\n    *   常用的基线是状态值函数 $V_{\\pi}(S_t)$。\n    *   **优势函数 (Advantage Function):** $A_{\\pi}(S_t, A_t) = Q_{\\pi}(S_t, A_t) - V_{\\pi}(S_t)$。\n    *   梯度变为：$\\nabla J(\\theta) = E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * A_{\\pi}(S_t, A_t) ]$\n\n**问题:** 如何在不知道 $Q_{\\pi}$ 和 $V_{\\pi}$ 的情况下，有效地估计优势函数 $A_{\\pi}$ 并进行策略更新？\n\n# Actor-Critic 框架\n\nActor-Critic (AC) 方法提供了一个优雅的解决方案，它结合了**策略梯度**和**TD学习**的思想。\n\n**核心思想:** 维护两个参数化的模型（通常是神经网络）：\n\n1.  **Actor (行动者):**\n    *   参数化的**策略** $\\pi(a|s, \\theta)$。\n    *   负责根据当前状态 $s$ **选择动作** $a$。\n    *   目标是优化参数 $\\theta$ 以改进策略。\n2.  **Critic (评论家):**\n    *   参数化的**价值函数**（通常是状态值函数 $V(s, w)$ 或动作值函数 $Q(s, a, w)$）。\n    *   负责**评估** Actor 选择的动作有多好。\n    *   目标是学习准确的价值估计，参数为 $w$。\n\n**交互流程:**\n\n1.  Actor 根据当前状态 $S_t$ 和策略 $\\pi(·|·, \\theta)$ 选择动作 $A_t$。\n2.  执行动作 $A_t$，观察到奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。\n3.  Critic 利用这个转移 $(S_t, A_t, R_{t+1}, S_{t+1})$ 来**评估**动作 $A_t$ 的好坏，并**更新**其价值函数参数 $w$。\n4.  Actor 利用 Critic 的评估信息来**更新**其策略参数 $\\theta$。\n\n![Actor-Critic Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20250224190459513673/Actor-Critic-Method.webp)\n*(图片来源: https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning)*\n\n**Critic 如何帮助 Actor？**\n\nCritic 的主要作用是提供一个比蒙特卡洛回报 $G_t$ **方差更低**的信号来指导 Actor 的学习。这通常通过以下方式实现：\n\n*   **计算 TD 误差:** 如果 Critic 学习的是状态值函数 $V(s, w)$，它可以计算 TD 误差：\n    *   $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}, w) - V(S_t, w)$\n    *   这个 TD 误差 $\\delta_t$ 可以作为优势函数 $A_{\\pi}(S_t, A_t)$ 的一个（有偏但低方差的）**估计**。\n*   **更新 Actor:** Actor 使用这个 TD 误差来更新策略参数 $\\theta$：\n    *   $\\theta \\leftarrow \\theta + \\alpha * \\nabla \\log \\pi(A_t|S_t, \\theta) * \\delta_t$\n    *   **直观理解:**\n        *   如果 $\\delta_t > 0$ (实际回报 $R_{t+1} + \\gamma V(S')$ 比当前预期 $V(S)$ 要好)，说明动作 $A_t$ 是个好动作，增加其概率。\n        *   如果 $\\delta_t < 0$ (实际回报比预期差)，说明动作 $A_t$ 是个坏动作，减小其概率。\n*   **更新 Critic:** Critic 也需要学习，通常使用 TD 学习来更新其参数 $w$，目标是最小化 TD 误差（使其对 $V_{\\pi}$ 的估计更准确）：\n    *   $w \\leftarrow w + \\beta * \\delta_t * \\nabla V(S_t, w)$ ($\\beta$ 是 Critic 的学习率)\n\n::: {.callout-note title=\"为什么 TD 误差是优势函数的估计?\" collapse=\"true\"}\nTD 误差 $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ 可以作为优势函数 $A_{\\pi}(S_t, A_t)$ 的估计，原因如下：\n\n1. **数学期望关系**:\n   - 优势函数定义为 $A_{\\pi}(s,a) = Q_{\\pi}(s,a) - V_{\\pi}(s)$\n   - TD 误差的期望满足: $E_{\\pi}[\\delta_t | S_t, A_t] = Q_{\\pi}(S_t, A_t) - V_{\\pi}(S_t) = A_{\\pi}(S_t, A_t)$\n   \n2. **直观理解**:\n   - $R_{t+1} + \\gamma V(S_{t+1})$ 是 $Q_{\\pi}(S_t, A_t)$ 的采样估计\n   - 减去 $V(S_t)$ 后，$\\delta_t$ 衡量了\"实际获得的回报\"与\"状态平均价值\"的差异\n   - 正值表示动作比平均好，负值表示比平均差\n\n3. **实际优势**:\n   - 相比蒙特卡洛回报 $G_t$，TD误差方差更低\n   - 不需要等待回合结束，支持在线学习\n   - 计算简单，只需一步转移 $(S_t,A_t,R_{t+1},S_{t+1})$\n:::\n\n\n::: {.callout-tip title=\"Actor-Critic 方法的优势\"}\n*   **结合策略梯度与TD学习:** 巧妙融合了策略梯度(PG)的直接策略优化能力和TD学习的低方差优势，有效改善了传统REINFORCE算法的高方差问题。\n*   **训练更稳定高效:** 相比蒙特卡洛式的REINFORCE算法，通常具有更快的收敛速度和更稳定的训练过程。\n:::\n\n# A2C / A3C 算法概念\n\n**A2C (Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**同步 (Synchronous)**、**确定性 (Deterministic)** 版本，其概念与 A3C (由 Mnih 等人于 2016 年提出) 紧密相连。\n*   通常使用**多个并行的环境**实例来收集经验数据。\n*   智能体在所有环境中执行一步，收集一批 (S, A, R, S') 数据。\n*   使用这批数据计算 TD 误差 δ 和梯度，然后**一次性**更新 Actor 和 Critic 的参数。\n*   由于是同步更新，实现相对简单。Stable Baselines3 中的 `A2C` 实现的就是这种思想。\n\n**A3C (Asynchronous Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**异步 (Asynchronous)** 版本，由 Mnih 等人于 2016 年在论文《Asynchronous Methods for Deep Reinforcement Learning》中首次提出，是早期深度强化学习 (DRL) 的一个重要里程碑式算法。\n*   **核心思想:** 创建多个并行的 Actor-Learner 线程，每个线程都有自己的环境副本和模型参数副本。\n*   每个线程独立地与环境交互，计算梯度（Actor 和 Critic 的梯度）。\n*   **异步更新:** 各个线程**独立地、异步地**将计算出的梯度应用到**全局共享**的模型参数上。\n*   **优点:** 不需要经验回放缓冲区（异步性本身提供了数据去相关性）；通过并行化提高了训练速度。\n*   **缺点:** 实现相对复杂；异步更新可能导致某些线程使用过时的参数进行计算。\n\n::: {.callout-note title=\"A2C vs. A3C 实践对比\"}\n研究表明，A2C（同步并行版本）在大多数实际任务中表现优于A3C，主要体现在：\n1. **性能更优**：同步更新机制使训练更稳定，通常能获得更高的最终回报\n2. **实现简单**：避免了异步更新的复杂性和潜在参数冲突\n3. **复现性强**：确定性更新过程确保实验结果可重复\n4. **资源利用率高**：能更好地利用现代GPU的并行计算能力\n\n因此主流框架(如Stable Baselines3)优先支持A2C实现，A3C更多具有历史研究价值。\n:::\n\n::: {.callout-note title=\"其他 Actor-Critic 变体与发展\" collapse=\"true\"}\nA2C 和 A3C 是 Actor-Critic 思想的重要实现，奠定了基础。在此之后，研究者们提出了许多更先进和性能更强的 Actor-Critic 变体：\n\n*   **DDPG (Deep Deterministic Policy Gradient):** 专门为**连续动作空间**设计。Actor 输出确定的动作（而不是概率分布），Critic 学习 Q(s,a)。结合了 DQN 中经验回放和目标网络等技巧。\n*   **TRPO (Trust Region Policy Optimization):** 旨在通过限制每次策略更新的幅度（在“信任区域”内进行优化）来保证策略的单调改进，从而提高训练的稳定性。\n*   **PPO (Proximal Policy Optimization):** TRPO 的一种简化版本，通过裁剪目标函数或使用 KL 散度惩罚项来实现类似 TRPO 的稳定更新效果，但实现更简单，计算效率更高。PPO 目前是许多应用中非常流行且效果出色的算法。\n*   **SAC (Soft Actor-Critic):** 一种基于最大熵强化学习框架的 Actor-Critic 算法。它在最大化累积回报的同时，也最大化策略的熵，从而鼓励探索，并能学习到更鲁棒的策略。SAC 在许多连续控制任务上取得了顶尖的性能。\n*   **TD3 (Twin Delayed Deep Deterministic Policy Gradient):** DDPG 的改进版本，通过使用两个 Critic 网络（取较小值）、延迟策略更新和目标策略平滑等技巧来缓解 Q 值过高估计的问题，从而提升了 DDPG 的性能和稳定性。\n\n**后续发展:**\n\n强化学习领域仍然在飞速发展。虽然 PPO 和 SAC 等算法在很多任务上表现优异，但研究者们仍在不断探索新的架构、优化方法和理论，例如：\n\n*   **基于模型的强化学习 (Model-Based RL):** 学习环境模型，然后利用模型进行规划或生成模拟经验。\n*   **离线强化学习 (Offline RL):** 从固定的数据集中学习策略，而无需与环境进行新的交互。\n*   **多智能体强化学习 (Multi-Agent RL):** 多个智能体在共享环境中学习和交互。\n*   **与大型语言模型 (LLM) 的结合:** 探索如何利用 LLM 的知识和推理能力来增强 RL 智能体的学习和决策。\n\n因此，虽然 A2C/A3C 是重要的里程碑，但后续的 PPO、SAC 等算法在性能和稳定性上通常有更佳表现，并且整个领域仍在不断涌现更为突破性的思想和算法。\n:::\n\n\n\n\n\n# Lab 7: 使用 Stable Baselines3 运行 A2C\n\n## 目标\n\n1.  使用 Stable Baselines3 (SB3) 运行 A2C 算法。\n2.  在 CartPole (离散动作) 或 Pendulum (连续动作) 环境上进行实验。\n3.  对比 A2C 和 DQN (在 CartPole 上) 的训练过程和结果。\n4.  理解 Actor-Critic 方法相对于 DQN 的优势（尤其是在处理连续动作空间方面）。\n\n## 环境选择\n\n*   **CartPole-v1:** 离散动作空间。可以与上周的 DQN 进行直接比较。\n*   **Pendulum-v1:** **连续动作空间**。\n    *   目标: 通过施加力矩，将倒立摆摆动到最高点并保持稳定。\n    *   状态: [cos(杆角度), sin(杆角度), 杆角速度] (连续)。\n    *   动作: 施加的力矩 (连续值，通常在 [-2.0, 2.0] 之间)。\n    *   奖励: 与杆子角度和角速度有关，目标是最大化奖励（最小化“成本”）。\n    *   **注意:** DQN 无法直接处理 Pendulum 的连续动作空间，而 A2C 可以。\n\n## 示例代码 (SB3 A2C on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym_a2c/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境 (A2C 通常需要向量化环境)\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=8) # A2C 通常使用更多并行环境\n\n# 2. 定义 A2C 模型\n# A2C 使用 \"MlpPolicy\" 或 \"CnnPolicy\"\n# 关键超参数:\n# n_steps: 每个环境在更新前运行多少步 (影响 TD 估计的长度)\n# vf_coef: 值函数损失的系数 (Critic loss weight)\n# ent_coef: 熵正则化系数 (鼓励探索)\nmodel = A2C(\"MlpPolicy\", vec_env, verbose=1,\n            gamma=0.99,             # 折扣因子\n            n_steps=5,              # 每个环境更新前运行 5 步\n            vf_coef=0.5,            # 值函数损失系数\n            ent_coef=0.0,           # 熵正则化系数 (CartPole 通常不需要太多探索)\n            learning_rate=7e-4,     # 学习率 (A2C 通常用稍高一点的学习率)\n            tensorboard_log=log_dir\n           )\n\n# 3. 训练模型\nprint(\"Starting A2C training on CartPole...\")\nstart_time = time.time()\nmodel.learn(total_timesteps=100000, log_interval=50) # 训练步数与 DQN 保持一致\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"a2c_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained A2C model...\")\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results (A2C): Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\nvec_env.close()\neval_env.close()\n\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n\n# --- (可选) 运行 A2C on Pendulum-v1 ---\n# print(\"\\nStarting A2C training on Pendulum...\")\n# log_dir_pendulum = \"/tmp/gym_a2c_pendulum/\"\n# os.makedirs(log_dir_pendulum, exist_ok=True)\n# vec_env_pendulum = make_vec_env(\"Pendulum-v1\", n_envs=8)\n# model_pendulum = A2C(\"MlpPolicy\", vec_env_pendulum, verbose=1,\n#                      gamma=0.99,\n#                      n_steps=5,\n#                      vf_coef=0.5,\n#                      ent_coef=0.0, # Pendulum 可能需要一点熵正则化\n#                      learning_rate=7e-4,\n#                      tensorboard_log=log_dir_pendulum\n#                     )\n# start_time = time.time()\n# model_pendulum.learn(total_timesteps=200000, log_interval=50) # Pendulum 可能需要更多步数\n# end_time = time.time()\n# print(f\"Pendulum training finished in {end_time - start_time:.2f} seconds.\")\n# model_path_pendulum = os.path.join(log_dir_pendulum, \"a2c_pendulum_sb3\")\n# model_pendulum.save(model_path_pendulum)\n# print(f\"Pendulum model saved to {model_path_pendulum}.zip\")\n\n# print(\"Evaluating trained A2C model on Pendulum...\")\n# eval_env_pendulum = gym.make(\"Pendulum-v1\")\n# mean_reward_p, std_reward_p = evaluate_policy(model_pendulum, eval_env_pendulum, n_eval_episodes=10, deterministic=True)\n# print(f\"Evaluation results (A2C on Pendulum): Mean reward = {mean_reward_p:.2f} +/- {std_reward_p:.2f}\")\n# vec_env_pendulum.close()\n# eval_env_pendulum.close()\n# print(f\"To view Pendulum training logs, run: tensorboard --logdir {log_dir_pendulum}\")\n```\n\n### 网络结构详解 (默认 `MlpPolicy`)\n\n在上面的示例代码中，我们使用了 `A2C(\"MlpPolicy\", vec_env, ...)`。这里的 `\"MlpPolicy\"` 是 Stable Baselines3 (SB3) 提供的一个预定义策略，它为 Actor-Critic 架构构建了基于多层感知机 (Multi-Layer Perceptron, MLP) 的神经网络。虽然代码中没有显式定义每一层的具体参数，但 `MlpPolicy` 会使用一套标准的默认配置。\n\n**通用结构:**\n\n*   **输入层 (Input Layer):** 接收环境的状态观测值。\n*   **隐藏层 (Hidden Layers):** Actor (策略网络) 和 Critic (价值网络) 通常各自拥有独立的隐藏层。对于 `MlpPolicy`，SB3 的默认配置通常是：\n    *   **网络结构 (net_arch):** `[dict(pi=[64, 64], vf=[64, 64])]`。这意味着 Actor (pi) 和 Critic (vf) 各自有**两个包含64个神经单元的隐藏层**。\n    *   **激活函数 (activation_fn):** 默认使用 **Tanh** 作为隐藏层的激活函数。\n*   **输出层 (Output Layer):**\n    *   **Actor 网络:** 输出动作或动作的参数。\n    *   **Critic 网络:** 输出状态的价值估计。\n\n**1. CartPole-v1 环境 (离散动作空间)**\n\n*   **状态观测 (Input):** 4个连续值 (小车位置、小车速度、杆子角度、杆尖速度)。\n*   **Actor 网络 ($\\pi(a|s, \\theta)$):**\n    *   输入层: 4 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 2 个单元 (对应向左和向右两个离散动作)，之后通常连接一个 **Softmax** 激活函数，输出每个动作的选择概率。\n*   **Critic 网络 ($V(s, w)$):**\n    *   输入层: 4 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 $V(s)$。\n\n**2. Pendulum-v1 环境 (连续动作空间)**\n\n*   **状态观测 (Input):** 3个连续值 ($\\cos(\\text{杆角度})$, $\\sin(\\text{杆角度})$, 杆角速度)。\n*   **Actor 网络 ($\\pi(a|s, \\theta)$):**\n    *   输入层: 3 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层:\n        *   对于连续动作空间，Actor 网络通常输出动作分布的参数。SB3 中，对于高斯策略 (Gaussian policy)，网络会输出动作的**均值 (mean)**。Pendulum 环境的动作是1个连续值 (力矩)，所以输出层有1个单元代表该均值 (通常是线性激活后接一个 Tanh 来限制动作范围)。\n        *   动作分布的**标准差 (standard deviation)** 也是学习的一部分。在 SB3 的 `MlpPolicy` 中，标准差（或其对数 `log_std`）通常是独立于状态的可学习参数，并为每个动作维度学习一个。\n*   **Critic 网络 ($V(s, w)$):**\n    *   输入层: 3 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 $V(s)$。\n\n### 如何自定义网络结构\n\n如果你想使用不同于默认设置的网络结构，例如改变隐藏层的数量、每层的神经元数量或激活函数，可以通过在创建 `A2C` 模型时传递 `policy_kwargs` 参数来实现。\n\n下面是一个示例，展示如何为 Actor 和 Critic 网络自定义隐藏层和激活函数：\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport torch.nn as nn # 引入 torch.nn 来指定激活函数\nimport os\n\n# 假设 log_dir 已创建\nlog_dir = \"/tmp/gym_a2c_custom/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n\n# 2. 定义自定义网络结构的参数\n# policy_kwargs 接受一个字典\n# net_arch: 定义 Actor (pi) 和 Critic (vf) 的网络层结构\n#   例如 [128, 128] 表示两个包含128个单元的隐藏层\n# activation_fn: 指定激活函数，例如 nn.ReLU, nn.Tanh, nn.ELU\npolicy_kwargs = dict(\n    net_arch=dict(pi=[128, 64], vf=[128, 64]), # Actor 和 Critic 各自两层，神经元数分别为 128, 64\n    activation_fn=nn.ReLU                     # 使用 ReLU 作为激活函数\n)\n\n# 3. 定义 A2C 模型，并传入 policy_kwargs\nmodel_custom = A2C(\n    \"MlpPolicy\",\n    vec_env,\n    verbose=1,\n    policy_kwargs=policy_kwargs, # 应用自定义网络结构\n    gamma=0.99,\n    n_steps=5,\n    vf_coef=0.5,\n    ent_coef=0.0,\n    learning_rate=7e-4,\n    tensorboard_log=log_dir\n)\n\n# 4. 训练模型\nprint(\"Starting A2C training on CartPole with custom network...\")\nmodel_custom.learn(total_timesteps=50000, log_interval=50) # 训练少量步数作为演示\nprint(\"Custom training finished.\")\n\n# (可选) 打印模型结构来查看 Actor 和 Critic 的网络\n# 注意：这会打印出 PyTorch 模块的详细信息\n# print(\"Actor's network architecture:\")\n# print(model_custom.policy.mlp_extractor.policy_net)\n# print(\"\\nCritic's network architecture:\")\n# print(model_custom.policy.mlp_extractor.value_net)\n\n# 5. 保存和评估 (与之前类似)\nmodel_path_custom = os.path.join(log_dir, \"a2c_cartpole_custom_sb3\")\nmodel_custom.save(model_path_custom)\nprint(f\"Custom model saved to {model_path_custom}.zip\")\n\neval_env_custom = gym.make(\"CartPole-v1\")\nmean_reward_custom, std_reward_custom = evaluate_policy(model_custom, eval_env_custom, n_eval_episodes=10)\nprint(f\"Evaluation results (Custom A2C): Mean reward = {mean_reward_custom:.2f} +/- {std_reward_custom:.2f}\")\n\nvec_env.close()\neval_env_custom.close()\nprint(f\"To view custom training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n**解释 `policy_kwargs`:**\n\n*   `net_arch`:\n    *   你可以为 Actor (`pi`) 和 Critic (`vf`) 指定不同的结构。例如 `dict(pi=[256, 128], vf=[64, 64])`。\n    *   如果共享层，可以这样定义：`[64, 64, dict(pi=[32], vf=[32])]`，表示先有两个共享的64单元层，然后 Actor 和 Critic 各自有一个32单元的输出前一层。但对于 A2C 的 `MlpPolicy`，通常 `pi` 和 `vf` 是独立的路径。\n*   `activation_fn`: 可以从 `torch.nn` 模块中选择不同的激活函数，如 `nn.ReLU`, `nn.Tanh`, `nn.LeakyReLU`, `nn.ELU` 等。\n\n通过这种方式，你可以更灵活地调整模型结构以适应不同任务的复杂性。\n\n## 任务与思考\n\n1.  **运行 A2C on CartPole:** 运行代码的前半部分（CartPole）。使用 TensorBoard 观察训练曲线 (`rollout/ep_rew_mean`)。查看最终的评估结果。\n2.  **对比 A2C 与 DQN (CartPole):**\n    *   比较 A2C 和上周 DQN 在 CartPole 上的**收敛速度**（达到相似性能所需的步数）和**最终性能**（评估奖励）。哪个表现更好或更快？（注意：超参数可能需要调整才能公平比较）。\n    *   考虑两种算法的**样本效率**。哪个算法似乎需要更少的交互步数来学习？（提示：DQN 使用经验回放，A2C 通常是 On-Policy）。\n3.  **(可选) 运行 A2C on Pendulum:** 取消注释代码的后半部分，运行 A2C 解决 Pendulum-v1 问题。观察训练曲线和评估结果。思考为什么 DQN 无法直接用于此任务，而 A2C 可以？\n4.  **分析 Actor-Critic:**\n    *   解释 Actor-Critic 框架如何结合策略学习和价值学习。\n    *   Critic 在 Actor-Critic 中扮演什么角色？它如何帮助 Actor 学习？\n    *   什么是优势函数？为什么在策略梯度更新中使用优势函数估计（如 TD 误差）通常比使用原始回报更好？","srcMarkdownNoYaml":"\n\n# 回顾：策略梯度 (Policy Gradient) 与 REINFORCE\n\n上周我们学习了策略梯度 (PG) 方法：\n\n*   **核心思想:** 直接参数化策略 $\\pi(a|s, \\theta)$ 并优化参数 $\\theta$ 以最大化预期回报 $J(\\theta)$。\n*   **优化方式:** 梯度上升 $\\theta \\leftarrow \\theta + \\alpha \\nabla J(\\theta)$。\n*   **策略梯度定理:** $\\nabla J(\\theta) = E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * Q_{\\pi}(S_t, A_t) ]$ (或使用 $G_t$)。\n*   **REINFORCE 算法:** 使用蒙特卡洛方法估计 $Q_{\\pi}$ (即使用完整回报 $G_t$)。\n    *   $\\nabla J(\\theta) \\approx E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * G_t ]$\n    *   **缺点:** 高方差，收敛慢，需要完整回合。\n*   **基线 (Baseline):** 为了减小方差，从回报中减去一个与动作无关的基线 $b(S_t)$。\n    *   $\\nabla J(\\theta) \\approx E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * (G_t - b(S_t)) ]$\n    *   常用的基线是状态值函数 $V_{\\pi}(S_t)$。\n    *   **优势函数 (Advantage Function):** $A_{\\pi}(S_t, A_t) = Q_{\\pi}(S_t, A_t) - V_{\\pi}(S_t)$。\n    *   梯度变为：$\\nabla J(\\theta) = E_{\\pi_{\\theta}} [ \\nabla \\log \\pi(A_t|S_t, \\theta) * A_{\\pi}(S_t, A_t) ]$\n\n**问题:** 如何在不知道 $Q_{\\pi}$ 和 $V_{\\pi}$ 的情况下，有效地估计优势函数 $A_{\\pi}$ 并进行策略更新？\n\n# Actor-Critic 框架\n\nActor-Critic (AC) 方法提供了一个优雅的解决方案，它结合了**策略梯度**和**TD学习**的思想。\n\n**核心思想:** 维护两个参数化的模型（通常是神经网络）：\n\n1.  **Actor (行动者):**\n    *   参数化的**策略** $\\pi(a|s, \\theta)$。\n    *   负责根据当前状态 $s$ **选择动作** $a$。\n    *   目标是优化参数 $\\theta$ 以改进策略。\n2.  **Critic (评论家):**\n    *   参数化的**价值函数**（通常是状态值函数 $V(s, w)$ 或动作值函数 $Q(s, a, w)$）。\n    *   负责**评估** Actor 选择的动作有多好。\n    *   目标是学习准确的价值估计，参数为 $w$。\n\n**交互流程:**\n\n1.  Actor 根据当前状态 $S_t$ 和策略 $\\pi(·|·, \\theta)$ 选择动作 $A_t$。\n2.  执行动作 $A_t$，观察到奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。\n3.  Critic 利用这个转移 $(S_t, A_t, R_{t+1}, S_{t+1})$ 来**评估**动作 $A_t$ 的好坏，并**更新**其价值函数参数 $w$。\n4.  Actor 利用 Critic 的评估信息来**更新**其策略参数 $\\theta$。\n\n![Actor-Critic Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20250224190459513673/Actor-Critic-Method.webp)\n*(图片来源: https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning)*\n\n**Critic 如何帮助 Actor？**\n\nCritic 的主要作用是提供一个比蒙特卡洛回报 $G_t$ **方差更低**的信号来指导 Actor 的学习。这通常通过以下方式实现：\n\n*   **计算 TD 误差:** 如果 Critic 学习的是状态值函数 $V(s, w)$，它可以计算 TD 误差：\n    *   $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}, w) - V(S_t, w)$\n    *   这个 TD 误差 $\\delta_t$ 可以作为优势函数 $A_{\\pi}(S_t, A_t)$ 的一个（有偏但低方差的）**估计**。\n*   **更新 Actor:** Actor 使用这个 TD 误差来更新策略参数 $\\theta$：\n    *   $\\theta \\leftarrow \\theta + \\alpha * \\nabla \\log \\pi(A_t|S_t, \\theta) * \\delta_t$\n    *   **直观理解:**\n        *   如果 $\\delta_t > 0$ (实际回报 $R_{t+1} + \\gamma V(S')$ 比当前预期 $V(S)$ 要好)，说明动作 $A_t$ 是个好动作，增加其概率。\n        *   如果 $\\delta_t < 0$ (实际回报比预期差)，说明动作 $A_t$ 是个坏动作，减小其概率。\n*   **更新 Critic:** Critic 也需要学习，通常使用 TD 学习来更新其参数 $w$，目标是最小化 TD 误差（使其对 $V_{\\pi}$ 的估计更准确）：\n    *   $w \\leftarrow w + \\beta * \\delta_t * \\nabla V(S_t, w)$ ($\\beta$ 是 Critic 的学习率)\n\n::: {.callout-note title=\"为什么 TD 误差是优势函数的估计?\" collapse=\"true\"}\nTD 误差 $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ 可以作为优势函数 $A_{\\pi}(S_t, A_t)$ 的估计，原因如下：\n\n1. **数学期望关系**:\n   - 优势函数定义为 $A_{\\pi}(s,a) = Q_{\\pi}(s,a) - V_{\\pi}(s)$\n   - TD 误差的期望满足: $E_{\\pi}[\\delta_t | S_t, A_t] = Q_{\\pi}(S_t, A_t) - V_{\\pi}(S_t) = A_{\\pi}(S_t, A_t)$\n   \n2. **直观理解**:\n   - $R_{t+1} + \\gamma V(S_{t+1})$ 是 $Q_{\\pi}(S_t, A_t)$ 的采样估计\n   - 减去 $V(S_t)$ 后，$\\delta_t$ 衡量了\"实际获得的回报\"与\"状态平均价值\"的差异\n   - 正值表示动作比平均好，负值表示比平均差\n\n3. **实际优势**:\n   - 相比蒙特卡洛回报 $G_t$，TD误差方差更低\n   - 不需要等待回合结束，支持在线学习\n   - 计算简单，只需一步转移 $(S_t,A_t,R_{t+1},S_{t+1})$\n:::\n\n\n::: {.callout-tip title=\"Actor-Critic 方法的优势\"}\n*   **结合策略梯度与TD学习:** 巧妙融合了策略梯度(PG)的直接策略优化能力和TD学习的低方差优势，有效改善了传统REINFORCE算法的高方差问题。\n*   **训练更稳定高效:** 相比蒙特卡洛式的REINFORCE算法，通常具有更快的收敛速度和更稳定的训练过程。\n:::\n\n# A2C / A3C 算法概念\n\n**A2C (Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**同步 (Synchronous)**、**确定性 (Deterministic)** 版本，其概念与 A3C (由 Mnih 等人于 2016 年提出) 紧密相连。\n*   通常使用**多个并行的环境**实例来收集经验数据。\n*   智能体在所有环境中执行一步，收集一批 (S, A, R, S') 数据。\n*   使用这批数据计算 TD 误差 δ 和梯度，然后**一次性**更新 Actor 和 Critic 的参数。\n*   由于是同步更新，实现相对简单。Stable Baselines3 中的 `A2C` 实现的就是这种思想。\n\n**A3C (Asynchronous Advantage Actor-Critic):**\n\n*   这是 Actor-Critic 的一个**异步 (Asynchronous)** 版本，由 Mnih 等人于 2016 年在论文《Asynchronous Methods for Deep Reinforcement Learning》中首次提出，是早期深度强化学习 (DRL) 的一个重要里程碑式算法。\n*   **核心思想:** 创建多个并行的 Actor-Learner 线程，每个线程都有自己的环境副本和模型参数副本。\n*   每个线程独立地与环境交互，计算梯度（Actor 和 Critic 的梯度）。\n*   **异步更新:** 各个线程**独立地、异步地**将计算出的梯度应用到**全局共享**的模型参数上。\n*   **优点:** 不需要经验回放缓冲区（异步性本身提供了数据去相关性）；通过并行化提高了训练速度。\n*   **缺点:** 实现相对复杂；异步更新可能导致某些线程使用过时的参数进行计算。\n\n::: {.callout-note title=\"A2C vs. A3C 实践对比\"}\n研究表明，A2C（同步并行版本）在大多数实际任务中表现优于A3C，主要体现在：\n1. **性能更优**：同步更新机制使训练更稳定，通常能获得更高的最终回报\n2. **实现简单**：避免了异步更新的复杂性和潜在参数冲突\n3. **复现性强**：确定性更新过程确保实验结果可重复\n4. **资源利用率高**：能更好地利用现代GPU的并行计算能力\n\n因此主流框架(如Stable Baselines3)优先支持A2C实现，A3C更多具有历史研究价值。\n:::\n\n::: {.callout-note title=\"其他 Actor-Critic 变体与发展\" collapse=\"true\"}\nA2C 和 A3C 是 Actor-Critic 思想的重要实现，奠定了基础。在此之后，研究者们提出了许多更先进和性能更强的 Actor-Critic 变体：\n\n*   **DDPG (Deep Deterministic Policy Gradient):** 专门为**连续动作空间**设计。Actor 输出确定的动作（而不是概率分布），Critic 学习 Q(s,a)。结合了 DQN 中经验回放和目标网络等技巧。\n*   **TRPO (Trust Region Policy Optimization):** 旨在通过限制每次策略更新的幅度（在“信任区域”内进行优化）来保证策略的单调改进，从而提高训练的稳定性。\n*   **PPO (Proximal Policy Optimization):** TRPO 的一种简化版本，通过裁剪目标函数或使用 KL 散度惩罚项来实现类似 TRPO 的稳定更新效果，但实现更简单，计算效率更高。PPO 目前是许多应用中非常流行且效果出色的算法。\n*   **SAC (Soft Actor-Critic):** 一种基于最大熵强化学习框架的 Actor-Critic 算法。它在最大化累积回报的同时，也最大化策略的熵，从而鼓励探索，并能学习到更鲁棒的策略。SAC 在许多连续控制任务上取得了顶尖的性能。\n*   **TD3 (Twin Delayed Deep Deterministic Policy Gradient):** DDPG 的改进版本，通过使用两个 Critic 网络（取较小值）、延迟策略更新和目标策略平滑等技巧来缓解 Q 值过高估计的问题，从而提升了 DDPG 的性能和稳定性。\n\n**后续发展:**\n\n强化学习领域仍然在飞速发展。虽然 PPO 和 SAC 等算法在很多任务上表现优异，但研究者们仍在不断探索新的架构、优化方法和理论，例如：\n\n*   **基于模型的强化学习 (Model-Based RL):** 学习环境模型，然后利用模型进行规划或生成模拟经验。\n*   **离线强化学习 (Offline RL):** 从固定的数据集中学习策略，而无需与环境进行新的交互。\n*   **多智能体强化学习 (Multi-Agent RL):** 多个智能体在共享环境中学习和交互。\n*   **与大型语言模型 (LLM) 的结合:** 探索如何利用 LLM 的知识和推理能力来增强 RL 智能体的学习和决策。\n\n因此，虽然 A2C/A3C 是重要的里程碑，但后续的 PPO、SAC 等算法在性能和稳定性上通常有更佳表现，并且整个领域仍在不断涌现更为突破性的思想和算法。\n:::\n\n\n\n\n\n# Lab 7: 使用 Stable Baselines3 运行 A2C\n\n## 目标\n\n1.  使用 Stable Baselines3 (SB3) 运行 A2C 算法。\n2.  在 CartPole (离散动作) 或 Pendulum (连续动作) 环境上进行实验。\n3.  对比 A2C 和 DQN (在 CartPole 上) 的训练过程和结果。\n4.  理解 Actor-Critic 方法相对于 DQN 的优势（尤其是在处理连续动作空间方面）。\n\n## 环境选择\n\n*   **CartPole-v1:** 离散动作空间。可以与上周的 DQN 进行直接比较。\n*   **Pendulum-v1:** **连续动作空间**。\n    *   目标: 通过施加力矩，将倒立摆摆动到最高点并保持稳定。\n    *   状态: [cos(杆角度), sin(杆角度), 杆角速度] (连续)。\n    *   动作: 施加的力矩 (连续值，通常在 [-2.0, 2.0] 之间)。\n    *   奖励: 与杆子角度和角速度有关，目标是最大化奖励（最小化“成本”）。\n    *   **注意:** DQN 无法直接处理 Pendulum 的连续动作空间，而 A2C 可以。\n\n## 示例代码 (SB3 A2C on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym_a2c/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境 (A2C 通常需要向量化环境)\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=8) # A2C 通常使用更多并行环境\n\n# 2. 定义 A2C 模型\n# A2C 使用 \"MlpPolicy\" 或 \"CnnPolicy\"\n# 关键超参数:\n# n_steps: 每个环境在更新前运行多少步 (影响 TD 估计的长度)\n# vf_coef: 值函数损失的系数 (Critic loss weight)\n# ent_coef: 熵正则化系数 (鼓励探索)\nmodel = A2C(\"MlpPolicy\", vec_env, verbose=1,\n            gamma=0.99,             # 折扣因子\n            n_steps=5,              # 每个环境更新前运行 5 步\n            vf_coef=0.5,            # 值函数损失系数\n            ent_coef=0.0,           # 熵正则化系数 (CartPole 通常不需要太多探索)\n            learning_rate=7e-4,     # 学习率 (A2C 通常用稍高一点的学习率)\n            tensorboard_log=log_dir\n           )\n\n# 3. 训练模型\nprint(\"Starting A2C training on CartPole...\")\nstart_time = time.time()\nmodel.learn(total_timesteps=100000, log_interval=50) # 训练步数与 DQN 保持一致\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"a2c_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained A2C model...\")\neval_env = gym.make(\"CartPole-v1\")\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results (A2C): Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\nvec_env.close()\neval_env.close()\n\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n\n# --- (可选) 运行 A2C on Pendulum-v1 ---\n# print(\"\\nStarting A2C training on Pendulum...\")\n# log_dir_pendulum = \"/tmp/gym_a2c_pendulum/\"\n# os.makedirs(log_dir_pendulum, exist_ok=True)\n# vec_env_pendulum = make_vec_env(\"Pendulum-v1\", n_envs=8)\n# model_pendulum = A2C(\"MlpPolicy\", vec_env_pendulum, verbose=1,\n#                      gamma=0.99,\n#                      n_steps=5,\n#                      vf_coef=0.5,\n#                      ent_coef=0.0, # Pendulum 可能需要一点熵正则化\n#                      learning_rate=7e-4,\n#                      tensorboard_log=log_dir_pendulum\n#                     )\n# start_time = time.time()\n# model_pendulum.learn(total_timesteps=200000, log_interval=50) # Pendulum 可能需要更多步数\n# end_time = time.time()\n# print(f\"Pendulum training finished in {end_time - start_time:.2f} seconds.\")\n# model_path_pendulum = os.path.join(log_dir_pendulum, \"a2c_pendulum_sb3\")\n# model_pendulum.save(model_path_pendulum)\n# print(f\"Pendulum model saved to {model_path_pendulum}.zip\")\n\n# print(\"Evaluating trained A2C model on Pendulum...\")\n# eval_env_pendulum = gym.make(\"Pendulum-v1\")\n# mean_reward_p, std_reward_p = evaluate_policy(model_pendulum, eval_env_pendulum, n_eval_episodes=10, deterministic=True)\n# print(f\"Evaluation results (A2C on Pendulum): Mean reward = {mean_reward_p:.2f} +/- {std_reward_p:.2f}\")\n# vec_env_pendulum.close()\n# eval_env_pendulum.close()\n# print(f\"To view Pendulum training logs, run: tensorboard --logdir {log_dir_pendulum}\")\n```\n\n### 网络结构详解 (默认 `MlpPolicy`)\n\n在上面的示例代码中，我们使用了 `A2C(\"MlpPolicy\", vec_env, ...)`。这里的 `\"MlpPolicy\"` 是 Stable Baselines3 (SB3) 提供的一个预定义策略，它为 Actor-Critic 架构构建了基于多层感知机 (Multi-Layer Perceptron, MLP) 的神经网络。虽然代码中没有显式定义每一层的具体参数，但 `MlpPolicy` 会使用一套标准的默认配置。\n\n**通用结构:**\n\n*   **输入层 (Input Layer):** 接收环境的状态观测值。\n*   **隐藏层 (Hidden Layers):** Actor (策略网络) 和 Critic (价值网络) 通常各自拥有独立的隐藏层。对于 `MlpPolicy`，SB3 的默认配置通常是：\n    *   **网络结构 (net_arch):** `[dict(pi=[64, 64], vf=[64, 64])]`。这意味着 Actor (pi) 和 Critic (vf) 各自有**两个包含64个神经单元的隐藏层**。\n    *   **激活函数 (activation_fn):** 默认使用 **Tanh** 作为隐藏层的激活函数。\n*   **输出层 (Output Layer):**\n    *   **Actor 网络:** 输出动作或动作的参数。\n    *   **Critic 网络:** 输出状态的价值估计。\n\n**1. CartPole-v1 环境 (离散动作空间)**\n\n*   **状态观测 (Input):** 4个连续值 (小车位置、小车速度、杆子角度、杆尖速度)。\n*   **Actor 网络 ($\\pi(a|s, \\theta)$):**\n    *   输入层: 4 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 2 个单元 (对应向左和向右两个离散动作)，之后通常连接一个 **Softmax** 激活函数，输出每个动作的选择概率。\n*   **Critic 网络 ($V(s, w)$):**\n    *   输入层: 4 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 $V(s)$。\n\n**2. Pendulum-v1 环境 (连续动作空间)**\n\n*   **状态观测 (Input):** 3个连续值 ($\\cos(\\text{杆角度})$, $\\sin(\\text{杆角度})$, 杆角速度)。\n*   **Actor 网络 ($\\pi(a|s, \\theta)$):**\n    *   输入层: 3 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层:\n        *   对于连续动作空间，Actor 网络通常输出动作分布的参数。SB3 中，对于高斯策略 (Gaussian policy)，网络会输出动作的**均值 (mean)**。Pendulum 环境的动作是1个连续值 (力矩)，所以输出层有1个单元代表该均值 (通常是线性激活后接一个 Tanh 来限制动作范围)。\n        *   动作分布的**标准差 (standard deviation)** 也是学习的一部分。在 SB3 的 `MlpPolicy` 中，标准差（或其对数 `log_std`）通常是独立于状态的可学习参数，并为每个动作维度学习一个。\n*   **Critic 网络 ($V(s, w)$):**\n    *   输入层: 3 个单元。\n    *   隐藏层 1: 64 个单元 (Tanh 激活)。\n    *   隐藏层 2: 64 个单元 (Tanh 激活)。\n    *   输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 $V(s)$。\n\n### 如何自定义网络结构\n\n如果你想使用不同于默认设置的网络结构，例如改变隐藏层的数量、每层的神经元数量或激活函数，可以通过在创建 `A2C` 模型时传递 `policy_kwargs` 参数来实现。\n\n下面是一个示例，展示如何为 Actor 和 Critic 网络自定义隐藏层和激活函数：\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport torch.nn as nn # 引入 torch.nn 来指定激活函数\nimport os\n\n# 假设 log_dir 已创建\nlog_dir = \"/tmp/gym_a2c_custom/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# 1. 创建环境\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n\n# 2. 定义自定义网络结构的参数\n# policy_kwargs 接受一个字典\n# net_arch: 定义 Actor (pi) 和 Critic (vf) 的网络层结构\n#   例如 [128, 128] 表示两个包含128个单元的隐藏层\n# activation_fn: 指定激活函数，例如 nn.ReLU, nn.Tanh, nn.ELU\npolicy_kwargs = dict(\n    net_arch=dict(pi=[128, 64], vf=[128, 64]), # Actor 和 Critic 各自两层，神经元数分别为 128, 64\n    activation_fn=nn.ReLU                     # 使用 ReLU 作为激活函数\n)\n\n# 3. 定义 A2C 模型，并传入 policy_kwargs\nmodel_custom = A2C(\n    \"MlpPolicy\",\n    vec_env,\n    verbose=1,\n    policy_kwargs=policy_kwargs, # 应用自定义网络结构\n    gamma=0.99,\n    n_steps=5,\n    vf_coef=0.5,\n    ent_coef=0.0,\n    learning_rate=7e-4,\n    tensorboard_log=log_dir\n)\n\n# 4. 训练模型\nprint(\"Starting A2C training on CartPole with custom network...\")\nmodel_custom.learn(total_timesteps=50000, log_interval=50) # 训练少量步数作为演示\nprint(\"Custom training finished.\")\n\n# (可选) 打印模型结构来查看 Actor 和 Critic 的网络\n# 注意：这会打印出 PyTorch 模块的详细信息\n# print(\"Actor's network architecture:\")\n# print(model_custom.policy.mlp_extractor.policy_net)\n# print(\"\\nCritic's network architecture:\")\n# print(model_custom.policy.mlp_extractor.value_net)\n\n# 5. 保存和评估 (与之前类似)\nmodel_path_custom = os.path.join(log_dir, \"a2c_cartpole_custom_sb3\")\nmodel_custom.save(model_path_custom)\nprint(f\"Custom model saved to {model_path_custom}.zip\")\n\neval_env_custom = gym.make(\"CartPole-v1\")\nmean_reward_custom, std_reward_custom = evaluate_policy(model_custom, eval_env_custom, n_eval_episodes=10)\nprint(f\"Evaluation results (Custom A2C): Mean reward = {mean_reward_custom:.2f} +/- {std_reward_custom:.2f}\")\n\nvec_env.close()\neval_env_custom.close()\nprint(f\"To view custom training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n**解释 `policy_kwargs`:**\n\n*   `net_arch`:\n    *   你可以为 Actor (`pi`) 和 Critic (`vf`) 指定不同的结构。例如 `dict(pi=[256, 128], vf=[64, 64])`。\n    *   如果共享层，可以这样定义：`[64, 64, dict(pi=[32], vf=[32])]`，表示先有两个共享的64单元层，然后 Actor 和 Critic 各自有一个32单元的输出前一层。但对于 A2C 的 `MlpPolicy`，通常 `pi` 和 `vf` 是独立的路径。\n*   `activation_fn`: 可以从 `torch.nn` 模块中选择不同的激活函数，如 `nn.ReLU`, `nn.Tanh`, `nn.LeakyReLU`, `nn.ELU` 等。\n\n通过这种方式，你可以更灵活地调整模型结构以适应不同任务的复杂性。\n\n## 任务与思考\n\n1.  **运行 A2C on CartPole:** 运行代码的前半部分（CartPole）。使用 TensorBoard 观察训练曲线 (`rollout/ep_rew_mean`)。查看最终的评估结果。\n2.  **对比 A2C 与 DQN (CartPole):**\n    *   比较 A2C 和上周 DQN 在 CartPole 上的**收敛速度**（达到相似性能所需的步数）和**最终性能**（评估奖励）。哪个表现更好或更快？（注意：超参数可能需要调整才能公平比较）。\n    *   考虑两种算法的**样本效率**。哪个算法似乎需要更少的交互步数来学习？（提示：DQN 使用经验回放，A2C 通常是 On-Policy）。\n3.  **(可选) 运行 A2C on Pendulum:** 取消注释代码的后半部分，运行 A2C 解决 Pendulum-v1 问题。观察训练曲线和评估结果。思考为什么 DQN 无法直接用于此任务，而 A2C 可以？\n4.  **分析 Actor-Critic:**\n    *   解释 Actor-Critic 框架如何结合策略学习和价值学习。\n    *   Critic 在 Actor-Critic 中扮演什么角色？它如何帮助 Actor 学习？\n    *   什么是优势函数？为什么在策略梯度更新中使用优势函数估计（如 TD 误差）通常比使用原始回报更好？"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week12_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 12: Actor-Critic 方法"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}