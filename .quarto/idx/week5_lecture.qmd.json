{"title":"Week 5: 时序差分学习 - 从不完整经验中学习","markdown":{"yaml":{"title":"Week 5: 时序差分学习 - 从不完整经验中学习"},"headingText":"回顾：蒙特卡洛 (MC) 方法","containsRefs":false,"markdown":"\n\n\n上周我们学习了蒙特卡洛 (MC) 方法用于无模型预测：\n\n*   **核心思想:** 通过模拟大量**完整回合**，用样本回报 $G_t$ 的平均值来估计 $V_{\\pi}(s)$ 或 $Q_{\\pi}(s, a)$。\n*   **优点:** 无模型、简单直观、无偏估计。\n*   **缺点:**\n    *   必须等待**回合结束**才能学习和更新价值函数。\n    *   **高方差**，导致收敛可能较慢。\n    *   基本形式只适用于**回合制任务**。\n\n这些缺点，特别是必须等待回合结束，限制了 MC 方法在许多需要快速响应或回合非常长的场景中的应用。本周我们将学习一种更灵活、应用更广泛的无模型预测方法：**时序差分学习 (Temporal-Difference Learning, TD)**。\n\n# 时序差分 (TD) 学习核心思想\n\nTD 学习结合了 MC 方法和动态规划 (Dynamic Programming, DP) 的思想。\n\n*   **像 MC 一样:** TD 直接从与环境交互的**经验**中学习，不需要环境模型 ($P$, $R$)。\n*   **像 DP 一样:** TD 使用**自举 (Bootstrapping)** 的思想，即用**当前估计的价值**来更新之前的估计，而不是等待最终的实际回报。这种思想类似于动态规划中利用已知信息逐步求解未知问题的过程。\n\n**关键区别:**\n\n*   **MC 更新目标:** 使用**完整的回报 $G_t$** 来更新 $V(S_t)$。\n    *   $V(S_t) ← V(S_t) + \\alpha [G_t - V(S_t)]$  ($\\alpha$ 是学习率)\n*   **TD 更新目标:** 使用**一步之后**的奖励 $R_{t+1}$ 和**后继状态的估计价值 $\\gamma V(S_{t+1})$** 来更新 $V(S_t)$。\n    *   $V(S_t) ← V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$\n\n\n::: {.callout-note title=\"TD 方法与 MC 方法的比较\"}\n**TD 方法的优点:**\n\n*   **实时性:** 不需要等待回合结束，可以在每一步之后立即更新，适用于需要快速响应的场景。\n*   **低方差:** 只依赖单步转移，相比 MC 的完整回合回报，方差更小。\n*   **适用性广:** 既适用于回合制任务，也适用于连续任务。\n\n**TD 方法的缺点:**\n\n*   **引入偏差:** 由于使用自举，依赖于当前的价值估计，可能导致偏差。\n*   **收敛性:** 在某些情况下，收敛性可能不如 MC 方法稳定。\n\n**MC 方法的优点:**\n\n*   **无偏性:** 使用实际回报，不依赖估计值，是无偏估计。\n*   **简单直观:** 直接使用完整回合的回报，概念上更容易理解。\n\n**MC 方法的缺点:**\n\n*   **高方差:** 完整回合的回报可能波动较大，导致高方差。\n*   **延迟更新:** 必须等待回合结束才能更新，不适合实时应用。\n*   **仅限回合制:** 基本形式只适用于回合制任务。\n:::\n\n::: {.callout-tip title=\"TD 方法的有效性及理论证明\"}\nTD 方法不仅在实践中表现出色，其有效性也得到了理论证明：\n\n*   **理论保证:** Sutton (1988) 在论文《Learning to Predict by the Methods of Temporal Differences》中首次系统性地提出了 TD 学习算法，并证明了其收敛性。\n*   **收敛性证明:** Tsitsiklis (1994) 在《Asynchronous Stochastic Approximation and Q-Learning》中严格证明了 TD(0) 在马尔可夫决策过程下的收敛性。\n*   **性能优势:** Singh 和 Dayan (1998) 在《Analytical Mean Squared Error Curves for Temporal Difference Learning》中通过理论分析表明，TD 方法在均方误差方面优于 MC 方法。\n*   **实际应用:** Tesauro (1995) 在《Temporal Difference Learning and TD-Gammon》中展示了 TD 方法在西洋双陆棋中的卓越表现，达到了人类专家水平。\n\n这些理论研究和实际应用共同证明了 TD 方法的有效性和实用性，使其成为强化学习领域最重要的算法之一。\n:::\n\n# TD(0) 预测 (最简单的 TD 方法)\n\nTD(0) 是最基础的 TD 预测算法，用于在给定策略 $\\pi$ 的情况下估计 $V_{\\pi}$。\n\n**更新规则:**\n在时间步 $t$，智能体处于状态 $S_t$，执行动作 $A_t$ (根据策略 $\\pi$)，得到奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。TD(0) 使用这个**单步转移 (transition)** $(S_t, A_t, R_{t+1}, S_{t+1})$ 来更新状态 $S_t$ 的价值估计 $V(S_t)$：\n\n$$\nV(S_t) ← V(S_t) + \\alpha [ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) ]\n$$\n\n其中:\n\n*   **$\\alpha$ (Alpha):** 学习率 (Learning Rate)，是一个小的正数 (e.g., 0.1, 0.01)。它控制了每次更新的步长。\n*   $R_{t+1} + \\gamma V(S_{t+1})$: 称为 **TD 目标 (TD Target)**。这是对状态 $S_t$ 应该具有的价值的一个**估计**。它由两部分组成：\n    *   $R_{t+1}$: 实际获得的即时奖励。\n    *   $\\gamma V(S_{t+1})$: 对未来所有奖励的折扣总和的**当前估计** (基于后继状态 $S_{t+1}$ 的当前价值估计 $V(S_{t+1})$ 进行自举)。\n*   $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$: 称为 **TD 误差 (TD Error)**。它衡量了当前估计 $V(S_t)$ 与更“准确”的 TD 目标之间的差距。TD 学习的目标就是通过调整 $V(S_t)$ 来减小这个误差。\n\n::: {.callout-note title=\"TD(0) 的含义\"}\n\"0\" 表示 TD 误差 $\\delta_t$ 只依赖于**下一步** ($t+1$) 的信息 ($R_{t+1}$, $S_{t+1}$)。还有更复杂的 TD($\\lambda$) 算法会考虑未来更多步的信息。\n:::\n\n## 自举 (Bootstrapping)\n\nTD 学习的核心特点是**自举 (Bootstrapping)**。它指的是**用当前的估计值来更新估计值**。\n\n在 TD(0) 中，我们使用 $V(S_{t+1})$（一个估计值）来计算 TD 目标，进而更新 $V(S_t)$（另一个估计值）。\n\n*   **优点:** 使得 TD 可以在每一步之后立即学习，不需要等待回合结束。\n*   **缺点:** 引入了**偏差 (Bias)**。因为 TD 目标 $R_{t+1} + \\gamma V(S_{t+1})$ 本身就依赖于可能不准确的 $V(S_{t+1})$。如果 $V(S_{t+1})$ 的估计不准，那么对 $V(S_t)$ 的更新也会受到影响。\n\n## TD(0) 预测算法伪代码\n\n\n    初始化:\n    π ← 要评估的策略\n    V(s) ← 任意状态值函数 (例如，对所有 s ∈ S，V(s)=0)\n    α ← 学习率 (一个小的正数)\n\n    对每个回合循环:\n    初始化 S (回合的第一个状态)\n    对回合的每一步循环:\n        A ← 根据策略 π 为 S 选择的动作\n        执行动作 A，观察 R, S' (下一个状态)\n        # 核心更新步骤\n        V(S) ← V(S) + α * [R + γ * V(S') - V(S)]\n        S ← S' # 转移到下一个状态\n        如果 S 是终止状态，跳出内层循环 (回合结束)\n\n\n**解释:**\n\n1.  初始化 $V(s)$ 和学习率 $\\alpha$。\n2.  对于每个回合：\n3.  获取起始状态 $S$。\n4.  对于回合中的每一步：\n5.  根据策略 $\\pi$ 选择动作 $A$。\n6.  执行动作 $A$，观察到奖励 $R$ 和下一个状态 $S'$。\n7.  **计算 TD 误差:** $\\delta = R + \\gamma * V(S') - V(S)$ (如果 $S'$ 是终止状态，则 $V(S')=0$)。\n8.  **更新当前状态的价值:** $V(S) ← V(S) + \\alpha * \\delta$。\n9.  将当前状态更新为下一个状态：$S ← S'$。\n10. 如果 $S'$ 是终止状态，则结束当前回合。\n\n# TD(0) vs. Monte Carlo (MC)\n\nTD(0) 和 MC 都是用于策略评估的无模型方法，但它们有显著的区别：\n\n| 特性             | Monte Carlo (MC)                     | Temporal-Difference (TD(0))             |\n| :--------------- | :----------------------------------- | :-------------------------------------- |\n| **更新时机**     | 回合结束后                           | 每一步之后 (Online)                     |\n| **更新目标**     | 完整回报 $G_t$                         | TD 目标: $R_{t+1} + \\gamma V(S_{t+1})$         |\n| **是否自举**     | 否 (No Bootstrapping)                | 是 (Bootstrapping)                      |\n| **偏差 (Bias)**  | 无偏 (Unbiased)                      | 有偏 (Biased, 依赖于 $V(S_{t+1})$)        |\n| **方差 (Variance)**| 高方差 (High Variance)               | 低方差 (Low Variance)                   |\n| **对初始值敏感度** | 低                                   | 高                                      |\n| **适用任务**     | 回合制 (Episodic)                    | 回合制 & 持续性 (Continuing)            |\n| **计算效率**     | 可能较低 (需存储整个回合)            | 通常较高 (只需存储上一步信息)           |\n| **收敛性 (Markov)**| 收敛到 $V_{\\pi}$                            | 收敛到 $V_{\\pi}$ (通常更快)                    |\n| **收敛性 (Non-Markov)** | 仍可应用，可能收敛到近似解       | 理论保证减弱，可能不收敛或收敛到错误值 |\n\n::: {.callout-note title=\"偏差-方差权衡 (Bias-Variance Tradeoff)\"}\n*   **MC:** 使用的是实际的、完整的样本回报 $G_t$，所以对 $V_{\\pi}(s)$ 的估计是**无偏**的。但 $G_t$ 受到整个回合随机性的影响，**方差很大**。\n*   **TD(0):** 使用的是 TD 目标 $R_{t+1} + \\gamma V(S_{t+1})$，它本身依赖于估计值 $V(S_{t+1})$，因此是有偏的。但 TD 目标只涉及一步随机性 ($R_{t+1}$, $S_{t+1}$)，**方差比 $G_t$ 小得多**。\n:::\n\n在实践中，TD 方法通常因为其较低的方差和在线学习能力而**收敛更快**，尤其是在状态空间较大的情况下。然而，TD 的偏差意味着它对初始值更敏感，并且在非马尔可夫环境中可能表现不佳。\n\n::: {.callout-tip title=\"何时选择？\"}\n*   如果环境模型已知 -> **动态规划 (DP)**\n*   如果环境模型未知，且任务是回合制的，可以轻松模拟大量完整回合 -> **MC** 或 **TD** (TD 通常更快)\n*   如果环境模型未知，且任务是持续性的，或者回合非常长 -> **TD**\n:::\n\n# Lab 3: TD(0) 预测实践\n\n## 目标\n\n1.  在一个简单的环境中（如 Gridworld）实现或运行 TD(0) 预测算法。\n2.  对比 TD(0) 和 MC 方法在相同任务上的收敛速度和最终价值估计结果。\n\n## 环境：Gridworld\n\nGridworld 是一个非常适合演示 TD 和 MC 区别的环境。我们可以定义一个简单的网格，包含起始点、目标点（正奖励）、陷阱点（负奖励）和普通格子（小负奖励，鼓励尽快结束）。\n\n由于 Gymnasium 没有内置的标准 Gridworld，你需要：\n\n*   **选项 A:** 自己实现一个简单的 Gridworld 环境类，遵循 Gymnasium 的 API (类似上周 Lab 的 `SimpleInventoryEnv`)。\n*   **选项 B:** 在网上搜索并使用一个现成的 Python Gridworld 实现 (确保它提供 `step` 和 `reset` 等基本接口)。\n\n**Gridworld 示例定义:**\n\n*   4x4 网格。\n*   状态: 格子坐标 (row, col)。\n*   动作: 上(0), 下(1), 左(2), 右(3)。\n*   起始点: (0, 0)。\n*   目标点: (3, 3)，奖励 +1，回合结束。\n*   陷阱点: (1, 1), (2, 3)，奖励 -1，回合结束。\n*   普通格子: 移动一步奖励 -0.1。\n*   撞墙: 状态不变，奖励 -0.1。\n*   策略 $\\pi$: 随机策略 (等概率选择上/下/左/右)。\n\n## 任务\n\n1.  **实现/获取 Gridworld 环境:** 确保你有一个可以运行的 Gridworld 环境。\n2.  **实现 TD(0) 预测:**\n    *   初始化 $V(s) = 0, \\forall s \\in S$。\n    *   选择一个学习率 $\\alpha$ (e.g., 0.1)。\n    *   选择一个折扣因子 $\\gamma$ (e.g., 0.9 or 1.0)。\n    *   运行 TD(0) 算法（使用随机策略 $\\pi$）足够多的回合 (e.g., 1000, 5000, 10000)。\n    *   记录或可视化 $V(s)$ 随着回合数的变化过程。\n3.  **实现 MC 预测 (首次访问或每次访问):**\n    *   使用相同的 Gridworld 环境和随机策略 $\\pi$。\n    *   运行 MC 预测算法相同的回合数。\n    *   记录或可视化 $V(s)$ 随着回合数的变化过程。\n4.  **对比分析:**\n    *   **收敛速度:** 比较 TD(0) 和 MC 的价值函数 $V(s)$ 达到稳定状态所需的回合数。哪个更快？\n    *   **最终结果:** 比较两种方法最终估计出的 $V(s)$ 是否相似？如果有差异，可能是什么原因？\n    *   **方差:** (可选) 如果记录了每次更新的值，可以观察 TD 误差和 MC 回报 $G_t$ 的波动情况。TD 误差的波动是否通常小于 $G_t$ 的波动？\n    *   **实验参数:** 尝试改变学习率 $\\alpha$ 和回合数，观察对 TD(0) 收敛速度和结果的影响。\n\n## 可视化\n\n对于 Gridworld，价值函数 $V(s)$ 可以方便地用热力图 (Heatmap) 可视化：创建一个与网格大小相同的矩阵，每个单元格的值对应 $V(row, col)$。\n\n```python\n# 假设 V 是一个字典，key 是 (row, col) 元组，value 是价值\n# grid_height, grid_width 是网格尺寸\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 用于更好的热力图\n\n# 假设 grid_height 和 grid_width 已定义\nvalue_grid = np.zeros((grid_height, grid_width))\n# 假设 V 是包含 (row, col): value 的字典\nfor (r, c), value in V.items():\n     if 0 <= r < grid_height and 0 <= c < grid_width: # Add boundary check\n        value_grid[r, c] = value\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(value_grid, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=False)\nplt.title(\"Value Function Heatmap (TD or MC)\")\nplt.show()\n```\n\n## 提交要求\n\n*   提交你的 Gridworld 环境代码（如果是自己实现的）或说明使用的来源。\n*   提交 TD(0) 和 MC 预测算法的实现代码。\n*   提交最终的价值函数可视化结果（热力图）。\n*   提交一份简短的对比分析报告，讨论收敛速度、最终结果和观察到的现象。\n\n---\n\n**下周预告:** 从预测到控制 - 学习如何找到最优策略。介绍第一个控制算法：同策略 TD 控制 - SARSA。","srcMarkdownNoYaml":"\n\n# 回顾：蒙特卡洛 (MC) 方法\n\n上周我们学习了蒙特卡洛 (MC) 方法用于无模型预测：\n\n*   **核心思想:** 通过模拟大量**完整回合**，用样本回报 $G_t$ 的平均值来估计 $V_{\\pi}(s)$ 或 $Q_{\\pi}(s, a)$。\n*   **优点:** 无模型、简单直观、无偏估计。\n*   **缺点:**\n    *   必须等待**回合结束**才能学习和更新价值函数。\n    *   **高方差**，导致收敛可能较慢。\n    *   基本形式只适用于**回合制任务**。\n\n这些缺点，特别是必须等待回合结束，限制了 MC 方法在许多需要快速响应或回合非常长的场景中的应用。本周我们将学习一种更灵活、应用更广泛的无模型预测方法：**时序差分学习 (Temporal-Difference Learning, TD)**。\n\n# 时序差分 (TD) 学习核心思想\n\nTD 学习结合了 MC 方法和动态规划 (Dynamic Programming, DP) 的思想。\n\n*   **像 MC 一样:** TD 直接从与环境交互的**经验**中学习，不需要环境模型 ($P$, $R$)。\n*   **像 DP 一样:** TD 使用**自举 (Bootstrapping)** 的思想，即用**当前估计的价值**来更新之前的估计，而不是等待最终的实际回报。这种思想类似于动态规划中利用已知信息逐步求解未知问题的过程。\n\n**关键区别:**\n\n*   **MC 更新目标:** 使用**完整的回报 $G_t$** 来更新 $V(S_t)$。\n    *   $V(S_t) ← V(S_t) + \\alpha [G_t - V(S_t)]$  ($\\alpha$ 是学习率)\n*   **TD 更新目标:** 使用**一步之后**的奖励 $R_{t+1}$ 和**后继状态的估计价值 $\\gamma V(S_{t+1})$** 来更新 $V(S_t)$。\n    *   $V(S_t) ← V(S_t) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]$\n\n\n::: {.callout-note title=\"TD 方法与 MC 方法的比较\"}\n**TD 方法的优点:**\n\n*   **实时性:** 不需要等待回合结束，可以在每一步之后立即更新，适用于需要快速响应的场景。\n*   **低方差:** 只依赖单步转移，相比 MC 的完整回合回报，方差更小。\n*   **适用性广:** 既适用于回合制任务，也适用于连续任务。\n\n**TD 方法的缺点:**\n\n*   **引入偏差:** 由于使用自举，依赖于当前的价值估计，可能导致偏差。\n*   **收敛性:** 在某些情况下，收敛性可能不如 MC 方法稳定。\n\n**MC 方法的优点:**\n\n*   **无偏性:** 使用实际回报，不依赖估计值，是无偏估计。\n*   **简单直观:** 直接使用完整回合的回报，概念上更容易理解。\n\n**MC 方法的缺点:**\n\n*   **高方差:** 完整回合的回报可能波动较大，导致高方差。\n*   **延迟更新:** 必须等待回合结束才能更新，不适合实时应用。\n*   **仅限回合制:** 基本形式只适用于回合制任务。\n:::\n\n::: {.callout-tip title=\"TD 方法的有效性及理论证明\"}\nTD 方法不仅在实践中表现出色，其有效性也得到了理论证明：\n\n*   **理论保证:** Sutton (1988) 在论文《Learning to Predict by the Methods of Temporal Differences》中首次系统性地提出了 TD 学习算法，并证明了其收敛性。\n*   **收敛性证明:** Tsitsiklis (1994) 在《Asynchronous Stochastic Approximation and Q-Learning》中严格证明了 TD(0) 在马尔可夫决策过程下的收敛性。\n*   **性能优势:** Singh 和 Dayan (1998) 在《Analytical Mean Squared Error Curves for Temporal Difference Learning》中通过理论分析表明，TD 方法在均方误差方面优于 MC 方法。\n*   **实际应用:** Tesauro (1995) 在《Temporal Difference Learning and TD-Gammon》中展示了 TD 方法在西洋双陆棋中的卓越表现，达到了人类专家水平。\n\n这些理论研究和实际应用共同证明了 TD 方法的有效性和实用性，使其成为强化学习领域最重要的算法之一。\n:::\n\n# TD(0) 预测 (最简单的 TD 方法)\n\nTD(0) 是最基础的 TD 预测算法，用于在给定策略 $\\pi$ 的情况下估计 $V_{\\pi}$。\n\n**更新规则:**\n在时间步 $t$，智能体处于状态 $S_t$，执行动作 $A_t$ (根据策略 $\\pi$)，得到奖励 $R_{t+1}$ 和下一个状态 $S_{t+1}$。TD(0) 使用这个**单步转移 (transition)** $(S_t, A_t, R_{t+1}, S_{t+1})$ 来更新状态 $S_t$ 的价值估计 $V(S_t)$：\n\n$$\nV(S_t) ← V(S_t) + \\alpha [ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) ]\n$$\n\n其中:\n\n*   **$\\alpha$ (Alpha):** 学习率 (Learning Rate)，是一个小的正数 (e.g., 0.1, 0.01)。它控制了每次更新的步长。\n*   $R_{t+1} + \\gamma V(S_{t+1})$: 称为 **TD 目标 (TD Target)**。这是对状态 $S_t$ 应该具有的价值的一个**估计**。它由两部分组成：\n    *   $R_{t+1}$: 实际获得的即时奖励。\n    *   $\\gamma V(S_{t+1})$: 对未来所有奖励的折扣总和的**当前估计** (基于后继状态 $S_{t+1}$ 的当前价值估计 $V(S_{t+1})$ 进行自举)。\n*   $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$: 称为 **TD 误差 (TD Error)**。它衡量了当前估计 $V(S_t)$ 与更“准确”的 TD 目标之间的差距。TD 学习的目标就是通过调整 $V(S_t)$ 来减小这个误差。\n\n::: {.callout-note title=\"TD(0) 的含义\"}\n\"0\" 表示 TD 误差 $\\delta_t$ 只依赖于**下一步** ($t+1$) 的信息 ($R_{t+1}$, $S_{t+1}$)。还有更复杂的 TD($\\lambda$) 算法会考虑未来更多步的信息。\n:::\n\n## 自举 (Bootstrapping)\n\nTD 学习的核心特点是**自举 (Bootstrapping)**。它指的是**用当前的估计值来更新估计值**。\n\n在 TD(0) 中，我们使用 $V(S_{t+1})$（一个估计值）来计算 TD 目标，进而更新 $V(S_t)$（另一个估计值）。\n\n*   **优点:** 使得 TD 可以在每一步之后立即学习，不需要等待回合结束。\n*   **缺点:** 引入了**偏差 (Bias)**。因为 TD 目标 $R_{t+1} + \\gamma V(S_{t+1})$ 本身就依赖于可能不准确的 $V(S_{t+1})$。如果 $V(S_{t+1})$ 的估计不准，那么对 $V(S_t)$ 的更新也会受到影响。\n\n## TD(0) 预测算法伪代码\n\n\n    初始化:\n    π ← 要评估的策略\n    V(s) ← 任意状态值函数 (例如，对所有 s ∈ S，V(s)=0)\n    α ← 学习率 (一个小的正数)\n\n    对每个回合循环:\n    初始化 S (回合的第一个状态)\n    对回合的每一步循环:\n        A ← 根据策略 π 为 S 选择的动作\n        执行动作 A，观察 R, S' (下一个状态)\n        # 核心更新步骤\n        V(S) ← V(S) + α * [R + γ * V(S') - V(S)]\n        S ← S' # 转移到下一个状态\n        如果 S 是终止状态，跳出内层循环 (回合结束)\n\n\n**解释:**\n\n1.  初始化 $V(s)$ 和学习率 $\\alpha$。\n2.  对于每个回合：\n3.  获取起始状态 $S$。\n4.  对于回合中的每一步：\n5.  根据策略 $\\pi$ 选择动作 $A$。\n6.  执行动作 $A$，观察到奖励 $R$ 和下一个状态 $S'$。\n7.  **计算 TD 误差:** $\\delta = R + \\gamma * V(S') - V(S)$ (如果 $S'$ 是终止状态，则 $V(S')=0$)。\n8.  **更新当前状态的价值:** $V(S) ← V(S) + \\alpha * \\delta$。\n9.  将当前状态更新为下一个状态：$S ← S'$。\n10. 如果 $S'$ 是终止状态，则结束当前回合。\n\n# TD(0) vs. Monte Carlo (MC)\n\nTD(0) 和 MC 都是用于策略评估的无模型方法，但它们有显著的区别：\n\n| 特性             | Monte Carlo (MC)                     | Temporal-Difference (TD(0))             |\n| :--------------- | :----------------------------------- | :-------------------------------------- |\n| **更新时机**     | 回合结束后                           | 每一步之后 (Online)                     |\n| **更新目标**     | 完整回报 $G_t$                         | TD 目标: $R_{t+1} + \\gamma V(S_{t+1})$         |\n| **是否自举**     | 否 (No Bootstrapping)                | 是 (Bootstrapping)                      |\n| **偏差 (Bias)**  | 无偏 (Unbiased)                      | 有偏 (Biased, 依赖于 $V(S_{t+1})$)        |\n| **方差 (Variance)**| 高方差 (High Variance)               | 低方差 (Low Variance)                   |\n| **对初始值敏感度** | 低                                   | 高                                      |\n| **适用任务**     | 回合制 (Episodic)                    | 回合制 & 持续性 (Continuing)            |\n| **计算效率**     | 可能较低 (需存储整个回合)            | 通常较高 (只需存储上一步信息)           |\n| **收敛性 (Markov)**| 收敛到 $V_{\\pi}$                            | 收敛到 $V_{\\pi}$ (通常更快)                    |\n| **收敛性 (Non-Markov)** | 仍可应用，可能收敛到近似解       | 理论保证减弱，可能不收敛或收敛到错误值 |\n\n::: {.callout-note title=\"偏差-方差权衡 (Bias-Variance Tradeoff)\"}\n*   **MC:** 使用的是实际的、完整的样本回报 $G_t$，所以对 $V_{\\pi}(s)$ 的估计是**无偏**的。但 $G_t$ 受到整个回合随机性的影响，**方差很大**。\n*   **TD(0):** 使用的是 TD 目标 $R_{t+1} + \\gamma V(S_{t+1})$，它本身依赖于估计值 $V(S_{t+1})$，因此是有偏的。但 TD 目标只涉及一步随机性 ($R_{t+1}$, $S_{t+1}$)，**方差比 $G_t$ 小得多**。\n:::\n\n在实践中，TD 方法通常因为其较低的方差和在线学习能力而**收敛更快**，尤其是在状态空间较大的情况下。然而，TD 的偏差意味着它对初始值更敏感，并且在非马尔可夫环境中可能表现不佳。\n\n::: {.callout-tip title=\"何时选择？\"}\n*   如果环境模型已知 -> **动态规划 (DP)**\n*   如果环境模型未知，且任务是回合制的，可以轻松模拟大量完整回合 -> **MC** 或 **TD** (TD 通常更快)\n*   如果环境模型未知，且任务是持续性的，或者回合非常长 -> **TD**\n:::\n\n# Lab 3: TD(0) 预测实践\n\n## 目标\n\n1.  在一个简单的环境中（如 Gridworld）实现或运行 TD(0) 预测算法。\n2.  对比 TD(0) 和 MC 方法在相同任务上的收敛速度和最终价值估计结果。\n\n## 环境：Gridworld\n\nGridworld 是一个非常适合演示 TD 和 MC 区别的环境。我们可以定义一个简单的网格，包含起始点、目标点（正奖励）、陷阱点（负奖励）和普通格子（小负奖励，鼓励尽快结束）。\n\n由于 Gymnasium 没有内置的标准 Gridworld，你需要：\n\n*   **选项 A:** 自己实现一个简单的 Gridworld 环境类，遵循 Gymnasium 的 API (类似上周 Lab 的 `SimpleInventoryEnv`)。\n*   **选项 B:** 在网上搜索并使用一个现成的 Python Gridworld 实现 (确保它提供 `step` 和 `reset` 等基本接口)。\n\n**Gridworld 示例定义:**\n\n*   4x4 网格。\n*   状态: 格子坐标 (row, col)。\n*   动作: 上(0), 下(1), 左(2), 右(3)。\n*   起始点: (0, 0)。\n*   目标点: (3, 3)，奖励 +1，回合结束。\n*   陷阱点: (1, 1), (2, 3)，奖励 -1，回合结束。\n*   普通格子: 移动一步奖励 -0.1。\n*   撞墙: 状态不变，奖励 -0.1。\n*   策略 $\\pi$: 随机策略 (等概率选择上/下/左/右)。\n\n## 任务\n\n1.  **实现/获取 Gridworld 环境:** 确保你有一个可以运行的 Gridworld 环境。\n2.  **实现 TD(0) 预测:**\n    *   初始化 $V(s) = 0, \\forall s \\in S$。\n    *   选择一个学习率 $\\alpha$ (e.g., 0.1)。\n    *   选择一个折扣因子 $\\gamma$ (e.g., 0.9 or 1.0)。\n    *   运行 TD(0) 算法（使用随机策略 $\\pi$）足够多的回合 (e.g., 1000, 5000, 10000)。\n    *   记录或可视化 $V(s)$ 随着回合数的变化过程。\n3.  **实现 MC 预测 (首次访问或每次访问):**\n    *   使用相同的 Gridworld 环境和随机策略 $\\pi$。\n    *   运行 MC 预测算法相同的回合数。\n    *   记录或可视化 $V(s)$ 随着回合数的变化过程。\n4.  **对比分析:**\n    *   **收敛速度:** 比较 TD(0) 和 MC 的价值函数 $V(s)$ 达到稳定状态所需的回合数。哪个更快？\n    *   **最终结果:** 比较两种方法最终估计出的 $V(s)$ 是否相似？如果有差异，可能是什么原因？\n    *   **方差:** (可选) 如果记录了每次更新的值，可以观察 TD 误差和 MC 回报 $G_t$ 的波动情况。TD 误差的波动是否通常小于 $G_t$ 的波动？\n    *   **实验参数:** 尝试改变学习率 $\\alpha$ 和回合数，观察对 TD(0) 收敛速度和结果的影响。\n\n## 可视化\n\n对于 Gridworld，价值函数 $V(s)$ 可以方便地用热力图 (Heatmap) 可视化：创建一个与网格大小相同的矩阵，每个单元格的值对应 $V(row, col)$。\n\n```python\n# 假设 V 是一个字典，key 是 (row, col) 元组，value 是价值\n# grid_height, grid_width 是网格尺寸\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 用于更好的热力图\n\n# 假设 grid_height 和 grid_width 已定义\nvalue_grid = np.zeros((grid_height, grid_width))\n# 假设 V 是包含 (row, col): value 的字典\nfor (r, c), value in V.items():\n     if 0 <= r < grid_height and 0 <= c < grid_width: # Add boundary check\n        value_grid[r, c] = value\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(value_grid, annot=True, fmt=\".2f\", cmap=\"viridis\", cbar=False)\nplt.title(\"Value Function Heatmap (TD or MC)\")\nplt.show()\n```\n\n## 提交要求\n\n*   提交你的 Gridworld 环境代码（如果是自己实现的）或说明使用的来源。\n*   提交 TD(0) 和 MC 预测算法的实现代码。\n*   提交最终的价值函数可视化结果（热力图）。\n*   提交一份简短的对比分析报告，讨论收敛速度、最终结果和观察到的现象。\n\n---\n\n**下周预告:** 从预测到控制 - 学习如何找到最优策略。介绍第一个控制算法：同策略 TD 控制 - SARSA。"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week5_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 5: 时序差分学习 - 从不完整经验中学习"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}