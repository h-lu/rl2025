{"title":"Week 15: 实践挑战、伦理规范与项目指导","markdown":{"yaml":{"title":"Week 15: 实践挑战、伦理规范与项目指导"},"headingText":"回顾：RL 商业案例分析","containsRefs":false,"markdown":"\n\n\n前两周我们探讨了 RL 在两个重要商业领域的应用：\n\n*   **动态定价与资源优化:** 如何平衡供需、最大化收益，以及面临的挑战（状态表示、奖励设计、Sim-to-Real 等）。\n*   **个性化推荐与营销:** 如何优化用户长期价值，以及需要关注的伦理问题（过滤气泡、公平性、隐私等）。\n\n这些案例分析突显了将 RL 从理论和模拟环境**成功落地**到实际商业场景中所面临的**共性挑战**。本周我们将系统性地总结这些挑战，并讨论负责任 AI 的原则和伦理规范，最后为期末项目提供指导。\n\n# RL 实践挑战总结 (落地挑战)\n\n将 RL 应用于真实商业决策，远不止选择一个算法并运行那么简单。以下是一些关键的实践挑战：\n\n## 1. 数据获取与质量 (Data Acquisition & Quality)\n\n*   **数据需求量大:** RL（尤其是 DRL）通常需要大量的交互数据来学习有效的策略。对于需要与真实环境交互的应用（如推荐、定价），收集足够数据的成本可能很高。\n*   **数据质量:**\n    *   **噪声与缺失:** 真实世界的数据往往包含噪声、错误和缺失值，需要仔细清洗和预处理。\n    *   **偏差 (Bias):** 历史数据可能反映了过去的次优策略或市场偏差，直接用于训练可能导致 RL 模型学到这些偏差。例如，历史推荐数据可能偏向热门商品，导致 RL 难以发现长尾商品的价值。\n    *   **日志策略 (Logging Policy):** 记录的数据是由哪个策略生成的？这对于 Off-Policy 学习和评估至关重要。\n*   **探索成本与风险:** 在线收集数据需要进行探索，但这可能导致短期性能下降或用户体验变差（例如，推荐不相关的商品、设定不合理的价格）。如何在探索和实际业务指标之间取得平衡是一个难题。\n\n::: {.callout-tip title=\"应对思路\"}\n*   利用 Offline RL 技术充分挖掘历史数据价值。\n*   设计更有效的探索策略（如结合领域知识）。\n*   构建高质量的模拟环境以减少对真实交互的依赖。\n*   进行 A/B 测试以小范围验证策略效果。\n:::\n\n## 2. 模拟环境构建与 Sim-to-Real Gap\n\n*   **模拟器是关键:** 由于在线实验成本高、风险大，构建一个能够**准确反映**真实世界动态的模拟环境对于 RL 模型的开发、训练和测试至关重要。\n*   **构建挑战:**\n    *   **复杂性:** 真实商业环境（如市场、用户行为）非常复杂，包含许多未知或难以建模的因素。\n    *   **保真度:** 模拟器需要足够逼真，才能保证在模拟器中训练好的策略在真实环境中也能有效（减小 Sim-to-Real Gap）。\n    *   **校准与验证:** 需要用真实数据不断校准和验证模拟器的准确性。\n*   **Sim-to-Real Gap:** 模拟环境与真实环境之间的差异。即使模拟器做得很好，也可能存在差距，导致在模拟中表现优异的策略在现实中效果不佳。\n    *   **原因:** 未建模的动态、噪声、延迟、用户行为的不可预测性等。\n\n::: {.callout-tip title=\"应对思路\"}\n*   迭代式开发：从简单模型开始，逐步增加模拟器的复杂度。\n*   数据驱动建模：利用历史数据构建用户行为模型、市场响应模型等。\n*   领域随机化 (Domain Randomization)：在模拟器中引入各种随机性（参数、噪声），使 RL 策略对环境变化更鲁棒。\n*   部署时进行微调 (Fine-tuning) 或在线学习。\n:::\n\n## 3. 奖励函数设计的艺术与陷阱 (Reward Engineering)\n\n*   **奖励函数定义目标:** RL 智能体只会优化你明确定义的奖励函数。奖励函数的设计直接决定了智能体的最终行为。\n*   **挑战:**\n    *   **对齐商业目标:** 设计的奖励函数是否真正反映了长期的商业目标？（例如，优化点击率 vs. 优化用户 LTV）。\n    *   **奖励稀疏性 (Sparse Rewards):** 很多商业目标（如最终购买、用户流失）是稀疏且延迟的，智能体很难从中学习。\n    *   **奖励塑形 (Reward Shaping):** 设计一些中间奖励来引导学习是常见的做法，但如果设计不当，可能导致智能体“钻空子”，学会利用中间奖励而忽略最终目标（例如，推荐系统只优化点击而不关心转化）。\n    *   **多目标冲突:** 商业目标通常是多个且可能冲突的（收入 vs. 用户满意度，效率 vs. 公平性）。如何平衡这些目标？\n*   **“奖励函数就是规约” (Reward is the Specification):** 你得到的（智能体行为）就是你指定的（奖励函数），即使它不是你真正想要的。\n\n::: {.callout-warning title=\"奖励设计的陷阱\"}\n*   **指标博弈 (Goodhart's Law):** 当一个指标成为目标时，它就不再是一个好的指标。过度优化某个代理指标（如点击率）可能损害真正的目标（如用户满意度）。\n*   **负面副作用 (Negative Side Effects):** 智能体为了最大化奖励可能采取意想不到的、有害的方式。\n:::\n\n::: {.callout-tip title=\"应对思路\"}\n*   仔细思考并明确长期的商业目标。\n*   尽可能使用与最终目标更相关的奖励信号。\n*   谨慎使用奖励塑形，并进行充分测试。\n*   考虑多目标优化方法或基于约束的 RL。\n*   迭代式设计和测试奖励函数。\n:::\n\n## 4. 安全性与鲁棒性测试 (Safety & Robustness)\n\n*   **高风险应用:** 在金融、自动驾驶、医疗等高风险领域，RL 策略的错误可能导致严重后果。即使在商业应用中（如定价、库存），错误的决策也可能导致巨大损失。\n*   **安全性:** 如何确保 RL 策略不会采取危险或破坏性的行为？\n*   **鲁棒性:** RL 策略在面对未曾见过的状态、噪声干扰或环境变化时，表现是否稳定？深度学习模型可能对输入的微小扰动非常敏感（对抗性攻击）。\n*   **探索的风险:** 探索过程本身可能导致不安全的行为。\n\n::: {.callout-tip title=\"应对思路\"}\n*   在模拟环境中进行广泛的压力测试和边缘案例测试。\n*   设置安全约束：限制动作空间、设定保护性规则。\n*   使用鲁棒性优化技术训练模型。\n*   部署时进行 A/B 测试和灰度发布。\n*   建立实时的监控和报警系统。\n*   必要时加入人工监督或干预机制。\n:::\n\n## 5. 部署与维护 (Deployment & Maintenance)\n\n*   **技术栈整合:** 如何将训练好的 RL 模型集成到现有的业务系统和技术架构中？\n*   **实时决策需求:** 许多应用（如 RTB 广告竞价）需要在毫秒级内做出决策，对模型的推理速度有很高要求。\n*   **模型更新:** 市场环境是变化的，需要定期重新训练或在线更新 RL 模型。如何管理模型的版本和更新过程？\n*   **监控与调试:** 如何监控线上 RL 策略的表现？出现问题时如何快速定位和调试？（RL 模型的调试通常比监督学习更困难）。\n*   **计算资源:** 训练复杂的 DRL 模型需要大量的计算资源 (GPU/TPU)。\n\n::: {.callout-tip title=\"应对思路\"}\n*   设计清晰的部署架构和 MLOps 流程。\n*   模型压缩和优化以满足实时推理需求。\n*   建立完善的监控指标体系（业务指标 + 模型内部指标）。\n*   制定模型更新策略和回滚计划。\n*   投入足够的计算资源和工程支持。\n:::\n\n# 负责任的 AI 与 RL 伦理框架\n\n正如我们在上周讨论推荐系统时看到的，RL 的应用（尤其是 DRL）可能带来显著的伦理风险。开发和部署 RL 系统时，必须遵循**负责任的人工智能 (Responsible AI)** 原则。\n\n关键伦理考量：\n\n1.  **公平性 (Fairness):**\n    *   避免算法对特定人群产生歧视或不公平的对待。\n    *   需要考虑数据偏差、算法偏差以及对不同群体的影响差异。\n    *   进行公平性审计，使用缓解偏差的技术。\n2.  **透明度 (Transparency):**\n    *   让用户和利益相关者了解系统是如何工作的（在可能的范围内）。\n    *   提供关于数据使用、模型目标和决策逻辑的信息。\n3.  **可解释性 (Explainability / Interpretability):**\n    *   能够解释模型为什么做出某个特定的决策。\n    *   对于复杂的 DRL 模型（黑箱），可解释性是一个重大挑战。\n    *   使用可解释性 AI (XAI) 技术，或者在某些场景下选择更简单的、可解释性更好的模型。\n4.  **问责制 (Accountability):**\n    *   明确谁对 AI 系统的行为及其后果负责。\n    *   建立清晰的治理结构和责任分配机制。\n    *   确保有途径进行申诉和补救。\n5.  **隐私 (Privacy):**\n    *   保护用户数据隐私，遵守相关法规（如 GDPR, CCPA）。\n    *   采用隐私保护技术（匿名化、差分隐私、联邦学习）。\n6.  **安全与可靠性 (Safety & Reliability):**\n    *   确保系统在各种情况下都能安全、可靠地运行，避免造成伤害。\n    *   进行充分的测试和验证。\n\n::: {.callout-important title=\"伦理先行\"}\n在 RL 项目的整个生命周期中（从问题定义、数据收集、模型设计到部署和监控），都应将伦理考量放在重要位置。这不仅是社会责任，也是建立用户信任、实现长期商业成功的关键。\n:::\n\n# 期末项目选题指导与 Q&A\n\n现在是时候开始思考期末项目了。根据课程大纲，主要有三个方向：\n\n1.  **模拟实验与分析:**\n    *   选择一个（简化的）商业问题（如动态定价、库存管理、简单推荐、资源分配等）。\n    *   将其形式化为 MDP。\n    *   选择合适的 RL 算法（可以使用表格型方法如 Q-Learning，或使用 Stable Baselines3 运行 DQN/A2C 等）。\n    *   在模拟环境中进行实验，调整参数，分析结果（学习曲线、最终策略、价值函数）。\n    *   **重点:** 清晰的问题定义、合理的实验设计、深入的结果分析和商业见解。**代码实现不是唯一重点，允许使用现成库。**\n\n2.  **应用方案设计:**\n    *   选择一个具体的商业场景（可以更复杂一些）。\n    *   **不要求实现代码或运行实验。**\n    *   **重点:** 设计一套**完整**的 RL 解决方案，包括：\n        *   清晰的商业目标。\n        *   详细的 MDP 定义（状态、动作、奖励 - 深入思考其合理性和挑战）。\n        *   合适的算法选择及理由。\n        *   所需的数据。\n        *   预期的实践挑战（数据、模拟、部署、伦理等）以及应对思路。\n        *   评估方案。\n\n3.  **文献综述与批判性分析:**\n    *   选择 RL 在**某一特定商业领域**（如金融科技、市场营销、运营管理、人力资源等）的应用。\n    *   调研相关文献（学术论文、技术报告、行业文章）。\n    *   **重点:** 综述该领域 RL 的应用现状、关键技术、成功案例、失败教训、面临的挑战以及未来发展趋势。需要进行**批判性分析**，而不仅仅是罗列文献。\n\n**选题建议:**\n\n*   选择你**感兴趣**且**有一定了解**的商业领域或问题。\n*   **范围要聚焦:** 不要试图解决过于庞大或复杂的问题。对于方向 1 和 2，简化是必要的。\n*   **可行性:** 考虑你的时间和技术能力。方向 1 需要编程和实验，方向 2 和 3 更侧重于研究、分析和写作。\n*   **利用课程所学:** 将课程中学习到的概念（MDP, Bellman, MC, TD, Q-Learning, DQN, A2C, 实践挑战, 伦理考量）应用到你的项目中。\n\n**本周和下周的答疑时间将重点用于项目选题和思路讨论，请大家积极准备和提问！**\n\n---\n\n**下周预告:** 课程总结与未来展望 / 项目展示。我们将回顾整个课程的核心内容，展望 RL 的前沿方向，并进行期末项目展示（或期末考试）。","srcMarkdownNoYaml":"\n\n# 回顾：RL 商业案例分析\n\n前两周我们探讨了 RL 在两个重要商业领域的应用：\n\n*   **动态定价与资源优化:** 如何平衡供需、最大化收益，以及面临的挑战（状态表示、奖励设计、Sim-to-Real 等）。\n*   **个性化推荐与营销:** 如何优化用户长期价值，以及需要关注的伦理问题（过滤气泡、公平性、隐私等）。\n\n这些案例分析突显了将 RL 从理论和模拟环境**成功落地**到实际商业场景中所面临的**共性挑战**。本周我们将系统性地总结这些挑战，并讨论负责任 AI 的原则和伦理规范，最后为期末项目提供指导。\n\n# RL 实践挑战总结 (落地挑战)\n\n将 RL 应用于真实商业决策，远不止选择一个算法并运行那么简单。以下是一些关键的实践挑战：\n\n## 1. 数据获取与质量 (Data Acquisition & Quality)\n\n*   **数据需求量大:** RL（尤其是 DRL）通常需要大量的交互数据来学习有效的策略。对于需要与真实环境交互的应用（如推荐、定价），收集足够数据的成本可能很高。\n*   **数据质量:**\n    *   **噪声与缺失:** 真实世界的数据往往包含噪声、错误和缺失值，需要仔细清洗和预处理。\n    *   **偏差 (Bias):** 历史数据可能反映了过去的次优策略或市场偏差，直接用于训练可能导致 RL 模型学到这些偏差。例如，历史推荐数据可能偏向热门商品，导致 RL 难以发现长尾商品的价值。\n    *   **日志策略 (Logging Policy):** 记录的数据是由哪个策略生成的？这对于 Off-Policy 学习和评估至关重要。\n*   **探索成本与风险:** 在线收集数据需要进行探索，但这可能导致短期性能下降或用户体验变差（例如，推荐不相关的商品、设定不合理的价格）。如何在探索和实际业务指标之间取得平衡是一个难题。\n\n::: {.callout-tip title=\"应对思路\"}\n*   利用 Offline RL 技术充分挖掘历史数据价值。\n*   设计更有效的探索策略（如结合领域知识）。\n*   构建高质量的模拟环境以减少对真实交互的依赖。\n*   进行 A/B 测试以小范围验证策略效果。\n:::\n\n## 2. 模拟环境构建与 Sim-to-Real Gap\n\n*   **模拟器是关键:** 由于在线实验成本高、风险大，构建一个能够**准确反映**真实世界动态的模拟环境对于 RL 模型的开发、训练和测试至关重要。\n*   **构建挑战:**\n    *   **复杂性:** 真实商业环境（如市场、用户行为）非常复杂，包含许多未知或难以建模的因素。\n    *   **保真度:** 模拟器需要足够逼真，才能保证在模拟器中训练好的策略在真实环境中也能有效（减小 Sim-to-Real Gap）。\n    *   **校准与验证:** 需要用真实数据不断校准和验证模拟器的准确性。\n*   **Sim-to-Real Gap:** 模拟环境与真实环境之间的差异。即使模拟器做得很好，也可能存在差距，导致在模拟中表现优异的策略在现实中效果不佳。\n    *   **原因:** 未建模的动态、噪声、延迟、用户行为的不可预测性等。\n\n::: {.callout-tip title=\"应对思路\"}\n*   迭代式开发：从简单模型开始，逐步增加模拟器的复杂度。\n*   数据驱动建模：利用历史数据构建用户行为模型、市场响应模型等。\n*   领域随机化 (Domain Randomization)：在模拟器中引入各种随机性（参数、噪声），使 RL 策略对环境变化更鲁棒。\n*   部署时进行微调 (Fine-tuning) 或在线学习。\n:::\n\n## 3. 奖励函数设计的艺术与陷阱 (Reward Engineering)\n\n*   **奖励函数定义目标:** RL 智能体只会优化你明确定义的奖励函数。奖励函数的设计直接决定了智能体的最终行为。\n*   **挑战:**\n    *   **对齐商业目标:** 设计的奖励函数是否真正反映了长期的商业目标？（例如，优化点击率 vs. 优化用户 LTV）。\n    *   **奖励稀疏性 (Sparse Rewards):** 很多商业目标（如最终购买、用户流失）是稀疏且延迟的，智能体很难从中学习。\n    *   **奖励塑形 (Reward Shaping):** 设计一些中间奖励来引导学习是常见的做法，但如果设计不当，可能导致智能体“钻空子”，学会利用中间奖励而忽略最终目标（例如，推荐系统只优化点击而不关心转化）。\n    *   **多目标冲突:** 商业目标通常是多个且可能冲突的（收入 vs. 用户满意度，效率 vs. 公平性）。如何平衡这些目标？\n*   **“奖励函数就是规约” (Reward is the Specification):** 你得到的（智能体行为）就是你指定的（奖励函数），即使它不是你真正想要的。\n\n::: {.callout-warning title=\"奖励设计的陷阱\"}\n*   **指标博弈 (Goodhart's Law):** 当一个指标成为目标时，它就不再是一个好的指标。过度优化某个代理指标（如点击率）可能损害真正的目标（如用户满意度）。\n*   **负面副作用 (Negative Side Effects):** 智能体为了最大化奖励可能采取意想不到的、有害的方式。\n:::\n\n::: {.callout-tip title=\"应对思路\"}\n*   仔细思考并明确长期的商业目标。\n*   尽可能使用与最终目标更相关的奖励信号。\n*   谨慎使用奖励塑形，并进行充分测试。\n*   考虑多目标优化方法或基于约束的 RL。\n*   迭代式设计和测试奖励函数。\n:::\n\n## 4. 安全性与鲁棒性测试 (Safety & Robustness)\n\n*   **高风险应用:** 在金融、自动驾驶、医疗等高风险领域，RL 策略的错误可能导致严重后果。即使在商业应用中（如定价、库存），错误的决策也可能导致巨大损失。\n*   **安全性:** 如何确保 RL 策略不会采取危险或破坏性的行为？\n*   **鲁棒性:** RL 策略在面对未曾见过的状态、噪声干扰或环境变化时，表现是否稳定？深度学习模型可能对输入的微小扰动非常敏感（对抗性攻击）。\n*   **探索的风险:** 探索过程本身可能导致不安全的行为。\n\n::: {.callout-tip title=\"应对思路\"}\n*   在模拟环境中进行广泛的压力测试和边缘案例测试。\n*   设置安全约束：限制动作空间、设定保护性规则。\n*   使用鲁棒性优化技术训练模型。\n*   部署时进行 A/B 测试和灰度发布。\n*   建立实时的监控和报警系统。\n*   必要时加入人工监督或干预机制。\n:::\n\n## 5. 部署与维护 (Deployment & Maintenance)\n\n*   **技术栈整合:** 如何将训练好的 RL 模型集成到现有的业务系统和技术架构中？\n*   **实时决策需求:** 许多应用（如 RTB 广告竞价）需要在毫秒级内做出决策，对模型的推理速度有很高要求。\n*   **模型更新:** 市场环境是变化的，需要定期重新训练或在线更新 RL 模型。如何管理模型的版本和更新过程？\n*   **监控与调试:** 如何监控线上 RL 策略的表现？出现问题时如何快速定位和调试？（RL 模型的调试通常比监督学习更困难）。\n*   **计算资源:** 训练复杂的 DRL 模型需要大量的计算资源 (GPU/TPU)。\n\n::: {.callout-tip title=\"应对思路\"}\n*   设计清晰的部署架构和 MLOps 流程。\n*   模型压缩和优化以满足实时推理需求。\n*   建立完善的监控指标体系（业务指标 + 模型内部指标）。\n*   制定模型更新策略和回滚计划。\n*   投入足够的计算资源和工程支持。\n:::\n\n# 负责任的 AI 与 RL 伦理框架\n\n正如我们在上周讨论推荐系统时看到的，RL 的应用（尤其是 DRL）可能带来显著的伦理风险。开发和部署 RL 系统时，必须遵循**负责任的人工智能 (Responsible AI)** 原则。\n\n关键伦理考量：\n\n1.  **公平性 (Fairness):**\n    *   避免算法对特定人群产生歧视或不公平的对待。\n    *   需要考虑数据偏差、算法偏差以及对不同群体的影响差异。\n    *   进行公平性审计，使用缓解偏差的技术。\n2.  **透明度 (Transparency):**\n    *   让用户和利益相关者了解系统是如何工作的（在可能的范围内）。\n    *   提供关于数据使用、模型目标和决策逻辑的信息。\n3.  **可解释性 (Explainability / Interpretability):**\n    *   能够解释模型为什么做出某个特定的决策。\n    *   对于复杂的 DRL 模型（黑箱），可解释性是一个重大挑战。\n    *   使用可解释性 AI (XAI) 技术，或者在某些场景下选择更简单的、可解释性更好的模型。\n4.  **问责制 (Accountability):**\n    *   明确谁对 AI 系统的行为及其后果负责。\n    *   建立清晰的治理结构和责任分配机制。\n    *   确保有途径进行申诉和补救。\n5.  **隐私 (Privacy):**\n    *   保护用户数据隐私，遵守相关法规（如 GDPR, CCPA）。\n    *   采用隐私保护技术（匿名化、差分隐私、联邦学习）。\n6.  **安全与可靠性 (Safety & Reliability):**\n    *   确保系统在各种情况下都能安全、可靠地运行，避免造成伤害。\n    *   进行充分的测试和验证。\n\n::: {.callout-important title=\"伦理先行\"}\n在 RL 项目的整个生命周期中（从问题定义、数据收集、模型设计到部署和监控），都应将伦理考量放在重要位置。这不仅是社会责任，也是建立用户信任、实现长期商业成功的关键。\n:::\n\n# 期末项目选题指导与 Q&A\n\n现在是时候开始思考期末项目了。根据课程大纲，主要有三个方向：\n\n1.  **模拟实验与分析:**\n    *   选择一个（简化的）商业问题（如动态定价、库存管理、简单推荐、资源分配等）。\n    *   将其形式化为 MDP。\n    *   选择合适的 RL 算法（可以使用表格型方法如 Q-Learning，或使用 Stable Baselines3 运行 DQN/A2C 等）。\n    *   在模拟环境中进行实验，调整参数，分析结果（学习曲线、最终策略、价值函数）。\n    *   **重点:** 清晰的问题定义、合理的实验设计、深入的结果分析和商业见解。**代码实现不是唯一重点，允许使用现成库。**\n\n2.  **应用方案设计:**\n    *   选择一个具体的商业场景（可以更复杂一些）。\n    *   **不要求实现代码或运行实验。**\n    *   **重点:** 设计一套**完整**的 RL 解决方案，包括：\n        *   清晰的商业目标。\n        *   详细的 MDP 定义（状态、动作、奖励 - 深入思考其合理性和挑战）。\n        *   合适的算法选择及理由。\n        *   所需的数据。\n        *   预期的实践挑战（数据、模拟、部署、伦理等）以及应对思路。\n        *   评估方案。\n\n3.  **文献综述与批判性分析:**\n    *   选择 RL 在**某一特定商业领域**（如金融科技、市场营销、运营管理、人力资源等）的应用。\n    *   调研相关文献（学术论文、技术报告、行业文章）。\n    *   **重点:** 综述该领域 RL 的应用现状、关键技术、成功案例、失败教训、面临的挑战以及未来发展趋势。需要进行**批判性分析**，而不仅仅是罗列文献。\n\n**选题建议:**\n\n*   选择你**感兴趣**且**有一定了解**的商业领域或问题。\n*   **范围要聚焦:** 不要试图解决过于庞大或复杂的问题。对于方向 1 和 2，简化是必要的。\n*   **可行性:** 考虑你的时间和技术能力。方向 1 需要编程和实验，方向 2 和 3 更侧重于研究、分析和写作。\n*   **利用课程所学:** 将课程中学习到的概念（MDP, Bellman, MC, TD, Q-Learning, DQN, A2C, 实践挑战, 伦理考量）应用到你的项目中。\n\n**本周和下周的答疑时间将重点用于项目选题和思路讨论，请大家积极准备和提问！**\n\n---\n\n**下周预告:** 课程总结与未来展望 / 项目展示。我们将回顾整个课程的核心内容，展望 RL 的前沿方向，并进行期末项目展示（或期末考试）。"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week15_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 15: 实践挑战、伦理规范与项目指导"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}