{"title":"Week 10: 深度 Q 网络 (DQN)","markdown":{"yaml":{"title":"Week 10: 深度 Q 网络 (DQN)"},"headingText":"回顾：函数逼近的必要性","containsRefs":false,"markdown":"\n\n\n上周我们讨论了表格型 RL 方法的局限性：\n\n*   无法处理**巨大或连续的状态空间**（维度灾难）。\n*   无法处理**连续动作空间**（对于 Q-Learning 等）。\n*   缺乏**泛化能力**，需要访问每个状态（或状态-动作对）多次。\n\n解决方案是使用**函数逼近 (Function Approximation)**，用带参数的函数 V̂(s, **w**) 或 Q̂(s, a, **w**) 来近似价值函数。\n\n*   **线性函数逼近:** 简单，但表达能力有限，且依赖特征工程。\n*   **非线性函数逼近 (如神经网络):** 表达能力强，可以自动学习特征表示，是现代强化学习（深度强化学习）的核心。\n\n今天，我们将学习第一个重要的深度强化学习算法：**深度 Q 网络 (Deep Q-Network, DQN)**。\n\n# DQN 核心思想：用神经网络逼近 Q 函数\n\nQ-Learning 的目标是学习最优动作值函数 Q\\*(s, a)。DQN 的核心思想就是使用一个**深度神经网络 (Deep Neural Network, DNN)** 作为函数逼近器来近似 Q\\*(s, a)。\n\nQ̂(s, a; **w**) ≈ Q\\*(s, a)\n\n其中 **w** 代表神经网络的权重和偏置参数。\n\n**网络结构通常是:**\n\n*   **输入:** 状态 s (通常表示为一个向量或张量，例如 CartPole 的 4 维向量，或 Atari 游戏的屏幕像素)。\n*   **输出:** 对于**每个离散动作 a**，输出一个对应的 Q 值估计 Q̂(s, a; **w**)。\n    *   例如，对于 CartPole (动作 0: 左, 动作 1: 右)，网络输出一个包含两个值的向量：[Q̂(s, 0; w), Q̂(s, 1; w)]。\n\n![DQN Network Architecture Example](https://pytorch.org/tutorials/_images/dqn.png)\n*(图片来源: PyTorch Tutorials - DQN)*\n\n**学习过程 (基于 Q-Learning):**\n\n我们仍然使用 Q-Learning 的更新思想，但现在是更新神经网络的参数 **w**，而不是更新表格条目。目标是最小化 **TD 误差**。\n\n回顾 Q-Learning 的 TD 目标：\nTarget = R + γ max_{a'} Q(S', a')\n\n在 DQN 中，我们用神经网络来计算这个目标：\nTarget = R + γ max_{a'} Q̂(S', a'; **w**)\n\n损失函数 (Loss Function) 通常使用**均方误差 (Mean Squared Error, MSE)** 或 **Huber Loss**:\nLoss(**w**) = E [ ( Target - Q̂(S, A; **w**) )² ]\n\n然后使用**梯度下降** (或其变种，如 Adam) 来更新参数 **w**，以减小这个损失：\n**w** ← **w** - α * ∇ Loss(**w**)\n\n# 挑战：Q-Learning + 神经网络 = 不稳定？\n\n将标准的 Q-Learning 直接与非线性函数逼近器（如神经网络）结合，在实践中发现**非常不稳定**，甚至可能**发散 (diverge)**。主要原因有两个：\n\n1.  **样本之间的相关性 (Correlations between samples):**\n    *   RL 智能体收集到的经验数据 (S, A, R, S') 是按时间顺序产生的，相邻的样本之间通常高度相关。\n    *   如果直接按顺序用这些相关的样本来训练神经网络，会违反许多优化算法（如 SGD）关于样本独立同分布 (i.i.d.) 的假设，导致训练效率低下，模型可能在局部数据上过拟合，忘记过去的经验。\n\n2.  **目标值与估计值的耦合 (Non-stationary targets):**\n    *   Q-Learning 的 TD 目标 `Target = R + γ max_{a'} Q̂(S', a'; **w**)` 依赖于当前的 Q 网络参数 **w**。\n    *   这意味着，在训练过程中，我们每更新一次参数 **w**，用于计算损失的**目标值本身也在变化**。\n    *   这就像在追逐一个移动的目标，使得训练过程非常不稳定，Q 值可能会剧烈震荡甚至发散。\n\n# DQN 的关键技巧\n\n为了解决上述稳定性问题，DQN 引入了两个关键技巧：\n\n## 1. 经验回放 (Experience Replay)\n\n**思想:** 不再按顺序使用实时产生的经验来训练网络，而是将经验存储起来，然后随机采样进行训练。\n\n**机制:**\n\n*   维护一个**回放缓冲区 (Replay Buffer / Memory)** D，用于存储大量的历史转移 (transitions): (S_t, A_t, R_{t+1}, S_{t+1}, done_flag)。`done_flag` 标记 S_{t+1} 是否是终止状态。\n*   在每个时间步 t，智能体执行动作 A_t，观察到 R_{t+1}, S_{t+1} 后，将这个转移 (S_t, A_t, R_{t+1}, S_{t+1}, done) 存入缓冲区 D。如果缓冲区满了，通常会移除最旧的经验。\n*   在**训练**时，不是使用刚刚产生的那个转移，而是从缓冲区 D 中**随机采样**一个**小批量 (mini-batch)** 的转移 (S_j, A_j, R_{j+1}, S_{j+1}, done_j)。\n*   使用这个 mini-batch 来计算损失并更新网络参数 **w**。\n\n**优点:**\n\n*   **打破数据相关性:** 随机采样打破了原始经验序列的时间相关性，使得样本更接近独立同分布，提高了训练的稳定性和效率。\n*   **提高数据利用率:** 一个经验转移可能被多次采样用于训练，使得智能体能够从过去的经验中反复学习，提高了样本效率。\n\n![Experience Replay](https://spinningup.openai.com/en/latest/_images/experience_replay.png)\n*(图片来源: OpenAI Spinning Up)*\n\n## 2. 目标网络 (Target Network)\n\n**思想:** 使用一个**独立的、更新较慢**的网络来计算 TD 目标值，从而稳定目标。\n\n**机制:**\n\n*   除了主要的 Q 网络 Q̂(s, a; **w**) (也称为 **Online Network**)，再创建一个结构完全相同但参数不同的**目标网络 (Target Network)** Q̂(s, a; **w⁻**)。\n*   在计算 TD 目标时，使用**目标网络**的参数 **w⁻**:\n    *   Target = R + γ max_{a'} Q̂(S', a'; **w⁻**) (如果 S' 非终止)\n    *   Target = R (如果 S' 终止)\n*   **在线网络 Q̂(s, a; w)** 的参数 **w** 在每个训练步（或每几个训练步）通过梯度下降进行更新。\n*   **目标网络 Q̂(s, a; w⁻)** 的参数 **w⁻** **不**通过梯度下降更新，而是**定期**从在线网络复制参数：**w⁻ ← w** (例如，每隔 C 步，C 通常是一个较大的数，如 1000 或 10000)。或者使用**软更新 (Soft Update)**：**w⁻ ← τw + (1-τ)w⁻**，其中 τ 是一个很小的数 (e.g., 0.005)，使得目标网络参数缓慢地跟踪在线网络参数。\n\n**优点:**\n\n*   **稳定 TD 目标:** 目标网络参数 **w⁻** 在一段时间内保持固定，使得 TD 目标值相对稳定，减少了 Q 值更新的震荡，提高了训练稳定性。在线网络 **w** 的更新不再直接影响当前计算的目标值。\n\n# DQN 算法流程 (结合 Experience Replay 和 Target Network)\n\n```\nInitialize:\n  Replay buffer D with capacity N\n  Online Q-network Q̂(s, a; w) with random weights w\n  Target Q-network Q̂(s, a; w⁻) with weights w⁻ = w\n  α ← learning rate\n  γ ← discount factor\n  ε ← initial exploration rate\n  C ← target network update frequency (for hard update) or τ (for soft update)\n\nLoop for each episode:\n  Initialize S (first state)\n  Loop for each step t = 1, T:\n    # 1. Choose action using behavior policy (ε-greedy on online network)\n    With probability ε select random action A_t\n    Otherwise select A_t = argmax_a Q̂(S_t, a; w)\n\n    # 2. Execute action, observe reward R_{t+1} and next state S_{t+1}\n    Execute A_t, observe R_{t+1}, S_{t+1}, done_flag\n\n    # 3. Store transition in replay buffer D\n    Store (S_t, A_t, R_{t+1}, S_{t+1}, done_flag) in D\n\n    # 4. Sample mini-batch from D (if buffer size > learning_starts)\n    If size of D > learning_starts:\n      Sample random mini-batch of transitions (S_j, A_j, R_{j+1}, S_{j+1}, done_j) from D\n\n      # 5. Calculate TD targets using target network\n      Targets = []\n      for j in mini-batch:\n        If done_j:\n          Target_j = R_{j+1}\n        Else:\n          # Use target network w⁻ to get max Q value for next state\n          # Original DQN: Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻)\n          # Double DQN variation (often used in practice):\n          #   a_max = argmax_{a'} Q̂(S_{j+1}, a'; w) # Action selected by online network\n          #   Q_next_target = Q̂(S_{j+1}, a_max; w⁻) # Value evaluated by target network\n          Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻) # Using original DQN target for simplicity here\n          Target_j = R_{j+1} + γ * Q_next_target\n        Targets.append(Target_j)\n\n      # 6. Perform gradient descent step on online network\n      # Get Q values for the actions taken (A_j) from the online network\n      Q_online_values = Q̂(S_j, A_j; w) for j in mini-batch\n      # Calculate loss: e.g., MSE Loss = (1/batch_size) * Σ_j (Targets[j] - Q_online_values[j])²\n      # Update online network weights w using gradient descent: w ← w - α * ∇ Loss(w)\n\n      # 7. Update target network (periodically or softly)\n      # Hard update:\n      # If t % C == 0:\n      #   w⁻ ← w\n      # Soft update (more common in SB3):\n      # w⁻ ← τ*w + (1-τ)*w⁻\n\n    S_t ← S_{t+1} # Move to next state\n\n    If done_flag, break inner loop (episode ends)\n\n  # (Optional) Decay ε\n```\n*(注：步骤 5 中提到了 Double DQN 的变体，这是对原始 DQN 的一个常用改进，用于缓解 Q 值过高估计的问题。原始 DQN 直接使用 `max_{a'} Q̂(S_{j+1}, a'; w⁻)`。SB3 的实现可能包含这类改进。为简化起见，伪代码中仍展示原始 DQN 的目标计算方式。)*\n\n# Lab 6: 使用 Stable Baselines3 运行 DQN 解决 CartPole\n\n## 目标\n\n1.  熟悉使用 Stable Baselines3 (SB3) 库的基本流程：创建环境、定义模型、训练、保存、评估。\n2.  使用 SB3 提供的 DQN 实现来解决 CartPole-v1 问题。\n3.  学习如何设置 DQN 的关键超参数。\n4.  学习如何监控训练过程（观察奖励曲线）。\n5.  评估训练好的模型性能。\n\n## Stable Baselines3 DQN 超参数简介\n\n我们在上周的 SB3 示例代码中看到了一些 DQN 的超参数，这里再解释一下关键的几个：\n\n*   `policy=\"MlpPolicy\"`: 指定使用多层感知机 (MLP) 作为 Q 网络。对于图像输入，可以使用 \"CnnPolicy\"。\n*   `env`: 传入的 Gym/Gymnasium 环境实例（或向量化环境）。\n*   `learning_rate`: 梯度下降的学习率 α。\n*   `buffer_size`: 经验回放缓冲区 D 的大小 N。\n*   `learning_starts`: 收集多少步经验后才开始训练网络（填充缓冲区）。\n*   `batch_size`: 每次从缓冲区采样多少经验进行训练。\n*   `tau`: 软更新目标网络的系数 (SB3 DQN 默认使用软更新，`tau=1.0` 相当于硬更新)。\n*   `gamma`: 折扣因子 γ。\n*   `train_freq`: 每收集多少步经验执行一次训练更新。可以是一个整数（步数），也可以是一个元组 `(frequency, unit)`，如 `(1, \"episode\")` 表示每回合结束时训练一次。\n*   `gradient_steps`: 每次训练更新执行多少次梯度下降步骤。\n*   `target_update_interval`: （硬更新时）每隔多少步将在线网络权重复制到目标网络。SB3 DQN 默认使用软更新（通过 `tau` 控制），这个参数可能不直接使用，但理解其概念很重要。\n*   `exploration_fraction`: 总训练步数中，用于将探索率 ε 从初始值衰减到最终值所占的比例。\n*   `exploration_initial_eps`: 初始探索率 ε (通常为 1.0)。\n*   `exploration_final_eps`: 最终探索率 ε (例如 0.05 或 0.1)。\n*   `verbose`: 控制打印信息的详细程度 (0: 不打印, 1: 打印训练信息, 2: 更详细)。\n\n## 示例代码 (SB3 DQN on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym/\"\nos.makedirs(log_dir, exist_ok=True)\n\n\n# 1. 创建环境 (使用向量化环境加速)\n# 使用 Monitor wrapper 来记录训练过程中的回合奖励等信息\nfrom stable_baselines3.common.monitor import Monitor\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n# Monitor wrapper 通常在 make_vec_env 内部自动添加，或者可以手动添加\n# vec_env = Monitor(vec_env, log_dir) # Monitor 通常用于单个环境，VecEnv有自己的日志记录\n\n# 2. 定义 DQN 模型 (可以调整超参数进行实验)\nmodel = DQN(\"MlpPolicy\", vec_env, verbose=1,\n            learning_rate=1e-4,       # 学习率\n            buffer_size=100000,       # Replay buffer 大小\n            learning_starts=5000,     # 多少步后开始学习\n            batch_size=32,            # Mini-batch 大小\n            tau=1.0,                  # Target network 更新系数 (1.0 for hard update)\n            gamma=0.99,               # 折扣因子\n            train_freq=4,             # 每 4 步训练一次\n            gradient_steps=1,         # 每次训练执行 1 次梯度更新\n            target_update_interval=10000, # Target network 更新频率 (硬更新)\n            exploration_fraction=0.1, # 10% 的步数用于探索率衰减\n            exploration_initial_eps=1.0,# 初始探索率\n            exploration_final_eps=0.05, # 最终探索率\n            optimize_memory_usage=False, # 在内存足够时设为 False 可能更快\n            tensorboard_log=log_dir   # 指定 TensorBoard 日志目录\n           )\n\n# 3. 训练模型\nprint(\"Starting training...\")\nstart_time = time.time()\n# 训练更长时间以看到效果\n# log_interval 控制打印到控制台的频率，TensorBoard 日志默认会记录\nmodel.learn(total_timesteps=100000, log_interval=100)\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"dqn_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained model...\")\n# 创建一个单独的评估环境\neval_env = gym.make(\"CartPole-v1\")\n# n_eval_episodes: 评估多少个回合\n# deterministic=True: 使用贪心策略进行评估\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results: Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 6. (可选) 加载模型并可视化\n# del model # 删除现有模型\n# loaded_model = DQN.load(model_path)\n# print(\"Model loaded.\")\n\n# # 可视化一个回合\n# vis_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n# obs, info = vis_env.reset()\n# terminated = False\n# truncated = False\n# total_reward_vis = 0\n# while not (terminated or truncated):\n#     action, _states = loaded_model.predict(obs, deterministic=True)\n#     obs, reward, terminated, truncated, info = vis_env.step(action)\n#     total_reward_vis += reward\n#     vis_env.render()\n#     # time.sleep(0.01) # Slow down rendering\n# print(f\"Visualization finished. Total reward: {total_reward_vis}\")\n# vis_env.close()\n\n\nvec_env.close()\neval_env.close()\n\n# 提示：可以通过 tensorboard --logdir /tmp/gym/ 查看训练曲线\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n## 任务与思考\n\n1.  **运行代码:** 确保你的环境安装了 `stable-baselines3[extra]`, `pytorch`, `tensorboard`。运行提供的 DQN 代码。观察训练过程中的输出信息。\n2.  **监控训练 (TensorBoard):** 在代码运行时或运行后，在终端中执行 `tensorboard --logdir /tmp/gym/` (或你指定的 `log_dir`)，然后在浏览器中打开显示的地址 (通常是 `http://localhost:6006/`)。查看 `rollout/ep_rew_mean` (平均回合奖励) 曲线。它是否随着训练步数的增加而提高？\n3.  **评估结果:** 查看 `evaluate_policy` 输出的平均奖励和标准差。CartPole-v1 的目标通常是达到平均奖励接近 500 (v1 版本的回合最大步数是 500)。你的模型达到了吗？\n4.  **超参数实验:**\n    *   尝试**改变学习率** (`learning_rate`，例如增大 10 倍或减小 10 倍）。重新训练并观察 TensorBoard 中的曲线以及最终评估结果。\n    *   尝试**改变经验回放缓冲区大小** (`buffer_size`，例如增大或减小）。对结果有什么影响？\n    *   尝试**改变探索参数** (`exploration_fraction`, `exploration_final_eps`）。例如，让探索持续更长时间或最终探索率更高/更低。对学习过程和最终性能有何影响？\n    *   (可选) 尝试**改变网络更新频率** (`train_freq`, `target_update_interval` 或 `tau`)。\n5.  **分析与讨论:**\n    *   解释经验回放和目标网络在 DQN 训练中的作用，它们如何提高稳定性？\n    *   讨论你观察到的不同超参数对训练结果的影响。为什么某些超参数设置效果更好/更差？\n    *   与表格型方法相比，DQN (使用 SB3) 在解决 CartPole 问题上表现如何？为什么函数逼近在这里是必要的？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 DQN 代码。\n*   提交训练过程的 TensorBoard 截图（显示 `rollout/ep_rew_mean` 曲线）。\n*   提交不同超参数设置下的评估结果 (`evaluate_policy` 的输出）。\n*   提交一份简短的分析报告，讨论：\n    *   经验回放和目标网络的作用。\n    *   你观察到的超参数（至少包括学习率和探索参数）对训练的影响。\n    *   DQN 在 CartPole 上的表现以及函数逼近的必要性。\n\n---\n\n**下周预告:** 策略梯度方法 (Policy Gradient Methods)。我们将学习一类不同的 RL 算法，它们直接学习策略函数 π(a|s, θ) 而不是价值函数。\n```\n\n我已经重新生成了 `week10_lecture.qmd` 的内容。请确认。\n\n```tool_code\n<write_to_file>\n<path>week10_lecture.qmd</path>\n<content>\n---\ntitle: \"Week 10: 深度 Q 网络 (DQN)\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    code-fold: show\n    code-tools: true\n---\n\n# 回顾：函数逼近的必要性\n\n上周我们讨论了表格型 RL 方法的局限性：\n\n*   无法处理**巨大或连续的状态空间**（维度灾难）。\n*   无法处理**连续动作空间**（对于 Q-Learning 等）。\n*   缺乏**泛化能力**，需要访问每个状态（或状态-动作对）多次。\n\n解决方案是使用**函数逼近 (Function Approximation)**，用带参数的函数 V̂(s, **w**) 或 Q̂(s, a, **w**) 来近似价值函数。\n\n*   **线性函数逼近:** 简单，但表达能力有限，且依赖特征工程。\n*   **非线性函数逼近 (如神经网络):** 表达能力强，可以自动学习特征表示，是现代强化学习（深度强化学习）的核心。\n\n今天，我们将学习第一个重要的深度强化学习算法：**深度 Q 网络 (Deep Q-Network, DQN)**。\n\n# DQN 核心思想：用神经网络逼近 Q 函数\n\nQ-Learning 的目标是学习最优动作值函数 Q\\*(s, a)。DQN 的核心思想就是使用一个**深度神经网络 (Deep Neural Network, DNN)** 作为函数逼近器来近似 Q\\*(s, a)。\n\nQ̂(s, a; **w**) ≈ Q\\*(s, a)\n\n其中 **w** 代表神经网络的权重和偏置参数。\n\n**网络结构通常是:**\n\n*   **输入:** 状态 s (通常表示为一个向量或张量，例如 CartPole 的 4 维向量，或 Atari 游戏的屏幕像素)。\n*   **输出:** 对于**每个离散动作 a**，输出一个对应的 Q 值估计 Q̂(s, a; **w**)。\n    *   例如，对于 CartPole (动作 0: 左, 动作 1: 右)，网络输出一个包含两个值的向量：[Q̂(s, 0; w), Q̂(s, 1; w)]。\n\n![DQN Network Architecture Example](https://pytorch.org/tutorials/_images/dqn.png)\n*(图片来源: PyTorch Tutorials - DQN)*\n\n**学习过程 (基于 Q-Learning):**\n\n我们仍然使用 Q-Learning 的更新思想，但现在是更新神经网络的参数 **w**，而不是更新表格条目。目标是最小化 **TD 误差**。\n\n回顾 Q-Learning 的 TD 目标：\nTarget = R + γ max_{a'} Q(S', a')\n\n在 DQN 中，我们用神经网络来计算这个目标：\nTarget = R + γ max_{a'} Q̂(S', a'; **w**)\n\n损失函数 (Loss Function) 通常使用**均方误差 (Mean Squared Error, MSE)** 或 **Huber Loss**:\nLoss(**w**) = E [ ( Target - Q̂(S, A; **w**) )² ]\n\n然后使用**梯度下降** (或其变种，如 Adam) 来更新参数 **w**，以减小这个损失：\n**w** ← **w** - α * ∇ Loss(**w**)\n\n# 挑战：Q-Learning + 神经网络 = 不稳定？\n\n将标准的 Q-Learning 直接与非线性函数逼近器（如神经网络）结合，在实践中发现**非常不稳定**，甚至可能**发散 (diverge)**。主要原因有两个：\n\n1.  **样本之间的相关性 (Correlations between samples):**\n    *   RL 智能体收集到的经验数据 (S, A, R, S') 是按时间顺序产生的，相邻的样本之间通常高度相关。\n    *   如果直接按顺序用这些相关的样本来训练神经网络，会违反许多优化算法（如 SGD）关于样本独立同分布 (i.i.d.) 的假设，导致训练效率低下，模型可能在局部数据上过拟合，忘记过去的经验。\n\n2.  **目标值与估计值的耦合 (Non-stationary targets):**\n    *   Q-Learning 的 TD 目标 `Target = R + γ max_{a'} Q̂(S', a'; **w**)` 依赖于当前的 Q 网络参数 **w**。\n    *   这意味着，在训练过程中，我们每更新一次参数 **w**，用于计算损失的**目标值本身也在变化**。\n    *   这就像在追逐一个移动的目标，使得训练过程非常不稳定，Q 值可能会剧烈震荡甚至发散。\n\n# DQN 的关键技巧\n\n为了解决上述稳定性问题，DQN 引入了两个关键技巧：\n\n## 1. 经验回放 (Experience Replay)\n\n**思想:** 不再按顺序使用实时产生的经验来训练网络，而是将经验存储起来，然后随机采样进行训练。\n\n**机制:**\n\n*   维护一个**回放缓冲区 (Replay Buffer / Memory)** D，用于存储大量的历史转移 (transitions): (S_t, A_t, R_{t+1}, S_{t+1}, done_flag)。`done_flag` 标记 S_{t+1} 是否是终止状态。\n*   在每个时间步 t，智能体执行动作 A_t，观察到 R_{t+1}, S_{t+1} 后，将这个转移 (S_t, A_t, R_{t+1}, S_{t+1}, done) 存入缓冲区 D。如果缓冲区满了，通常会移除最旧的经验。\n*   在**训练**时，不是使用刚刚产生的那个转移，而是从缓冲区 D 中**随机采样**一个**小批量 (mini-batch)** 的转移 (S_j, A_j, R_{j+1}, S_{j+1}, done_j)。\n*   使用这个 mini-batch 来计算损失并更新网络参数 **w**。\n\n**优点:**\n\n*   **打破数据相关性:** 随机采样打破了原始经验序列的时间相关性，使得样本更接近独立同分布，提高了训练的稳定性和效率。\n*   **提高数据利用率:** 一个经验转移可能被多次采样用于训练，使得智能体能够从过去的经验中反复学习，提高了样本效率。\n\n![Experience Replay](https://spinningup.openai.com/en/latest/_images/experience_replay.png)\n*(图片来源: OpenAI Spinning Up)*\n\n## 2. 目标网络 (Target Network)\n\n**思想:** 使用一个**独立的、更新较慢**的网络来计算 TD 目标值，从而稳定目标。\n\n**机制:**\n\n*   除了主要的 Q 网络 Q̂(s, a; **w**) (也称为 **Online Network**)，再创建一个结构完全相同但参数不同的**目标网络 (Target Network)** Q̂(s, a; **w⁻**)。\n*   在计算 TD 目标时，使用**目标网络**的参数 **w⁻**:\n    *   Target = R + γ max_{a'} Q̂(S', a'; **w⁻**) (如果 S' 非终止)\n    *   Target = R (如果 S' 终止)\n*   **在线网络 Q̂(s, a; w)** 的参数 **w** 在每个训练步（或每几个训练步）通过梯度下降进行更新。\n*   **目标网络 Q̂(s, a; w⁻)** 的参数 **w⁻** **不**通过梯度下降更新，而是**定期**从在线网络复制参数：**w⁻ ← w** (例如，每隔 C 步，C 通常是一个较大的数，如 1000 或 10000)。或者使用**软更新 (Soft Update)**：**w⁻ ← τw + (1-τ)w⁻**，其中 τ 是一个很小的数 (e.g., 0.005)，使得目标网络参数缓慢地跟踪在线网络参数。\n\n**优点:**\n\n*   **稳定 TD 目标:** 目标网络参数 **w⁻** 在一段时间内保持固定，使得 TD 目标值相对稳定，减少了 Q 值更新的震荡，提高了训练稳定性。在线网络 **w** 的更新不再直接影响当前计算的目标值。\n\n# DQN 算法流程 (结合 Experience Replay 和 Target Network)\n\n```\nInitialize:\n  Replay buffer D with capacity N\n  Online Q-network Q̂(s, a; w) with random weights w\n  Target Q-network Q̂(s, a; w⁻) with weights w⁻ = w\n  α ← learning rate\n  γ ← discount factor\n  ε ← initial exploration rate\n  C ← target network update frequency (for hard update) or τ (for soft update)\n\nLoop for each episode:\n  Initialize S (first state)\n  Loop for each step t = 1, T:\n    # 1. Choose action using behavior policy (ε-greedy on online network)\n    With probability ε select random action A_t\n    Otherwise select A_t = argmax_a Q̂(S_t, a; w)\n\n    # 2. Execute action, observe reward R_{t+1} and next state S_{t+1}\n    Execute A_t, observe R_{t+1}, S_{t+1}, done_flag\n\n    # 3. Store transition in replay buffer D\n    Store (S_t, A_t, R_{t+1}, S_{t+1}, done_flag) in D\n\n    # 4. Sample mini-batch from D (if buffer size > learning_starts)\n    If size of D > learning_starts:\n      Sample random mini-batch of transitions (S_j, A_j, R_{j+1}, S_{j+1}, done_j) from D\n\n      # 5. Calculate TD targets using target network\n      Targets = []\n      for j in mini-batch:\n        If done_j:\n          Target_j = R_{j+1}\n        Else:\n          # Use target network w⁻ to get max Q value for next state\n          # Original DQN: Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻)\n          # Double DQN variation (often used in practice):\n          #   a_max = argmax_{a'} Q̂(S_{j+1}, a'; w) # Action selected by online network\n          #   Q_next_target = Q̂(S_{j+1}, a_max; w⁻) # Value evaluated by target network\n          Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻) # Using original DQN target for simplicity here\n          Target_j = R_{j+1} + γ * Q_next_target\n        Targets.append(Target_j)\n\n      # 6. Perform gradient descent step on online network\n      # Get Q values for the actions taken (A_j) from the online network\n      Q_online_values = Q̂(S_j, A_j; w) for j in mini-batch\n      # Calculate loss: e.g., MSE Loss = (1/batch_size) * Σ_j (Targets[j] - Q_online_values[j])²\n      # Update online network weights w using gradient descent: w ← w - α * ∇ Loss(w)\n\n      # 7. Update target network (periodically or softly)\n      # Hard update:\n      # If t % C == 0:\n      #   w⁻ ← w\n      # Soft update (more common in SB3):\n      # w⁻ ← τ*w + (1-τ)*w⁻\n\n    S_t ← S_{t+1} # Move to next state\n\n    If done_flag, break inner loop (episode ends)\n\n  # (Optional) Decay ε\n```\n*(注：步骤 5 中提到了 Double DQN 的变体，这是对原始 DQN 的一个常用改进，用于缓解 Q 值过高估计的问题。原始 DQN 直接使用 `max_{a'} Q̂(S_{j+1}, a'; w⁻)`。SB3 的实现可能包含这类改进。为简化起见，伪代码中仍展示原始 DQN 的目标计算方式。)*\n\n# Lab 6: 使用 Stable Baselines3 运行 DQN 解决 CartPole\n\n## 目标\n\n1.  熟悉使用 Stable Baselines3 (SB3) 库的基本流程：创建环境、定义模型、训练、保存、评估。\n2.  使用 SB3 提供的 DQN 实现来解决 CartPole-v1 问题。\n3.  学习如何设置 DQN 的关键超参数。\n4.  学习如何监控训练过程（观察奖励曲线）。\n5.  评估训练好的模型性能。\n\n## Stable Baselines3 DQN 超参数简介\n\n我们在上周的 SB3 示例代码中看到了一些 DQN 的超参数，这里再解释一下关键的几个：\n\n*   `policy=\"MlpPolicy\"`: 指定使用多层感知机 (MLP) 作为 Q 网络。对于图像输入，可以使用 \"CnnPolicy\"。\n*   `env`: 传入的 Gym/Gymnasium 环境实例（或向量化环境）。\n*   `learning_rate`: 梯度下降的学习率 α。\n*   `buffer_size`: 经验回放缓冲区 D 的大小 N。\n*   `learning_starts`: 收集多少步经验后才开始训练网络（填充缓冲区）。\n*   `batch_size`: 每次从缓冲区采样多少经验进行训练。\n*   `tau`: 软更新目标网络的系数 (SB3 DQN 默认使用软更新，`tau=1.0` 相当于硬更新)。\n*   `gamma`: 折扣因子 γ。\n*   `train_freq`: 每收集多少步经验执行一次训练更新。可以是一个整数（步数），也可以是一个元组 `(frequency, unit)`，如 `(1, \"episode\")` 表示每回合结束时训练一次。\n*   `gradient_steps`: 每次训练更新执行多少次梯度下降步骤。\n*   `target_update_interval`: （硬更新时）每隔多少步将在线网络权重复制到目标网络。SB3 DQN 默认使用软更新（通过 `tau` 控制），这个参数可能不直接使用，但理解其概念很重要。\n*   `exploration_fraction`: 总训练步数中，用于将探索率 ε 从初始值衰减到最终值所占的比例。\n*   `exploration_initial_eps`: 初始探索率 ε (通常为 1.0)。\n*   `exploration_final_eps`: 最终探索率 ε (例如 0.05 或 0.1)。\n*   `verbose`: 控制打印信息的详细程度 (0: 不打印, 1: 打印训练信息, 2: 更详细)。\n\n## 示例代码 (SB3 DQN on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym/\"\nos.makedirs(log_dir, exist_ok=True)\n\n\n# 1. 创建环境 (使用向量化环境加速)\n# 使用 Monitor wrapper 来记录训练过程中的回合奖励等信息\nfrom stable_baselines3.common.monitor import Monitor\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n# Monitor wrapper 通常在 make_vec_env 内部自动添加，或者可以手动添加\n# vec_env = Monitor(vec_env, log_dir) # Monitor 通常用于单个环境，VecEnv有自己的日志记录\n\n# 2. 定义 DQN 模型 (可以调整超参数进行实验)\nmodel = DQN(\"MlpPolicy\", vec_env, verbose=1,\n            learning_rate=1e-4,       # 学习率\n            buffer_size=100000,       # Replay buffer 大小\n            learning_starts=5000,     # 多少步后开始学习\n            batch_size=32,            # Mini-batch 大小\n            tau=1.0,                  # Target network 更新系数 (1.0 for hard update)\n            gamma=0.99,               # 折扣因子\n            train_freq=4,             # 每 4 步训练一次\n            gradient_steps=1,         # 每次训练执行 1 次梯度更新\n            target_update_interval=10000, # Target network 更新频率 (硬更新)\n            exploration_fraction=0.1, # 10% 的步数用于探索率衰减\n            exploration_initial_eps=1.0,# 初始探索率\n            exploration_final_eps=0.05, # 最终探索率\n            optimize_memory_usage=False, # 在内存足够时设为 False 可能更快\n            tensorboard_log=log_dir   # 指定 TensorBoard 日志目录\n           )\n\n# 3. 训练模型\nprint(\"Starting training...\")\nstart_time = time.time()\n# 训练更长时间以看到效果\n# log_interval 控制打印到控制台的频率，TensorBoard 日志默认会记录\nmodel.learn(total_timesteps=100000, log_interval=100)\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"dqn_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained model...\")\n# 创建一个单独的评估环境\neval_env = gym.make(\"CartPole-v1\")\n# n_eval_episodes: 评估多少个回合\n# deterministic=True: 使用贪心策略进行评估\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results: Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 6. (可选) 加载模型并可视化\n# del model # 删除现有模型\n# loaded_model = DQN.load(model_path)\n# print(\"Model loaded.\")\n\n# # 可视化一个回合\n# vis_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n# obs, info = vis_env.reset()\n# terminated = False\n# truncated = False\n# total_reward_vis = 0\n# while not (terminated or truncated):\n#     action, _states = loaded_model.predict(obs, deterministic=True)\n#     obs, reward, terminated, truncated, info = vis_env.step(action)\n#     total_reward_vis += reward\n#     vis_env.render()\n#     # time.sleep(0.01) # Slow down rendering\n# print(f\"Visualization finished. Total reward: {total_reward_vis}\")\n# vis_env.close()\n\n\nvec_env.close()\neval_env.close()\n\n# 提示：可以通过 tensorboard --logdir /tmp/gym/ 查看训练曲线\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n## 任务与思考\n\n1.  **运行代码:** 确保你的环境安装了 `stable-baselines3[extra]`, `pytorch`, `tensorboard`。运行提供的 DQN 代码。观察训练过程中的输出信息。\n2.  **监控训练 (TensorBoard):** 在代码运行时或运行后，在终端中执行 `tensorboard --logdir /tmp/gym/` (或你指定的 `log_dir`)，然后在浏览器中打开显示的地址 (通常是 `http://localhost:6006/`)。查看 `rollout/ep_rew_mean` (平均回合奖励) 曲线。它是否随着训练步数的增加而提高？\n3.  **评估结果:** 查看 `evaluate_policy` 输出的平均奖励和标准差。CartPole-v1 的目标通常是达到平均奖励接近 500 (v1 版本的回合最大步数是 500)。你的模型达到了吗？\n4.  **超参数实验:**\n    *   尝试**改变学习率** (`learning_rate`，例如增大 10 倍或减小 10 倍）。重新训练并观察 TensorBoard 中的曲线以及最终评估结果。\n    *   尝试**改变经验回放缓冲区大小** (`buffer_size`，例如增大或减小）。对结果有什么影响？\n    *   尝试**改变探索参数** (`exploration_fraction`, `exploration_final_eps`）。例如，让探索持续更长时间或最终探索率更高/更低。对学习过程和最终性能有何影响？\n    *   (可选) 尝试**改变网络更新频率** (`train_freq`, `target_update_interval` 或 `tau`)。\n5.  **分析与讨论:**\n    *   解释经验回放和目标网络在 DQN 训练中的作用，它们如何提高稳定性？\n    *   讨论你观察到的不同超参数对训练结果的影响。为什么某些超参数设置效果更好/更差？\n    *   与表格型方法相比，DQN (使用 SB3) 在解决 CartPole 问题上表现如何？为什么函数逼近在这里是必要的？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 DQN 代码。\n*   提交训练过程的 TensorBoard 截图（显示 `rollout/ep_rew_mean` 曲线）。\n*   提交不同超参数设置下的评估结果 (`evaluate_policy` 的输出）。\n*   提交一份简短的分析报告，讨论：\n    *   经验回放和目标网络的作用。\n    *   你观察到的超参数（至少包括学习率和探索参数）对训练的影响。\n    *   DQN 在 CartPole 上的表现以及函数逼近的必要性。\n\n---\n\n**下周预告:** 策略梯度方法 (Policy Gradient Methods)。我们将学习一类不同的 RL 算法，它们直接学习策略函数 π(a|s, θ) 而不是价值函数。","srcMarkdownNoYaml":"\n\n# 回顾：函数逼近的必要性\n\n上周我们讨论了表格型 RL 方法的局限性：\n\n*   无法处理**巨大或连续的状态空间**（维度灾难）。\n*   无法处理**连续动作空间**（对于 Q-Learning 等）。\n*   缺乏**泛化能力**，需要访问每个状态（或状态-动作对）多次。\n\n解决方案是使用**函数逼近 (Function Approximation)**，用带参数的函数 V̂(s, **w**) 或 Q̂(s, a, **w**) 来近似价值函数。\n\n*   **线性函数逼近:** 简单，但表达能力有限，且依赖特征工程。\n*   **非线性函数逼近 (如神经网络):** 表达能力强，可以自动学习特征表示，是现代强化学习（深度强化学习）的核心。\n\n今天，我们将学习第一个重要的深度强化学习算法：**深度 Q 网络 (Deep Q-Network, DQN)**。\n\n# DQN 核心思想：用神经网络逼近 Q 函数\n\nQ-Learning 的目标是学习最优动作值函数 Q\\*(s, a)。DQN 的核心思想就是使用一个**深度神经网络 (Deep Neural Network, DNN)** 作为函数逼近器来近似 Q\\*(s, a)。\n\nQ̂(s, a; **w**) ≈ Q\\*(s, a)\n\n其中 **w** 代表神经网络的权重和偏置参数。\n\n**网络结构通常是:**\n\n*   **输入:** 状态 s (通常表示为一个向量或张量，例如 CartPole 的 4 维向量，或 Atari 游戏的屏幕像素)。\n*   **输出:** 对于**每个离散动作 a**，输出一个对应的 Q 值估计 Q̂(s, a; **w**)。\n    *   例如，对于 CartPole (动作 0: 左, 动作 1: 右)，网络输出一个包含两个值的向量：[Q̂(s, 0; w), Q̂(s, 1; w)]。\n\n![DQN Network Architecture Example](https://pytorch.org/tutorials/_images/dqn.png)\n*(图片来源: PyTorch Tutorials - DQN)*\n\n**学习过程 (基于 Q-Learning):**\n\n我们仍然使用 Q-Learning 的更新思想，但现在是更新神经网络的参数 **w**，而不是更新表格条目。目标是最小化 **TD 误差**。\n\n回顾 Q-Learning 的 TD 目标：\nTarget = R + γ max_{a'} Q(S', a')\n\n在 DQN 中，我们用神经网络来计算这个目标：\nTarget = R + γ max_{a'} Q̂(S', a'; **w**)\n\n损失函数 (Loss Function) 通常使用**均方误差 (Mean Squared Error, MSE)** 或 **Huber Loss**:\nLoss(**w**) = E [ ( Target - Q̂(S, A; **w**) )² ]\n\n然后使用**梯度下降** (或其变种，如 Adam) 来更新参数 **w**，以减小这个损失：\n**w** ← **w** - α * ∇ Loss(**w**)\n\n# 挑战：Q-Learning + 神经网络 = 不稳定？\n\n将标准的 Q-Learning 直接与非线性函数逼近器（如神经网络）结合，在实践中发现**非常不稳定**，甚至可能**发散 (diverge)**。主要原因有两个：\n\n1.  **样本之间的相关性 (Correlations between samples):**\n    *   RL 智能体收集到的经验数据 (S, A, R, S') 是按时间顺序产生的，相邻的样本之间通常高度相关。\n    *   如果直接按顺序用这些相关的样本来训练神经网络，会违反许多优化算法（如 SGD）关于样本独立同分布 (i.i.d.) 的假设，导致训练效率低下，模型可能在局部数据上过拟合，忘记过去的经验。\n\n2.  **目标值与估计值的耦合 (Non-stationary targets):**\n    *   Q-Learning 的 TD 目标 `Target = R + γ max_{a'} Q̂(S', a'; **w**)` 依赖于当前的 Q 网络参数 **w**。\n    *   这意味着，在训练过程中，我们每更新一次参数 **w**，用于计算损失的**目标值本身也在变化**。\n    *   这就像在追逐一个移动的目标，使得训练过程非常不稳定，Q 值可能会剧烈震荡甚至发散。\n\n# DQN 的关键技巧\n\n为了解决上述稳定性问题，DQN 引入了两个关键技巧：\n\n## 1. 经验回放 (Experience Replay)\n\n**思想:** 不再按顺序使用实时产生的经验来训练网络，而是将经验存储起来，然后随机采样进行训练。\n\n**机制:**\n\n*   维护一个**回放缓冲区 (Replay Buffer / Memory)** D，用于存储大量的历史转移 (transitions): (S_t, A_t, R_{t+1}, S_{t+1}, done_flag)。`done_flag` 标记 S_{t+1} 是否是终止状态。\n*   在每个时间步 t，智能体执行动作 A_t，观察到 R_{t+1}, S_{t+1} 后，将这个转移 (S_t, A_t, R_{t+1}, S_{t+1}, done) 存入缓冲区 D。如果缓冲区满了，通常会移除最旧的经验。\n*   在**训练**时，不是使用刚刚产生的那个转移，而是从缓冲区 D 中**随机采样**一个**小批量 (mini-batch)** 的转移 (S_j, A_j, R_{j+1}, S_{j+1}, done_j)。\n*   使用这个 mini-batch 来计算损失并更新网络参数 **w**。\n\n**优点:**\n\n*   **打破数据相关性:** 随机采样打破了原始经验序列的时间相关性，使得样本更接近独立同分布，提高了训练的稳定性和效率。\n*   **提高数据利用率:** 一个经验转移可能被多次采样用于训练，使得智能体能够从过去的经验中反复学习，提高了样本效率。\n\n![Experience Replay](https://spinningup.openai.com/en/latest/_images/experience_replay.png)\n*(图片来源: OpenAI Spinning Up)*\n\n## 2. 目标网络 (Target Network)\n\n**思想:** 使用一个**独立的、更新较慢**的网络来计算 TD 目标值，从而稳定目标。\n\n**机制:**\n\n*   除了主要的 Q 网络 Q̂(s, a; **w**) (也称为 **Online Network**)，再创建一个结构完全相同但参数不同的**目标网络 (Target Network)** Q̂(s, a; **w⁻**)。\n*   在计算 TD 目标时，使用**目标网络**的参数 **w⁻**:\n    *   Target = R + γ max_{a'} Q̂(S', a'; **w⁻**) (如果 S' 非终止)\n    *   Target = R (如果 S' 终止)\n*   **在线网络 Q̂(s, a; w)** 的参数 **w** 在每个训练步（或每几个训练步）通过梯度下降进行更新。\n*   **目标网络 Q̂(s, a; w⁻)** 的参数 **w⁻** **不**通过梯度下降更新，而是**定期**从在线网络复制参数：**w⁻ ← w** (例如，每隔 C 步，C 通常是一个较大的数，如 1000 或 10000)。或者使用**软更新 (Soft Update)**：**w⁻ ← τw + (1-τ)w⁻**，其中 τ 是一个很小的数 (e.g., 0.005)，使得目标网络参数缓慢地跟踪在线网络参数。\n\n**优点:**\n\n*   **稳定 TD 目标:** 目标网络参数 **w⁻** 在一段时间内保持固定，使得 TD 目标值相对稳定，减少了 Q 值更新的震荡，提高了训练稳定性。在线网络 **w** 的更新不再直接影响当前计算的目标值。\n\n# DQN 算法流程 (结合 Experience Replay 和 Target Network)\n\n```\nInitialize:\n  Replay buffer D with capacity N\n  Online Q-network Q̂(s, a; w) with random weights w\n  Target Q-network Q̂(s, a; w⁻) with weights w⁻ = w\n  α ← learning rate\n  γ ← discount factor\n  ε ← initial exploration rate\n  C ← target network update frequency (for hard update) or τ (for soft update)\n\nLoop for each episode:\n  Initialize S (first state)\n  Loop for each step t = 1, T:\n    # 1. Choose action using behavior policy (ε-greedy on online network)\n    With probability ε select random action A_t\n    Otherwise select A_t = argmax_a Q̂(S_t, a; w)\n\n    # 2. Execute action, observe reward R_{t+1} and next state S_{t+1}\n    Execute A_t, observe R_{t+1}, S_{t+1}, done_flag\n\n    # 3. Store transition in replay buffer D\n    Store (S_t, A_t, R_{t+1}, S_{t+1}, done_flag) in D\n\n    # 4. Sample mini-batch from D (if buffer size > learning_starts)\n    If size of D > learning_starts:\n      Sample random mini-batch of transitions (S_j, A_j, R_{j+1}, S_{j+1}, done_j) from D\n\n      # 5. Calculate TD targets using target network\n      Targets = []\n      for j in mini-batch:\n        If done_j:\n          Target_j = R_{j+1}\n        Else:\n          # Use target network w⁻ to get max Q value for next state\n          # Original DQN: Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻)\n          # Double DQN variation (often used in practice):\n          #   a_max = argmax_{a'} Q̂(S_{j+1}, a'; w) # Action selected by online network\n          #   Q_next_target = Q̂(S_{j+1}, a_max; w⁻) # Value evaluated by target network\n          Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻) # Using original DQN target for simplicity here\n          Target_j = R_{j+1} + γ * Q_next_target\n        Targets.append(Target_j)\n\n      # 6. Perform gradient descent step on online network\n      # Get Q values for the actions taken (A_j) from the online network\n      Q_online_values = Q̂(S_j, A_j; w) for j in mini-batch\n      # Calculate loss: e.g., MSE Loss = (1/batch_size) * Σ_j (Targets[j] - Q_online_values[j])²\n      # Update online network weights w using gradient descent: w ← w - α * ∇ Loss(w)\n\n      # 7. Update target network (periodically or softly)\n      # Hard update:\n      # If t % C == 0:\n      #   w⁻ ← w\n      # Soft update (more common in SB3):\n      # w⁻ ← τ*w + (1-τ)*w⁻\n\n    S_t ← S_{t+1} # Move to next state\n\n    If done_flag, break inner loop (episode ends)\n\n  # (Optional) Decay ε\n```\n*(注：步骤 5 中提到了 Double DQN 的变体，这是对原始 DQN 的一个常用改进，用于缓解 Q 值过高估计的问题。原始 DQN 直接使用 `max_{a'} Q̂(S_{j+1}, a'; w⁻)`。SB3 的实现可能包含这类改进。为简化起见，伪代码中仍展示原始 DQN 的目标计算方式。)*\n\n# Lab 6: 使用 Stable Baselines3 运行 DQN 解决 CartPole\n\n## 目标\n\n1.  熟悉使用 Stable Baselines3 (SB3) 库的基本流程：创建环境、定义模型、训练、保存、评估。\n2.  使用 SB3 提供的 DQN 实现来解决 CartPole-v1 问题。\n3.  学习如何设置 DQN 的关键超参数。\n4.  学习如何监控训练过程（观察奖励曲线）。\n5.  评估训练好的模型性能。\n\n## Stable Baselines3 DQN 超参数简介\n\n我们在上周的 SB3 示例代码中看到了一些 DQN 的超参数，这里再解释一下关键的几个：\n\n*   `policy=\"MlpPolicy\"`: 指定使用多层感知机 (MLP) 作为 Q 网络。对于图像输入，可以使用 \"CnnPolicy\"。\n*   `env`: 传入的 Gym/Gymnasium 环境实例（或向量化环境）。\n*   `learning_rate`: 梯度下降的学习率 α。\n*   `buffer_size`: 经验回放缓冲区 D 的大小 N。\n*   `learning_starts`: 收集多少步经验后才开始训练网络（填充缓冲区）。\n*   `batch_size`: 每次从缓冲区采样多少经验进行训练。\n*   `tau`: 软更新目标网络的系数 (SB3 DQN 默认使用软更新，`tau=1.0` 相当于硬更新)。\n*   `gamma`: 折扣因子 γ。\n*   `train_freq`: 每收集多少步经验执行一次训练更新。可以是一个整数（步数），也可以是一个元组 `(frequency, unit)`，如 `(1, \"episode\")` 表示每回合结束时训练一次。\n*   `gradient_steps`: 每次训练更新执行多少次梯度下降步骤。\n*   `target_update_interval`: （硬更新时）每隔多少步将在线网络权重复制到目标网络。SB3 DQN 默认使用软更新（通过 `tau` 控制），这个参数可能不直接使用，但理解其概念很重要。\n*   `exploration_fraction`: 总训练步数中，用于将探索率 ε 从初始值衰减到最终值所占的比例。\n*   `exploration_initial_eps`: 初始探索率 ε (通常为 1.0)。\n*   `exploration_final_eps`: 最终探索率 ε (例如 0.05 或 0.1)。\n*   `verbose`: 控制打印信息的详细程度 (0: 不打印, 1: 打印训练信息, 2: 更详细)。\n\n## 示例代码 (SB3 DQN on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym/\"\nos.makedirs(log_dir, exist_ok=True)\n\n\n# 1. 创建环境 (使用向量化环境加速)\n# 使用 Monitor wrapper 来记录训练过程中的回合奖励等信息\nfrom stable_baselines3.common.monitor import Monitor\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n# Monitor wrapper 通常在 make_vec_env 内部自动添加，或者可以手动添加\n# vec_env = Monitor(vec_env, log_dir) # Monitor 通常用于单个环境，VecEnv有自己的日志记录\n\n# 2. 定义 DQN 模型 (可以调整超参数进行实验)\nmodel = DQN(\"MlpPolicy\", vec_env, verbose=1,\n            learning_rate=1e-4,       # 学习率\n            buffer_size=100000,       # Replay buffer 大小\n            learning_starts=5000,     # 多少步后开始学习\n            batch_size=32,            # Mini-batch 大小\n            tau=1.0,                  # Target network 更新系数 (1.0 for hard update)\n            gamma=0.99,               # 折扣因子\n            train_freq=4,             # 每 4 步训练一次\n            gradient_steps=1,         # 每次训练执行 1 次梯度更新\n            target_update_interval=10000, # Target network 更新频率 (硬更新)\n            exploration_fraction=0.1, # 10% 的步数用于探索率衰减\n            exploration_initial_eps=1.0,# 初始探索率\n            exploration_final_eps=0.05, # 最终探索率\n            optimize_memory_usage=False, # 在内存足够时设为 False 可能更快\n            tensorboard_log=log_dir   # 指定 TensorBoard 日志目录\n           )\n\n# 3. 训练模型\nprint(\"Starting training...\")\nstart_time = time.time()\n# 训练更长时间以看到效果\n# log_interval 控制打印到控制台的频率，TensorBoard 日志默认会记录\nmodel.learn(total_timesteps=100000, log_interval=100)\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"dqn_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained model...\")\n# 创建一个单独的评估环境\neval_env = gym.make(\"CartPole-v1\")\n# n_eval_episodes: 评估多少个回合\n# deterministic=True: 使用贪心策略进行评估\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results: Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 6. (可选) 加载模型并可视化\n# del model # 删除现有模型\n# loaded_model = DQN.load(model_path)\n# print(\"Model loaded.\")\n\n# # 可视化一个回合\n# vis_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n# obs, info = vis_env.reset()\n# terminated = False\n# truncated = False\n# total_reward_vis = 0\n# while not (terminated or truncated):\n#     action, _states = loaded_model.predict(obs, deterministic=True)\n#     obs, reward, terminated, truncated, info = vis_env.step(action)\n#     total_reward_vis += reward\n#     vis_env.render()\n#     # time.sleep(0.01) # Slow down rendering\n# print(f\"Visualization finished. Total reward: {total_reward_vis}\")\n# vis_env.close()\n\n\nvec_env.close()\neval_env.close()\n\n# 提示：可以通过 tensorboard --logdir /tmp/gym/ 查看训练曲线\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n## 任务与思考\n\n1.  **运行代码:** 确保你的环境安装了 `stable-baselines3[extra]`, `pytorch`, `tensorboard`。运行提供的 DQN 代码。观察训练过程中的输出信息。\n2.  **监控训练 (TensorBoard):** 在代码运行时或运行后，在终端中执行 `tensorboard --logdir /tmp/gym/` (或你指定的 `log_dir`)，然后在浏览器中打开显示的地址 (通常是 `http://localhost:6006/`)。查看 `rollout/ep_rew_mean` (平均回合奖励) 曲线。它是否随着训练步数的增加而提高？\n3.  **评估结果:** 查看 `evaluate_policy` 输出的平均奖励和标准差。CartPole-v1 的目标通常是达到平均奖励接近 500 (v1 版本的回合最大步数是 500)。你的模型达到了吗？\n4.  **超参数实验:**\n    *   尝试**改变学习率** (`learning_rate`，例如增大 10 倍或减小 10 倍）。重新训练并观察 TensorBoard 中的曲线以及最终评估结果。\n    *   尝试**改变经验回放缓冲区大小** (`buffer_size`，例如增大或减小）。对结果有什么影响？\n    *   尝试**改变探索参数** (`exploration_fraction`, `exploration_final_eps`）。例如，让探索持续更长时间或最终探索率更高/更低。对学习过程和最终性能有何影响？\n    *   (可选) 尝试**改变网络更新频率** (`train_freq`, `target_update_interval` 或 `tau`)。\n5.  **分析与讨论:**\n    *   解释经验回放和目标网络在 DQN 训练中的作用，它们如何提高稳定性？\n    *   讨论你观察到的不同超参数对训练结果的影响。为什么某些超参数设置效果更好/更差？\n    *   与表格型方法相比，DQN (使用 SB3) 在解决 CartPole 问题上表现如何？为什么函数逼近在这里是必要的？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 DQN 代码。\n*   提交训练过程的 TensorBoard 截图（显示 `rollout/ep_rew_mean` 曲线）。\n*   提交不同超参数设置下的评估结果 (`evaluate_policy` 的输出）。\n*   提交一份简短的分析报告，讨论：\n    *   经验回放和目标网络的作用。\n    *   你观察到的超参数（至少包括学习率和探索参数）对训练的影响。\n    *   DQN 在 CartPole 上的表现以及函数逼近的必要性。\n\n---\n\n**下周预告:** 策略梯度方法 (Policy Gradient Methods)。我们将学习一类不同的 RL 算法，它们直接学习策略函数 π(a|s, θ) 而不是价值函数。\n```\n\n我已经重新生成了 `week10_lecture.qmd` 的内容。请确认。\n\n```tool_code\n<write_to_file>\n<path>week10_lecture.qmd</path>\n<content>\n---\ntitle: \"Week 10: 深度 Q 网络 (DQN)\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    number-sections: true\n    code-fold: show\n    code-tools: true\n---\n\n# 回顾：函数逼近的必要性\n\n上周我们讨论了表格型 RL 方法的局限性：\n\n*   无法处理**巨大或连续的状态空间**（维度灾难）。\n*   无法处理**连续动作空间**（对于 Q-Learning 等）。\n*   缺乏**泛化能力**，需要访问每个状态（或状态-动作对）多次。\n\n解决方案是使用**函数逼近 (Function Approximation)**，用带参数的函数 V̂(s, **w**) 或 Q̂(s, a, **w**) 来近似价值函数。\n\n*   **线性函数逼近:** 简单，但表达能力有限，且依赖特征工程。\n*   **非线性函数逼近 (如神经网络):** 表达能力强，可以自动学习特征表示，是现代强化学习（深度强化学习）的核心。\n\n今天，我们将学习第一个重要的深度强化学习算法：**深度 Q 网络 (Deep Q-Network, DQN)**。\n\n# DQN 核心思想：用神经网络逼近 Q 函数\n\nQ-Learning 的目标是学习最优动作值函数 Q\\*(s, a)。DQN 的核心思想就是使用一个**深度神经网络 (Deep Neural Network, DNN)** 作为函数逼近器来近似 Q\\*(s, a)。\n\nQ̂(s, a; **w**) ≈ Q\\*(s, a)\n\n其中 **w** 代表神经网络的权重和偏置参数。\n\n**网络结构通常是:**\n\n*   **输入:** 状态 s (通常表示为一个向量或张量，例如 CartPole 的 4 维向量，或 Atari 游戏的屏幕像素)。\n*   **输出:** 对于**每个离散动作 a**，输出一个对应的 Q 值估计 Q̂(s, a; **w**)。\n    *   例如，对于 CartPole (动作 0: 左, 动作 1: 右)，网络输出一个包含两个值的向量：[Q̂(s, 0; w), Q̂(s, 1; w)]。\n\n![DQN Network Architecture Example](https://pytorch.org/tutorials/_images/dqn.png)\n*(图片来源: PyTorch Tutorials - DQN)*\n\n**学习过程 (基于 Q-Learning):**\n\n我们仍然使用 Q-Learning 的更新思想，但现在是更新神经网络的参数 **w**，而不是更新表格条目。目标是最小化 **TD 误差**。\n\n回顾 Q-Learning 的 TD 目标：\nTarget = R + γ max_{a'} Q(S', a')\n\n在 DQN 中，我们用神经网络来计算这个目标：\nTarget = R + γ max_{a'} Q̂(S', a'; **w**)\n\n损失函数 (Loss Function) 通常使用**均方误差 (Mean Squared Error, MSE)** 或 **Huber Loss**:\nLoss(**w**) = E [ ( Target - Q̂(S, A; **w**) )² ]\n\n然后使用**梯度下降** (或其变种，如 Adam) 来更新参数 **w**，以减小这个损失：\n**w** ← **w** - α * ∇ Loss(**w**)\n\n# 挑战：Q-Learning + 神经网络 = 不稳定？\n\n将标准的 Q-Learning 直接与非线性函数逼近器（如神经网络）结合，在实践中发现**非常不稳定**，甚至可能**发散 (diverge)**。主要原因有两个：\n\n1.  **样本之间的相关性 (Correlations between samples):**\n    *   RL 智能体收集到的经验数据 (S, A, R, S') 是按时间顺序产生的，相邻的样本之间通常高度相关。\n    *   如果直接按顺序用这些相关的样本来训练神经网络，会违反许多优化算法（如 SGD）关于样本独立同分布 (i.i.d.) 的假设，导致训练效率低下，模型可能在局部数据上过拟合，忘记过去的经验。\n\n2.  **目标值与估计值的耦合 (Non-stationary targets):**\n    *   Q-Learning 的 TD 目标 `Target = R + γ max_{a'} Q̂(S', a'; **w**)` 依赖于当前的 Q 网络参数 **w**。\n    *   这意味着，在训练过程中，我们每更新一次参数 **w**，用于计算损失的**目标值本身也在变化**。\n    *   这就像在追逐一个移动的目标，使得训练过程非常不稳定，Q 值可能会剧烈震荡甚至发散。\n\n# DQN 的关键技巧\n\n为了解决上述稳定性问题，DQN 引入了两个关键技巧：\n\n## 1. 经验回放 (Experience Replay)\n\n**思想:** 不再按顺序使用实时产生的经验来训练网络，而是将经验存储起来，然后随机采样进行训练。\n\n**机制:**\n\n*   维护一个**回放缓冲区 (Replay Buffer / Memory)** D，用于存储大量的历史转移 (transitions): (S_t, A_t, R_{t+1}, S_{t+1}, done_flag)。`done_flag` 标记 S_{t+1} 是否是终止状态。\n*   在每个时间步 t，智能体执行动作 A_t，观察到 R_{t+1}, S_{t+1} 后，将这个转移 (S_t, A_t, R_{t+1}, S_{t+1}, done) 存入缓冲区 D。如果缓冲区满了，通常会移除最旧的经验。\n*   在**训练**时，不是使用刚刚产生的那个转移，而是从缓冲区 D 中**随机采样**一个**小批量 (mini-batch)** 的转移 (S_j, A_j, R_{j+1}, S_{j+1}, done_j)。\n*   使用这个 mini-batch 来计算损失并更新网络参数 **w**。\n\n**优点:**\n\n*   **打破数据相关性:** 随机采样打破了原始经验序列的时间相关性，使得样本更接近独立同分布，提高了训练的稳定性和效率。\n*   **提高数据利用率:** 一个经验转移可能被多次采样用于训练，使得智能体能够从过去的经验中反复学习，提高了样本效率。\n\n![Experience Replay](https://spinningup.openai.com/en/latest/_images/experience_replay.png)\n*(图片来源: OpenAI Spinning Up)*\n\n## 2. 目标网络 (Target Network)\n\n**思想:** 使用一个**独立的、更新较慢**的网络来计算 TD 目标值，从而稳定目标。\n\n**机制:**\n\n*   除了主要的 Q 网络 Q̂(s, a; **w**) (也称为 **Online Network**)，再创建一个结构完全相同但参数不同的**目标网络 (Target Network)** Q̂(s, a; **w⁻**)。\n*   在计算 TD 目标时，使用**目标网络**的参数 **w⁻**:\n    *   Target = R + γ max_{a'} Q̂(S', a'; **w⁻**) (如果 S' 非终止)\n    *   Target = R (如果 S' 终止)\n*   **在线网络 Q̂(s, a; w)** 的参数 **w** 在每个训练步（或每几个训练步）通过梯度下降进行更新。\n*   **目标网络 Q̂(s, a; w⁻)** 的参数 **w⁻** **不**通过梯度下降更新，而是**定期**从在线网络复制参数：**w⁻ ← w** (例如，每隔 C 步，C 通常是一个较大的数，如 1000 或 10000)。或者使用**软更新 (Soft Update)**：**w⁻ ← τw + (1-τ)w⁻**，其中 τ 是一个很小的数 (e.g., 0.005)，使得目标网络参数缓慢地跟踪在线网络参数。\n\n**优点:**\n\n*   **稳定 TD 目标:** 目标网络参数 **w⁻** 在一段时间内保持固定，使得 TD 目标值相对稳定，减少了 Q 值更新的震荡，提高了训练稳定性。在线网络 **w** 的更新不再直接影响当前计算的目标值。\n\n# DQN 算法流程 (结合 Experience Replay 和 Target Network)\n\n```\nInitialize:\n  Replay buffer D with capacity N\n  Online Q-network Q̂(s, a; w) with random weights w\n  Target Q-network Q̂(s, a; w⁻) with weights w⁻ = w\n  α ← learning rate\n  γ ← discount factor\n  ε ← initial exploration rate\n  C ← target network update frequency (for hard update) or τ (for soft update)\n\nLoop for each episode:\n  Initialize S (first state)\n  Loop for each step t = 1, T:\n    # 1. Choose action using behavior policy (ε-greedy on online network)\n    With probability ε select random action A_t\n    Otherwise select A_t = argmax_a Q̂(S_t, a; w)\n\n    # 2. Execute action, observe reward R_{t+1} and next state S_{t+1}\n    Execute A_t, observe R_{t+1}, S_{t+1}, done_flag\n\n    # 3. Store transition in replay buffer D\n    Store (S_t, A_t, R_{t+1}, S_{t+1}, done_flag) in D\n\n    # 4. Sample mini-batch from D (if buffer size > learning_starts)\n    If size of D > learning_starts:\n      Sample random mini-batch of transitions (S_j, A_j, R_{j+1}, S_{j+1}, done_j) from D\n\n      # 5. Calculate TD targets using target network\n      Targets = []\n      for j in mini-batch:\n        If done_j:\n          Target_j = R_{j+1}\n        Else:\n          # Use target network w⁻ to get max Q value for next state\n          # Original DQN: Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻)\n          # Double DQN variation (often used in practice):\n          #   a_max = argmax_{a'} Q̂(S_{j+1}, a'; w) # Action selected by online network\n          #   Q_next_target = Q̂(S_{j+1}, a_max; w⁻) # Value evaluated by target network\n          Q_next_target = max_{a'} Q̂(S_{j+1}, a'; w⁻) # Using original DQN target for simplicity here\n          Target_j = R_{j+1} + γ * Q_next_target\n        Targets.append(Target_j)\n\n      # 6. Perform gradient descent step on online network\n      # Get Q values for the actions taken (A_j) from the online network\n      Q_online_values = Q̂(S_j, A_j; w) for j in mini-batch\n      # Calculate loss: e.g., MSE Loss = (1/batch_size) * Σ_j (Targets[j] - Q_online_values[j])²\n      # Update online network weights w using gradient descent: w ← w - α * ∇ Loss(w)\n\n      # 7. Update target network (periodically or softly)\n      # Hard update:\n      # If t % C == 0:\n      #   w⁻ ← w\n      # Soft update (more common in SB3):\n      # w⁻ ← τ*w + (1-τ)*w⁻\n\n    S_t ← S_{t+1} # Move to next state\n\n    If done_flag, break inner loop (episode ends)\n\n  # (Optional) Decay ε\n```\n*(注：步骤 5 中提到了 Double DQN 的变体，这是对原始 DQN 的一个常用改进，用于缓解 Q 值过高估计的问题。原始 DQN 直接使用 `max_{a'} Q̂(S_{j+1}, a'; w⁻)`。SB3 的实现可能包含这类改进。为简化起见，伪代码中仍展示原始 DQN 的目标计算方式。)*\n\n# Lab 6: 使用 Stable Baselines3 运行 DQN 解决 CartPole\n\n## 目标\n\n1.  熟悉使用 Stable Baselines3 (SB3) 库的基本流程：创建环境、定义模型、训练、保存、评估。\n2.  使用 SB3 提供的 DQN 实现来解决 CartPole-v1 问题。\n3.  学习如何设置 DQN 的关键超参数。\n4.  学习如何监控训练过程（观察奖励曲线）。\n5.  评估训练好的模型性能。\n\n## Stable Baselines3 DQN 超参数简介\n\n我们在上周的 SB3 示例代码中看到了一些 DQN 的超参数，这里再解释一下关键的几个：\n\n*   `policy=\"MlpPolicy\"`: 指定使用多层感知机 (MLP) 作为 Q 网络。对于图像输入，可以使用 \"CnnPolicy\"。\n*   `env`: 传入的 Gym/Gymnasium 环境实例（或向量化环境）。\n*   `learning_rate`: 梯度下降的学习率 α。\n*   `buffer_size`: 经验回放缓冲区 D 的大小 N。\n*   `learning_starts`: 收集多少步经验后才开始训练网络（填充缓冲区）。\n*   `batch_size`: 每次从缓冲区采样多少经验进行训练。\n*   `tau`: 软更新目标网络的系数 (SB3 DQN 默认使用软更新，`tau=1.0` 相当于硬更新)。\n*   `gamma`: 折扣因子 γ。\n*   `train_freq`: 每收集多少步经验执行一次训练更新。可以是一个整数（步数），也可以是一个元组 `(frequency, unit)`，如 `(1, \"episode\")` 表示每回合结束时训练一次。\n*   `gradient_steps`: 每次训练更新执行多少次梯度下降步骤。\n*   `target_update_interval`: （硬更新时）每隔多少步将在线网络权重复制到目标网络。SB3 DQN 默认使用软更新（通过 `tau` 控制），这个参数可能不直接使用，但理解其概念很重要。\n*   `exploration_fraction`: 总训练步数中，用于将探索率 ε 从初始值衰减到最终值所占的比例。\n*   `exploration_initial_eps`: 初始探索率 ε (通常为 1.0)。\n*   `exploration_final_eps`: 最终探索率 ε (例如 0.05 或 0.1)。\n*   `verbose`: 控制打印信息的详细程度 (0: 不打印, 1: 打印训练信息, 2: 更详细)。\n\n## 示例代码 (SB3 DQN on CartPole)\n\n```python\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport time\nimport os\n\n# 创建日志目录\nlog_dir = \"/tmp/gym/\"\nos.makedirs(log_dir, exist_ok=True)\n\n\n# 1. 创建环境 (使用向量化环境加速)\n# 使用 Monitor wrapper 来记录训练过程中的回合奖励等信息\nfrom stable_baselines3.common.monitor import Monitor\nvec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n# Monitor wrapper 通常在 make_vec_env 内部自动添加，或者可以手动添加\n# vec_env = Monitor(vec_env, log_dir) # Monitor 通常用于单个环境，VecEnv有自己的日志记录\n\n# 2. 定义 DQN 模型 (可以调整超参数进行实验)\nmodel = DQN(\"MlpPolicy\", vec_env, verbose=1,\n            learning_rate=1e-4,       # 学习率\n            buffer_size=100000,       # Replay buffer 大小\n            learning_starts=5000,     # 多少步后开始学习\n            batch_size=32,            # Mini-batch 大小\n            tau=1.0,                  # Target network 更新系数 (1.0 for hard update)\n            gamma=0.99,               # 折扣因子\n            train_freq=4,             # 每 4 步训练一次\n            gradient_steps=1,         # 每次训练执行 1 次梯度更新\n            target_update_interval=10000, # Target network 更新频率 (硬更新)\n            exploration_fraction=0.1, # 10% 的步数用于探索率衰减\n            exploration_initial_eps=1.0,# 初始探索率\n            exploration_final_eps=0.05, # 最终探索率\n            optimize_memory_usage=False, # 在内存足够时设为 False 可能更快\n            tensorboard_log=log_dir   # 指定 TensorBoard 日志目录\n           )\n\n# 3. 训练模型\nprint(\"Starting training...\")\nstart_time = time.time()\n# 训练更长时间以看到效果\n# log_interval 控制打印到控制台的频率，TensorBoard 日志默认会记录\nmodel.learn(total_timesteps=100000, log_interval=100)\nend_time = time.time()\nprint(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n\n# 4. 保存模型\nmodel_path = os.path.join(log_dir, \"dqn_cartpole_sb3\")\nmodel.save(model_path)\nprint(f\"Model saved to {model_path}.zip\")\n\n# 5. 评估训练好的模型\nprint(\"Evaluating trained model...\")\n# 创建一个单独的评估环境\neval_env = gym.make(\"CartPole-v1\")\n# n_eval_episodes: 评估多少个回合\n# deterministic=True: 使用贪心策略进行评估\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20, deterministic=True)\nprint(f\"Evaluation results: Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")\n\n# 6. (可选) 加载模型并可视化\n# del model # 删除现有模型\n# loaded_model = DQN.load(model_path)\n# print(\"Model loaded.\")\n\n# # 可视化一个回合\n# vis_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n# obs, info = vis_env.reset()\n# terminated = False\n# truncated = False\n# total_reward_vis = 0\n# while not (terminated or truncated):\n#     action, _states = loaded_model.predict(obs, deterministic=True)\n#     obs, reward, terminated, truncated, info = vis_env.step(action)\n#     total_reward_vis += reward\n#     vis_env.render()\n#     # time.sleep(0.01) # Slow down rendering\n# print(f\"Visualization finished. Total reward: {total_reward_vis}\")\n# vis_env.close()\n\n\nvec_env.close()\neval_env.close()\n\n# 提示：可以通过 tensorboard --logdir /tmp/gym/ 查看训练曲线\nprint(f\"To view training logs, run: tensorboard --logdir {log_dir}\")\n```\n\n## 任务与思考\n\n1.  **运行代码:** 确保你的环境安装了 `stable-baselines3[extra]`, `pytorch`, `tensorboard`。运行提供的 DQN 代码。观察训练过程中的输出信息。\n2.  **监控训练 (TensorBoard):** 在代码运行时或运行后，在终端中执行 `tensorboard --logdir /tmp/gym/` (或你指定的 `log_dir`)，然后在浏览器中打开显示的地址 (通常是 `http://localhost:6006/`)。查看 `rollout/ep_rew_mean` (平均回合奖励) 曲线。它是否随着训练步数的增加而提高？\n3.  **评估结果:** 查看 `evaluate_policy` 输出的平均奖励和标准差。CartPole-v1 的目标通常是达到平均奖励接近 500 (v1 版本的回合最大步数是 500)。你的模型达到了吗？\n4.  **超参数实验:**\n    *   尝试**改变学习率** (`learning_rate`，例如增大 10 倍或减小 10 倍）。重新训练并观察 TensorBoard 中的曲线以及最终评估结果。\n    *   尝试**改变经验回放缓冲区大小** (`buffer_size`，例如增大或减小）。对结果有什么影响？\n    *   尝试**改变探索参数** (`exploration_fraction`, `exploration_final_eps`）。例如，让探索持续更长时间或最终探索率更高/更低。对学习过程和最终性能有何影响？\n    *   (可选) 尝试**改变网络更新频率** (`train_freq`, `target_update_interval` 或 `tau`)。\n5.  **分析与讨论:**\n    *   解释经验回放和目标网络在 DQN 训练中的作用，它们如何提高稳定性？\n    *   讨论你观察到的不同超参数对训练结果的影响。为什么某些超参数设置效果更好/更差？\n    *   与表格型方法相比，DQN (使用 SB3) 在解决 CartPole 问题上表现如何？为什么函数逼近在这里是必要的？\n\n## 提交要求\n\n*   提交你运行和修改后的 SB3 DQN 代码。\n*   提交训练过程的 TensorBoard 截图（显示 `rollout/ep_rew_mean` 曲线）。\n*   提交不同超参数设置下的评估结果 (`evaluate_policy` 的输出）。\n*   提交一份简短的分析报告，讨论：\n    *   经验回放和目标网络的作用。\n    *   你观察到的超参数（至少包括学习率和探索参数）对训练的影响。\n    *   DQN 在 CartPole 上的表现以及函数逼近的必要性。\n\n---\n\n**下周预告:** 策略梯度方法 (Policy Gradient Methods)。我们将学习一类不同的 RL 算法，它们直接学习策略函数 π(a|s, θ) 而不是价值函数。"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles/custom.css"],"toc":true,"number-sections":false,"include-in-header":[{"text":"<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\" integrity=\"sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\">\n"}],"output-file":"week10_lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","callout-appearance":"none","title":"Week 10: 深度 Q 网络 (DQN)"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}