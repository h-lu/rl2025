---
title: "Week 6 - 学生练习"
subtitle: "同策略控制 - SARSA"
format:
  html:
    toc: true
    toc-location: left
    code-fold: show
---

# 练习目标

*   理解广义策略迭代 (GPI) 的评估-改进循环。
*   区分同策略 (On-Policy) 和异策略 (Off-Policy) 学习。
*   掌握 SARSA 算法的更新规则和同策略特性。
*   理解 ε-greedy 策略及其在控制算法中的作用。
*   通过分析 Lab 4 代码和结果，加深对 SARSA 算法和参数影响的理解。

# 练习内容

## 练习 1: GPI 与 On/Off-Policy 理解

1.  **GPI 循环:** 广义策略迭代 (GPI) 包含哪两个交替进行的过程？这两个过程的目标分别是什么？
2.  **为何学 Q:** 在无模型强化学习**控制**问题中，为什么我们通常选择学习动作值函数 Q(s, a) 而不是状态值函数 V(s)？
3.  **On-Policy vs. Off-Policy:**
    *   请用你自己的话解释同策略 (On-Policy) 学习和异策略 (Off-Policy) 学习的主要区别。
    *   SARSA 属于哪一类？为什么？

## 练习 2: SARSA 更新计算

假设我们正在使用 SARSA 算法。当前状态 S，根据 ε-greedy 策略选择了动作 A。执行动作 A 后，获得奖励 R = -1，并转移到下一个状态 S'。在状态 S'，再次根据 ε-greedy 策略选择了下一个动作 A'。

当前的 Q 值估计如下：
*   Q(S, A) = 5.0
*   Q(S', A') = 6.0

假设学习率 α = 0.1，折扣因子 γ = 0.9。

请根据 SARSA 更新规则计算更新后的 Q(S, A) 的值：
Q(S, A) ← Q(S, A) + α [R + γ Q(S', A') - Q(S, A)] = ?

（请写出计算步骤和最终结果）

## 练习 3: ε-Greedy 策略

假设在一个状态 s，有 3 个可选动作 a₁, a₂, a₃。当前的 Q 值估计为：
*   Q(s, a₁) = 10
*   Q(s, a₂) = 8
*   Q(s, a₃) = 8

如果使用 ε-greedy 策略，且探索率 ε = 0.1：

1.  选择动作 a₁ (当前最优动作) 的概率是多少？
2.  选择动作 a₂ 的概率是多少？（注意：a₂ 和 a₃ 的 Q 值相同）
3.  选择动作 a₃ 的概率是多少？
4.  如果 ε = 0，选择每个动作的概率分别是多少？这对应什么策略？
5.  如果 ε = 1，选择每个动作的概率分别是多少？这对应什么策略？

## 练习 4: SARSA Lab 代码与结果分析 (基于 Lab 4)

本练习基于讲义/Lab 4 中提供的 CliffWalking 环境和 SARSA 实现代码。

1.  **代码理解:**
    *   请指出 SARSA 代码中，选择**当前**动作 A (`action`) 和选择**下一个**动作 A' (`next_action`) 的代码行。它们都使用了哪个策略函数？
    *   请指出计算 SARSA 的 TD 目标 (`td_target`) 的代码行。它依赖于哪个动作，A 还是 A'？
2.  **ε 的影响分析:**
    *   回顾你在 Lab 4 中尝试不同 ε 值（或预期）的结果。当 ε 值**很小**（接近 0）时，SARSA 的学习过程（奖励曲线）和最终策略可能有什么特点？为什么？
    *   当 ε 值**很大**（接近 1）时，SARSA 的学习过程和最终策略又可能有什么特点？为什么？
3.  **路径选择思考:** 为什么 SARSA 在 CliffWalking 环境中倾向于学习一条远离悬崖的“安全”路径，而不是理论上最短的路径？请结合 SARSA 的更新规则和 ε-greedy 策略进行解释。

# 提交要求

*   请将你的答案整理成一个文档（如 Word, PDF, 或 Markdown 文件）。
*   对于练习 1, 3, 4，请清晰地回答问题并阐述理由。
*   对于练习 2，请写出计算步骤和结果。
*   文件命名格式：`姓名_学号_Week6_Exercise.xxx`。
*   通过教学平台提交。