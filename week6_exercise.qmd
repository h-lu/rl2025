---
title: "Week 6 - 学生练习"
subtitle: "同策略控制 - SARSA"
format:
  html:
    toc: true
    toc-location: left
    code-fold: show
---

# 练习目标

*   理解广义策略迭代 (GPI) 的评估-改进循环。
*   区分同策略 (On-Policy) 和异策略 (Off-Policy) 学习。
*   掌握 SARSA 算法的更新规则和同策略特性。
*   理解 $\epsilon$-greedy 策略及其在控制算法中的作用。
*   通过分析 Lab 4 代码和结果，加深对 SARSA 算法和参数影响的理解。
*   理解学习率、折扣因子和探索率衰减在 SARSA 中的作用。

# 练习内容

## 练习 1: GPI 与 On/Off-Policy 理解

1.  **GPI 循环:** 广义策略迭代 (GPI) 包含哪两个交替进行的过程？这两个过程的目标分别是什么？
2.  **为何学 Q:** 在无模型强化学习**控制**问题中，为什么我们通常选择学习动作值函数 $Q(s, a)$ 而不是状态值函数 $V(s)$？
3.  **On-Policy vs. Off-Policy:**
    *   请用你自己的话解释同策略 (On-Policy) 学习和异策略 (Off-Policy) 学习的主要区别。
    *   SARSA 属于哪一类？
4.  **SARSA 的同策略性:** 结合 SARSA 的更新规则 $Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S', A') - Q(S, A)]$，解释为什么选择下一个动作 $A'$ 的方式使得 SARSA 成为一种**同策略**算法？

## 练习 2: SARSA 更新计算

假设我们正在使用 SARSA 算法。当前状态 $S$，根据 $\epsilon$-greedy 策略选择了动作 $A$。执行动作 $A$ 后，获得奖励 $R = -1$，并转移到下一个状态 $S'$。在状态 $S'$，再次根据 $\epsilon$-greedy 策略选择了下一个动作 $A'$。

当前的 $Q$ 值估计如下：

*   $Q(S, A) = 5.0$
*   $Q(S', A') = 6.0$

假设学习率 $\alpha = 0.1$，折扣因子 $\gamma = 0.9$。

1.  请根据 SARSA 更新规则计算更新后的 $Q(S, A)$ 的值：
    $$
    Q(S, A) \leftarrow Q(S, A) + \alpha [R + \gamma Q(S', A') - Q(S, A)] = ?
    $$
2.  **终止状态处理:** 如果在上述场景中，$S'$ 是一个**终止状态** (例如，到达了目标或者掉下了悬崖)，那么 SARSA 更新规则中的 TD 目标 $R + \gamma Q(S', A')$ 应该如何计算？更新后的 $Q(S, A)$ 又是多少？（假设到达终止状态的奖励 R 仍然是 -1）

## 练习 3: $\epsilon$-Greedy 策略

假设在一个状态 $s$，有 3 个可选动作 $a_1, a_2, a_3$。当前的 $Q$ 值估计为：

*   $Q(s, a_1) = 10$
*   $Q(s, a_2) = 8$
*   $Q(s, a_3) = 8$

如果使用 $\epsilon$-greedy 策略，且探索率 $\epsilon = 0.1$：

1.  选择动作 $a_1$ (当前最优动作) 的概率是多少？
2.  选择动作 $a_2$ 的概率是多少？（注意：$a_2$ 和 $a_3$ 的 $Q$ 值相同）
3.  选择动作 $a_3$ 的概率是多少？
4.  如果 $\epsilon = 0$，选择每个动作的概率分别是多少？这对应什么策略？
5.  如果 $\epsilon = 1$，选择每个动作的概率分别是多少？这对应什么策略？

## 练习 4: SARSA Lab 代码与结果分析 (基于 Lab 4)

本练习基于讲义/Lab 4 中提供的 CliffWalking 环境和 SARSA 实现代码。

1.  **代码理解:**
    *   请指出 SARSA 代码中，选择**当前**动作 A (`action`) 和选择**下一个**动作 A' (`next_action`) 的代码行。它们都使用了哪个策略函数？
    *   请指出计算 SARSA 的 TD 目标 (`td_target`) 的代码行。它依赖于哪个动作，A 还是 A'？(这与练习 1.4 相关)
2.  **ε 的影响分析:**
    *   回顾你在 Lab 4 中尝试不同 ε 值（或预期）的结果。当 ε 值**很小**（接近 0）时，SARSA 的学习过程（奖励曲线）和最终策略可能有什么特点？为什么？
    *   当 ε 值**很大**（接近 1）时，SARSA 的学习过程和最终策略又可能有什么特点？为什么？
3.  **路径选择思考:** 为什么 SARSA 在 CliffWalking 环境中倾向于学习一条远离悬崖的"安全"路径，而不是理论上最短的路径？请结合 SARSA 的更新规则和 ε-greedy 策略进行解释。

## 练习 5: 参数理解

1.  **折扣因子 $\gamma$ 的影响:** 在 CliffWalking 环境中，折扣因子 $\gamma$ 控制了未来奖励相对于即时奖励的重要性。
    *   如果我们将 $\gamma$ 设置得**非常接近 0** (例如 0.1)，SARSA 学到的策略可能会更关注什么？其路径选择可能会有什么特点？
    *   如果将 $\gamma$ 设置得**非常接近 1** (例如 0.999)，SARSA 学到的策略又会更关注什么？这与 $\gamma$ 接近 0 的情况相比，路径选择可能会有什么不同？请解释原因。
2.  **学习率 $\alpha$ 的作用:** 简要说明学习率 $\alpha$ 在 SARSA 更新规则 $Q(S, A) \leftarrow Q(S, A) + \alpha [\text{TD 目标} - Q(S, A)]$ 中的作用。
    *   过大的 $\alpha$ (例如接近 1) 可能导致什么问题？
    *   过小的 $\alpha$ (例如接近 0) 可能导致什么问题？

## 练习 6: Epsilon 衰减 ($\epsilon$-Decay)

在 Lab 4 的 SARSA 代码示例中，探索率 $\epsilon$ 是一个固定的值。但在实践中，我们通常希望 $\epsilon$ 能够随着训练的进行而逐渐减小。

1.  **为何需要 $\epsilon$-衰减?** 为什么在 SARSA 的训练过程中，逐渐减小探索率 $\epsilon$ 通常是一个好主意？这样做有什么好处（或者说，保持固定的 $\epsilon$ 可能有什么坏处）？
2.  **如何实现 $\epsilon$-衰减?** 请描述至少一种实现 $\epsilon$ 衰减的方法（例如，线性衰减或指数衰减）。只需要描述其更新逻辑即可，不需要编写完整的 Python 代码。（提示：衰减通常在每个回合结束时进行）
