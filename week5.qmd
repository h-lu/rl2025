---
title: "第五周：从Q-Learning到深度Q网络"
---

::: {.callout-tip appearance="simple"}
## 本周学习目标

1.  **回顾与反思：** 通过项目一的实践，理解表格型Q-Learning在实际应用中的优势与局限。
2.  **认识局限：** 明确Q-Learning在处理复杂问题（大规模/连续状态空间）时面临的挑战。
3.  **搭建桥梁：** 了解神经网络作为函数近似器的基本原理及其在强化学习中的潜力。
4.  **掌握核心：** 理解深度Q网络 (DQN) 的基本思想，特别是两大关键创新：经验回放 (Experience Replay) 和目标网络 (Target Network) 的作用与原理。
5.  **初步实践：** 了解DQN的基本算法流程和架构。
:::

## 项目一回顾与Q-Learning的局限

在完成并展示了第一个项目后，我们对Q-Learning有了更深入的实践体会。它在解决状态和动作空间有限的问题上表现出色。但同时，我们也可能遇到了它的瓶颈。

**思考一下：** 在你的项目一中，如果状态空间变得极其庞大（例如，从简单的网格世界变成高分辨率的图像输入），或者状态变成连续的（例如，机器人的关节角度），你认为基于Q表格的方法会遇到什么问题？

### Q-Learning面临的主要挑战

1.  **状态空间爆炸 (State Space Explosion):**
    *   对于具有大量状态的环境，维护一个巨大的Q表格变得不现实。想象一下，即使是一个简单的84x84像素的黑白图像，状态数量也超过 \(2^{84 \times 84}\)！彩色图像或更复杂的输入会让这个数字变得更加天文。
    *   现实世界中的许多问题都涉及高维或连续的状态空间，表格方法难以适用。

2.  **缺乏泛化能力 (Lack of Generalization):**
    *   表格型Q-Learning为每一个状态独立学习Q值，无法利用状态之间的相似性。
    *   它无法将在某个状态学到的经验泛化到"相似但未见过"的状态。
    *   这意味着智能体需要访问并学习几乎所有可能的状态，导致样本效率低下。

为了克服这些限制，我们需要引入更强大的工具——**深度学习**。

---

## 神经网络：强化学习的新引擎

神经网络（特别是深度神经网络）以其强大的**函数近似 (Function Approximation)** 能力，为解决Q-Learning的局限性提供了可能。

### 为什么是神经网络？

*   **自动特征提取:** 神经网络可以从原始输入（如图像像素、传感器读数）中自动学习有用的特征表示，无需人工设计。
*   **泛化能力:** 通过在不同状态间共享参数（权重），神经网络可以将学到的知识泛化到相似的、未曾明确访问过的状态。
*   **处理高维输入:** 神经网络天然适合处理高维数据，如图像、文本等。

### 在强化学习中的应用

我们可以用一个神经网络来近似Q函数，记作 \(Q(s, a; \theta)\)，其中 \(\theta\) 代表网络的所有可学习参数（权重和偏置）。

*   **输入:** 状态 \(s\) (例如，游戏画面的像素矩阵，或状态特征向量)。
*   **输出:** 对于给定的状态 \(s\)，网络输出所有可能动作 \(a\) 的Q值估计。
*   **学习:** 通过优化算法（如随机梯度下降及其变种）调整参数 \(\theta\)，使得网络的输出 \(Q(s, a; \theta)\) 尽可能接近目标Q值。

这种用神经网络来近似Q函数的方法，正是**深度Q网络 (Deep Q-Network, DQN)** 的核心思想。

---

## 深度Q网络 (DQN) 原理详解

DQN由DeepMind提出，首次成功地将深度学习与强化学习结合，并在Atari游戏上达到了超越人类水平的表现。

### 基本思想

用一个深度神经网络（通常是卷积神经网络CNN处理图像输入，或全连接网络MLP处理向量输入）来代替巨大的Q表格，即 \(Q(s, a) \approx Q(s, a; \theta)\)。

### 关键挑战与创新

直接用神经网络替代Q表进行训练并不稳定。主要因为：
1.  **样本相关性:** 强化学习中连续采集的样本 (s, a, r, s') 之间高度相关，违反了许多机器学习算法的独立同分布假设，导致训练不稳定。
2.  **目标值不稳定:** Q值的目标 \(y = r + \gamma \max_{a'} Q(s', a'; \theta)\) 依赖于正在更新的网络参数 \(\theta\)。目标值随着网络的更新而不断变化，就像追逐一个移动的目标，容易导致训练发散。

DQN引入了两大关键技术来解决这些问题：

1.  **经验回放 (Experience Replay):**
    *   **是什么？** 创建一个存储经验的缓冲区（通常是一个固定大小的队列），将智能体与环境交互产生的经验元组 \((s_t, a_t, r_{t+1}, s_{t+1}, \text{done})\) 存储起来。
    *   **怎么用？** 在训练时，不直接使用刚产生的经验，而是从缓冲区中**随机采样**一个小批量 (mini-batch) 的经验来训练Q网络。
    *   **为什么有效？**
        *   **打破相关性:** 随机采样打乱了经验的时间顺序，降低了样本间的相关性。
        *   **提高数据效率:** 每个经验可能被多次采样和学习，充分利用了来之不易的交互数据。
        *   **平滑学习:** 通过批次学习，梯度的更新更加平稳。

2.  **目标网络 (Target Network):**
    *   **是什么？** 使用两个结构相同但参数不同的神经网络：
        *   **Q网络 (或称主网络):** 参数为 \(\theta\)，这个网络在训练过程中**频繁更新**，并且用于**实际的动作选择**。
        *   **目标网络:** 参数为 \(\theta^-\)，这个网络的参数**定期**从Q网络复制而来（例如，每 C 步），在计算目标Q值时**保持固定**。
    *   **怎么用？** 在计算TD目标 \(y\) 时，使用**目标网络**来估计下一状态的最大Q值：
        *   如果 \(s'\) 是终止状态: \(y = r\)
        *   否则: \(y = r + \gamma \max_{a'} Q(s', a'; \theta^-)\) （注意这里用的是 \(\theta^-\)）
    *   **为什么有效？** 通过固定目标网络一段时间，使得TD目标 \(y\) 相对稳定，降低了Q网络更新目标与自身变化之间的耦合，从而提高了训练的稳定性。

### DQN 算法流程概览

```python
# 初始化
创建 Q网络 Q(s, a; θ) 和 目标网络 Q'(s, a; θ⁻)
将 Q网络 的参数复制给 目标网络 (θ⁻ ← θ)
初始化 经验回放缓冲区 D，容量为 N

for 每个回合 (episode):
    获取初始状态 s
    
    for 每个时间步 t:
        # 1. 与环境交互
        根据当前 Q网络 Q(s, ·; θ) 和 ε-greedy 策略选择动作 a
        执行动作 a，观察奖励 r 和下一个状态 s'
        将经验转移 (s, a, r, s', done) 存储到缓冲区 D
        
        # 2. 从缓冲区采样
        如果 D 中经验数量足够:
            从 D 中随机采样一个 小批量 (mini-batch) 的经验 (sⱼ, aⱼ, rⱼ₊₁, sⱼ₊₁, doneⱼ)
            
            # 3. 计算目标 Q 值
            计算目标 yⱼ:
                if doneⱼ:
                    yⱼ = rⱼ₊₁
                else:
                    # 使用目标网络计算下一状态的最大Q值
                    yⱼ = rⱼ₊₁ + γ * maxₐ' Q'(sⱼ₊₁, a'; θ⁻)
            
            # 4. 计算损失并更新 Q 网络
            计算损失 L = ( Q(sⱼ, aⱼ; θ) - yⱼ )²  (通常是均方误差或 Huber loss)
            执行一步梯度下降，更新 Q网络 的参数 θ
            
        # 5. 更新状态
        s = s'
        
        # 6. 定期更新目标网络
        每隔 C 步:
            将 Q网络 的参数复制给 目标网络 (θ⁻ ← θ)
        
        if done: # 如果回合结束
            break
```

### DQN 架构图

为了更直观地理解各组件如何协同工作，请看下面的架构图：

```{=html}
<svg width="800" height="600" xmlns="http://www.w3.org/2000/svg">
    <!-- 背景 -->
    <rect width="800" height="600" fill="#f9f9f9"/>
    
    <!-- 标题 -->
    <text x="400" y="40" font-family="Arial, sans-serif" font-size="24" text-anchor="middle" font-weight="bold">深度Q网络(DQN)架构图</text>
    
    <!-- 智能体容器 -->
    <rect x="60" y="70" width="680" height="480" rx="10" ry="10" fill="#eef6ff" stroke="#0066cc" stroke-width="2" stroke-dasharray="5,5"/>
    <text x="110" y="95" font-family="Arial, sans-serif" font-size="18" fill="#0066cc">DQN智能体</text>
    
    <!-- 环境 -->
    <rect x="100" y="130" width="160" height="80" rx="8" ry="8" fill="#d1e7dd" stroke="#198754" stroke-width="2"/>
    <text x="180" y="175" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#198754">环境</text>
    <text x="180" y="195" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#198754">(CartPole/Atari)</text>
    
    <!-- Q网络 -->
    <rect x="320" y="130" width="160" height="80" rx="8" ry="8" fill="#cfe2ff" stroke="#0d6efd" stroke-width="2"/>
    <text x="400" y="170" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#0d6efd">Q网络</text>
    <text x="400" y="190" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#0d6efd">(频繁更新)</text>
    
    <!-- 目标网络 -->
    <rect x="540" y="130" width="160" height="80" rx="8" ry="8" fill="#cfe2ff" stroke="#0d6efd" stroke-width="2"/>
    <text x="620" y="170" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#0d6efd">目标网络</text>
    <text x="620" y="190" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#0d6efd">(定期更新)</text>
    
    <!-- 经验回放缓冲区 -->
    <rect x="320" y="360" width="160" height="80" rx="8" ry="8" fill="#fff3cd" stroke="#ffc107" stroke-width="2"/>
    <text x="400" y="400" font-family="Arial, sans-serif" font-size="16" text-anchor="middle" fill="#774600">经验回放缓冲区</text>
    <text x="400" y="420" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#774600">(存储和采样经验)</text>
    
    <!-- Q网络到环境的动作选择 -->
    <path d="M320,170 L260,170" stroke="#0d6efd" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
    <text x="290" y="160" font-family="Arial, sans-serif" font-size="12" text-anchor="middle">选择动作 (a)</text>
    
    <!-- 环境到经验缓冲区的连接 -->
    <path d="M180,210 L180,300 L320,400" stroke="#198754" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
    <text x="170" y="270" font-family="Arial, sans-serif" font-size="12" text-anchor="middle">存储经验</text>
    <text x="170" y="285" font-family="Arial, sans-serif" font-size="12" text-anchor="middle">(s,a,r,s',done)</text>
    
    <!-- 经验缓冲区到Q网络的连接 -->
    <path d="M400,360 L400,210" stroke="#ffc107" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
    <text x="430" y="290" font-family="Arial, sans-serif" font-size="12" text-anchor="start">采样批次 (sⱼ,aⱼ,rⱼ₊₁,sⱼ₊₁)</text>
    
    <!-- 目标网络用于计算目标值 -->
    <path d="M540,190 C 500,230 460,230 480,190" stroke="#0d6efd" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
    <text x="510" y="230" font-family="Arial, sans-serif" font-size="12" text-anchor="middle">计算目标值 yⱼ</text>
    <text x="510" y="245" font-family="Arial, sans-serif" font-size="10" text-anchor="middle">(使用 Q')</text>
    
    <!-- Q网络到目标网络的更新 -->
    <path d="M480,150 L540,150" stroke="#ff5722" stroke-width="2" fill="none" marker-end="url(#arrowhead)" stroke-dasharray="5,3"/>
    <text x="510" y="140" font-family="Arial, sans-serif" font-size="12" text-anchor="middle" fill="#ff5722">定期复制参数 (θ⁻ ← θ)</text>
    
    <!-- 损失计算和反向传播 -->
    <rect x="320" y="250" width="160" height="60" rx="8" ry="8" fill="#f8d7da" stroke="#dc3545" stroke-width="2"/>
    <text x="400" y="280" font-family="Arial, sans-serif" font-size="14" text-anchor="middle" fill="#dc3545">计算损失</text>
    <text x="400" y="295" font-family="Arial, sans-serif" font-size="11" text-anchor="middle" fill="#dc3545">(Q(sⱼ,aⱼ;θ) - yⱼ)²</text>
    
    <!-- 箭头定义 -->
    <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
            <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
        </marker>
    </defs>
    
    <!-- 图例 -->
    <rect x="100" y="500" width="600" height="30" rx="5" ry="5" fill="#f8f9fa" stroke="#6c757d" stroke-width="1"/>
    <text x="400" y="520" font-family="Arial, sans-serif" font-size="14" text-anchor="middle">
        <tspan fill="#0d6efd">■ 神经网络</tspan>
        <tspan dx="20" fill="#198754">■ 环境</tspan>
        <tspan dx="20" fill="#774600">■ 经验回放</tspan>
        <tspan dx="20" fill="#dc3545">■ 损失计算</tspan>
    </text>
</svg>
```

这张图清晰地展示了DQN的各个部分如何交互：Q网络与环境互动并产生经验，经验存入缓冲区，然后从缓冲区采样用于训练Q网络，而目标网络则提供稳定的目标值计算，并定期从Q网络同步参数。

---

## 深入理解两大关键技术

让我们更详细地探讨经验回放和目标网络。

::: {.panel-tabset}
#### 经验回放 (Experience Replay)

*   **核心作用:**
    *   **打破样本相关性:** 强化学习的序贯决策过程导致相邻样本高度相关。随机采样打破了这种时间上的依赖，使得样本更接近独立同分布，有利于神经网络训练。
    *   **提高数据效率:** 环境交互通常是昂贵的。经验回放使得智能体可以反复利用过去的经验进行学习，极大提高了样本的利用率。
    *   **稳定训练过程:** 对整个缓冲区进行采样，可以平滑数据分布的变化，避免训练过程因遇到连续的相似或极端样本而剧烈波动。

*   **实现要点:**
    ```python
    from collections import deque
    import random
    import numpy as np
    
    class ReplayBuffer:
        def __init__(self, capacity):
            # 使用双端队列存储经验，自动丢弃旧经验
            self.buffer = deque(maxlen=capacity)
        
        def add(self, state, action, reward, next_state, done):
            # 将经验元组添加到缓冲区
            experience = (state, action, reward, next_state, done)
            self.buffer.append(experience)
        
        def sample(self, batch_size):
            # 从缓冲区中随机采样指定数量的经验
            # 确保缓冲区内有足够经验才进行采样
            if len(self.buffer) < batch_size:
                return None # 或者抛出异常，或返回空值
            
            batch = random.sample(self.buffer, batch_size)
            
            # 将批次中的经验解压，方便后续处理
            # 注意：这里假设状态等是numpy数组或其他可以直接堆叠的格式
            states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))
            return states, actions, rewards, next_states, dones
        
        def __len__(self):
            # 返回当前缓冲区中的经验数量
            return len(self.buffer)
    ```

*   **思考与讨论:** 缓冲区大小 (capacity) 如何选择？太大或太小会有什么影响？除了随机采样，还有哪些更高级的采样策略？（提示：第七周会讲到优先经验回放）

#### 目标网络 (Target Network)

*   **核心原理:** 解决"移动目标"问题。
    *   在标准的Q-Learning更新 \(Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]\) 中，如果用神经网络近似Q函数，目标值 \(r + \gamma \max_{a'} Q(s',a'; \theta)\) 会随着 \(\theta\) 的更新而不断变化。
    *   这意味着我们用来计算更新目标的"尺子"本身也在不停地变动，导致学习过程不稳定，容易震荡或发散。
    *   目标网络通过"冻结"目标Q值计算中使用的网络参数 \(\theta^-\) 一段时间，提供了一个相对稳定的学习目标。

*   **关键实现:**
    ```python
    import tensorflow as tf # 或 torch
    
    # 假设 model_builder() 函数能创建我们的Q网络模型
    q_network = model_builder()
    target_network = model_builder()
    
    # 初始化：确保目标网络权重与Q网络一致
    target_network.set_weights(q_network.get_weights())
    
    # 在学习/训练函数中:
    def train_step(batch):
        states, actions, rewards, next_states, dones = batch
        
        # 1. 使用 *目标网络* 预测下一状态的Q值
        next_q_values_target = target_network.predict(next_states) 
        # 2. 计算下一状态的最大Q值
        max_next_q = np.max(next_q_values_target, axis=1)
        # 3. 计算 TD 目标 y
        target_y = rewards + gamma * max_next_q * (1 - dones) 
        
        # 4. 使用 *Q网络* 计算损失并执行梯度更新
        with tf.GradientTape() as tape:
            # 获取当前状态下，实际采取动作的Q值预测
            q_values = q_network(states)
            action_indices = tf.stack([tf.range(len(actions)), actions], axis=1)
            current_q_pred = tf.gather_nd(q_values, action_indices)
            
            # 计算损失 (例如 MSE)
            loss = tf.keras.losses.mean_squared_error(target_y, current_q_pred)
            
        # 计算梯度并更新 Q 网络
        grads = tape.gradient(loss, q_network.trainable_variables)
        optimizer.apply_gradients(zip(grads, q_network.trainable_variables))
        
        return loss
    
    # 在主训练循环中定期更新目标网络:
    if global_step % target_update_frequency == 0:
        print(f"Step {global_step}: Updating target network.")
        target_network.set_weights(q_network.get_weights())
    ```

*   **更新策略:**
    *   **硬更新 (Hard Update):** 每隔 C 步，将Q网络的参数 \(\theta\) 完全复制给目标网络 \(\theta^-\) (如上代码所示)。这是原始DQN论文中使用的方法。
    *   **软更新 (Soft Update):** 每一步（或每隔几步）都进行微小的更新： \(\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-\) 。其中 \(\tau\) 是一个很小的数（例如 0.001 或 0.01）。这种方式更新更平滑，在一些算法（如DDPG）中更常用。
    *   **思考:** 硬更新的更新频率 C 如何选择？软更新的 \(\tau\) 如何选择？它们各有什么优缺点？
:::

---

## DQN 与 Q-Learning 对比总结

| 特性             | 表格型 Q-Learning                     | 深度 Q 网络 (DQN)                         |
| :--------------- | :------------------------------------ | :---------------------------------------- |
| **价值表示**     | Q表格 (显式存储每个状态-动作对的值) | 神经网络 (隐式表示，通过参数近似)         |
| **状态空间**     | 仅限小规模、离散状态                | 可处理大规模、高维甚至连续状态 (通过离散化或特定架构) |
| **价值更新**     | 单个样本更新 Q(s,a)                 | 从经验回放中采样**批量**样本进行更新        |
| **参数数量**     | 状态数 × 动作数                       | 神经网络的权重和偏置数量 (通常远小于前者) |
| **稳定性技巧**   | 无特定技巧                            | **经验回放** + **目标网络**               |
| **泛化能力**     | 基本无                                | **强** (神经网络的核心优势)               |
| **样本效率**     | 低 (需要大量探索覆盖状态空间)         | **相对较高** (经验回放提高了利用率)       |
| **计算复杂度**   | 更新快，查询快 (表格查找)             | 训练慢 (神经网络前向/反向传播)，查询相对快 |
| **适用场景**     | 状态/动作空间小的简单问题             | 状态空间复杂、需要泛化能力的问题 (如游戏) |

---

## 展望：DQN的扩展与改进

DQN是一个里程碑，但并非终点。后续研究提出了许多重要的改进，进一步提升了其性能和稳定性，我们将在后续课程中深入探讨其中一些：

*   **双重DQN (Double DQN):** 旨在解决标准DQN中的Q值过高估计问题。
*   **决斗网络架构 (Dueling Network Architecture):** 将Q值分解为状态价值和动作优势，可能在某些环境中学习更高效。
*   **优先经验回放 (Prioritized Experience Replay):** 更智能地从经验回放缓冲区中采样，优先学习"有价值"的经验。
*   **分布式DQN (Distributional DQN):** 不再只学习Q值的期望，而是学习其完整的值分布。
*   **噪声网络 (Noisy Nets):** 一种更有效的探索策略。
*   **彩虹DQN (Rainbow):** 集成了上述多种改进技术，达到了非常高的性能。

---

::: {.callout-tip}
## 课后活动与实践

1.  **文献阅读:** 尝试阅读DQN的原始论文 ([Playing Atari with Deep Reinforcement Learning by Mnih et al., 2013/2015](https://arxiv.org/abs/1312.5602) 或 Nature 版本 [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236))，重点理解引言、方法（特别是经验回放和目标网络部分）和实验设置。
2.  **代码探索:** 下载并运行课程提供的DQN代码示例。仔细阅读代码，理解`ReplayBuffer`类和目标网络的创建、使用及更新逻辑。
3.  **动手实践:** 选择一个你熟悉且相对简单的环境（例如Gym库中的`CartPole-v1`），尝试实现一个基础版本的DQN算法。
    *   **网络结构:** 可以从简单的全连接网络开始（例如，输入层 -> 隐藏层(ReLU) -> 输出层(线性)）。
    *   **核心组件:** 确保正确实现了经验回放缓冲区和目标网络的定期更新。
    *   **超参数:** 尝试调整一些基本超参数，如学习率、ε-greedy的衰减率、缓冲区大小、目标网络更新频率、批次大小等，观察它们对训练过程和最终性能的影响。
4.  **思考:** 在CartPole环境中，状态是4维的连续向量。DQN如何处理这种连续状态输入？与处理离散状态的Q-Learning相比有何不同？
:::

::: {.callout-warning}
## 下周预习重点

本周我们学习了DQN的基础，下周将深入探讨其重要的变体和调优技巧。请提前思考和了解：

1.  **Double DQN** 和 **Dueling DQN** 分别试图解决标准DQN的什么问题？它们的核心思想是什么？
2.  除了ε-greedy，还有哪些**探索策略**？它们相比ε-greedy有何优劣？（例如，了解一下Noisy Networks或Boltzmann探索）
3.  训练深度强化学习模型时，有哪些常见的**超参数**需要调整？（例如，学习率、批次大小、缓冲区大小、目标网络更新频率等）它们各自的作用是什么？
4.  如何有效地**监控和调试**深度强化学习的训练过程？（了解一下TensorBoard等工具的作用）
::: 