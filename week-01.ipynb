{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **第一周：强化学习入门**\n",
    "\n",
    "**课程目标:**\n",
    "\n",
    "*   了解强化学习 (Reinforcement Learning, RL) 的基本概念和核心思想。\n",
    "*   区分强化学习与监督学习、无监督学习。\n",
    "*   了解强化学习在商业领域的应用案例。\n",
    "*   初步体验强化学习环境，为后续实践打下基础。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第一次课：什么是强化学习？**\n",
    "\n",
    "**内容概要:**\n",
    "\n",
    "*   强化学习的核心概念：智能体 (Agent)、环境 (Environment)、动作 (Action)、状态 (State)、奖励 (Reward)。\n",
    "*   强化学习与监督学习、无监督学习的对比与区分。\n",
    "*   强化学习在商业领域的应用案例。\n",
    "*   介绍个人项目一：平衡杆游戏 (CartPole)。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 强化学习的核心概念**\n",
    "\n",
    "强化学习是一种**试错学习 (Trial-and-Error Learning)** 范式，智能体 (Agent) 在与环境 (Environment) 的交互中学习，通过接收奖励 (Reward) 或惩罚来优化其行为策略，目标是最大化累积奖励。\n",
    "\n",
    "**核心要素:**\n",
    "\n",
    "*   **智能体 (Agent):**  学习和做出决策的实体，例如：一个机器人、一个游戏角色、一个推荐系统。\n",
    "*   **环境 (Environment):**  智能体所处的世界，它可以是真实的物理世界，也可以是虚拟的模拟环境。环境接收智能体的动作，并返回新的状态和奖励。\n",
    "*   **动作 (Action):**  智能体在环境中可以执行的操作，动作会影响环境的状态。\n",
    "*   **状态 (State):**  环境在某一时刻的描述，智能体根据状态来决定下一步的动作。\n",
    "*   **奖励 (Reward):**  环境对智能体动作的反馈信号，可以是正面的 (奖励) 或负面的 (惩罚)。奖励用于指导智能体学习，使其朝着目标方向行动。\n",
    "\n",
    "**用生活中的例子来理解:**\n",
    "\n",
    "*   **例子 1: 训练小狗**\n",
    "    *   **智能体:** 小狗\n",
    "    *   **环境:** 你的家和周围环境\n",
    "    *   **动作:** 坐下、站立、叫、跑等等\n",
    "    *   **状态:** 你发出的指令、周围的环境（是否有食物、玩具等）\n",
    "    *   **奖励:** 零食、夸奖 (当你希望小狗做出特定动作时给予奖励)\n",
    "\n",
    "*   **例子 2: 玩电子游戏**\n",
    "    *   **智能体:** 游戏玩家 (你)\n",
    "    *   **环境:** 游戏世界\n",
    "    *   **动作:**  控制游戏角色移动、跳跃、攻击等等\n",
    "    *   **状态:** 游戏屏幕上的画面 (敌人位置、自身血量、道具等)\n",
    "    *   **奖励:**  得分、通关、击败敌人 (游戏设计者设定的奖励机制)\n",
    "\n",
    "**强化学习的目标:**  智能体学习一个**策略 (Policy)**，策略定义了在每个状态下应该采取的最佳动作，以最大化长期累积奖励。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 强化学习 vs. 监督学习 vs. 无监督学习**\n",
    "\n",
    "为了更好理解强化学习，我们将其与大家可能更熟悉的监督学习和无监督学习进行对比：\n",
    "\n",
    "| 特征           | 监督学习 (Supervised Learning) | 无监督学习 (Unsupervised Learning) | 强化学习 (Reinforcement Learning) |\n",
    "| -------------- | ----------------------------- | ------------------------------- | --------------------------------- |\n",
    "| **学习方式**     | 从**带标签**的数据中学习       | 从**无标签**的数据中学习         | 从与**环境交互**的经验中学习       |\n",
    "| **数据类型**     | 输入-输出对 (特征-标签)         | 无标签数据 (只有特征)             | 状态、动作、奖励序列               |\n",
    "| **反馈信号**     | 标签 (正确答案)                 | 无明确反馈                      | 奖励 (Reward)                     |\n",
    "| **学习目标**     | 预测新数据的标签                | 发现数据中的模式和结构            | 学习策略，最大化累积奖励            |\n",
    "| **应用场景**     | 图像分类、文本分类、回归预测     | 聚类、降维、异常检测              | 游戏AI、机器人控制、推荐系统、自动驾驶 |\n",
    "\n",
    "**关键区别:**\n",
    "\n",
    "*   **监督学习:**  像老师教学生一样，告诉模型什么是对的，什么是错的 (通过标签)。\n",
    "*   **无监督学习:**  像自己探索一样，让模型自己去发现数据中的规律和结构。\n",
    "*   **强化学习:**  更像是一种**自主学习**，智能体在环境中不断尝试，根据环境的反馈 (奖励) 来调整自己的行为，最终学会完成任务。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 强化学习在商业领域的应用案例**\n",
    "\n",
    "强化学习在商业领域有着广泛的应用潜力，以下是一些常见的案例：\n",
    "\n",
    "*   **推荐系统:**  根据用户的历史行为和偏好，智能推荐商品或内容，最大化用户点击率、购买率或用户满意度。\n",
    "*   **动态定价:**  根据市场需求、竞争对手价格、库存水平等因素，实时调整商品价格，最大化收益。\n",
    "*   **库存管理:**  预测未来需求，优化库存水平，降低库存成本，提高库存周转率。\n",
    "*   **广告投放:**  智能选择广告投放平台、受众和出价策略，最大化广告点击率或转化率。\n",
    "*   **金融交易:**  开发交易策略，进行股票、期货等金融资产的自动交易，追求更高的投资回报。\n",
    "*   **智能客服:**  通过对话交互，自动解答用户问题，提供客户服务，降低人工客服成本。\n",
    "*   **供应链优化:**  优化物流、生产、仓储等环节，提高供应链效率，降低成本。\n",
    "\n",
    "**案例：动态定价**\n",
    "\n",
    "假设一家电商平台要为某商品制定动态定价策略。\n",
    "\n",
    "*   **智能体:**  动态定价系统\n",
    "*   **环境:**  市场环境 (用户需求、竞争对手价格等)\n",
    "*   **状态:**  当前的市场需求、商品库存、竞争对手价格等\n",
    "*   **动作:**  调整商品价格 (提高或降低)\n",
    "*   **奖励:**  销售额、利润\n",
    "\n",
    "动态定价系统通过不断尝试不同的价格，并观察市场反馈 (销售额、利润)，学习到最优的定价策略，从而在不同的市场条件下都能获得更高的收益。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4 个人项目一：平衡杆游戏 (CartPole)**\n",
    "\n",
    "本课程的第一个个人项目是 **平衡杆游戏 (CartPole)**。这是一个经典的强化学习入门环境，非常适合初学者理解强化学习的基本概念和算法。\n",
    "\n",
    "**环境搭建:**\n",
    "\n",
    "要运行 CartPole 环境，我们需要安装 **Gymnasium** 库。Gymnasium 是一个用于开发和比较强化学习算法的 Python 库，它提供了各种各样的标准化环境，包括 CartPole。\n",
    "\n",
    "**安装 Gymnasium 库:**\n",
    "\n",
    "在终端或命令提示符中运行以下命令即可安装 Gymnasium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你希望可视化 CartPole 游戏画面，还需要安装 `pygame` 依赖:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*   **状态空间 (Observation Space):**  环境可能呈现给智能体的所有状态的集合。例如，CartPole 的状态空间包括小车的位置、速度、杆子的角度和角速度。\n",
    "*   **动作空间 (Action Space):**  智能体可以在环境中执行的所有有效动作的集合。例如，CartPole 的动作空间包括向左推动小车和向右推动小车。\n",
    "*   **奖励函数 (Reward Function):**  环境根据智能体的动作给出的反馈信号，用于指导智能体学习。例如，CartPole 中，每保持杆子不倒一步，智能体获得 +1 奖励。\n",
    "\n",
    "Gymnasium 提供了一系列标准化的环境接口，使得我们可以方便地创建、交互和评估强化学习环境。\n",
    "\n",
    "**CartPole 环境详解:**\n",
    "\n",
    "**项目描述:**\n",
    "\n",
    "CartPole 环境中，有一个小车 (cart) 在水平轨道上左右移动，车上通过一个关节连接着一根杆子 (pole)。智能体的目标是通过控制小车的左右移动，使得杆子尽可能保持竖直不倒。\n",
    "\n",
    "**环境要素:**\n",
    "\n",
    "*   **状态 (State/Observation):**  环境的状态由以下 4 个变量组成 (Gymnasium 返回的 `observation`):\n",
    "    *   **小车的位置 (Cart Position):**  小车中心点的水平位置，取值范围为实数。\n",
    "    *   **小车的速度 (Cart Velocity):**  小车的水平速度，取值范围为实数。\n",
    "    *   **杆子的角度 (Pole Angle):**  杆子与竖直方向的夹角，弧度制，正值表示顺时针偏离，负值表示逆时针偏离，取值范围约为 ±0.2095 rad (±12度)。\n",
    "    *   **杆子的角速度 (Pole Angular Velocity):**  杆子的角速度，弧度/秒，取值范围为实数。\n",
    "\n",
    "*   **动作 (Action):**  智能体可以采取两个离散动作 (Gymnasium 动作空间 `Discrete(2)`):\n",
    "    *   **0: 向左推动小车 (Push cart to the left)**\n",
    "    *   **1: 向右推动小车 (Push cart to the right)**\n",
    "\n",
    "*   **奖励 (Reward):**  每当杆子保持在竖直方向 (角度在 ±12度范围内) 一步，智能体获得 +1 的奖励。如果杆子倾倒超过 ±12度，或者小车超出轨道边界 (±2.4)，则回合结束 (terminated)。\n",
    "\n",
    "*   **回合结束 (Termination & Truncation):**\n",
    "    *   **Terminated:**  当杆子倾倒角度超过 ±12度(约 ±0.2095 弧度)，或小车位置超出轨道边界 (±2.4) 时，`terminated` 标志为 `True`，回合结束。\n",
    "    *   **Truncated:**  CartPole-v1 环境有最大步数限制，当回合达到 500 步时，`truncated` 标志为 `True`，回合也会结束。这表示回合被截断 (truncated)，通常用于防止回合过长。\n",
    "\n",
    "**项目目标:**\n",
    "\n",
    "使用强化学习算法 (例如：后续课程中会学习的 Q-Learning, DQN 等)，训练一个智能体，使其能够尽可能长时间地保持杆子平衡，获得尽可能高的累积奖励。\n",
    "\n",
    "**为什么选择 CartPole?**\n",
    "\n",
    "*   **简单易懂:**  环境规则和状态空间都比较简单，容易理解。\n",
    "*   **经典案例:**  是强化学习的经典入门案例，有大量的学习资源和代码示例。\n",
    "*   **可视化:**  可以直观地看到智能体与环境的交互过程，方便调试和理解。\n",
    "*   **为后续学习打基础:**  CartPole 环境为后续学习更复杂的强化学习算法和环境打下基础。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**随机控制系统示例:**\n",
    "\n",
    "下面是一个使用随机动作控制 CartPole 环境的 Python 代码示例。这个示例中，智能体在每一步都随机选择一个动作 (向左或向右推)，我们运行 100 个时间步，并渲染环境动画。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 创建 CartPole 环境\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\") #  添加 render_mode=\"human\" 以启用可视化窗口\n",
    "\n",
    "# 重置环境，获取初始状态\n",
    "observation, info = env.reset()\n",
    "\n",
    "# 循环交互\n",
    "for _ in range(100):\n",
    "    # 随机选择一个动作 (0 或 1)\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # 执行动作，获取下一个状态、奖励、是否结束、其他信息\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # 渲染环境 (可视化) -  render_mode=\"human\" 后，step() 会自动渲染，此处render()可以省略\n",
    "    # env.render()\n",
    "\n",
    "    # 打印当前状态参数\n",
    "    print(f\"状态: 位置={observation[0]:.2f}, 速度={observation[1]:.2f}, 角度={observation[2]:.2f}, 角速度={observation[3]:.2f}\")\n",
    "\n",
    "    # 如果游戏结束，则重置环境\n",
    "    if terminated or truncated:\n",
    "        print(\"游戏结束，最终状态:\") # 打印游戏结束信息\n",
    "        print(f\"最终状态: 位置={observation[0]:.2f}, 速度={observation[1]:.2f}, 角度={observation[2]:.2f}, 角速度={observation[3]:.2f}\") # 打印最终状态值\n",
    "        observation, info = env.reset()\n",
    "        break\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**代码解释:**\n",
    "\n",
    "*   `import gymnasium as gym`: 导入 Gymnasium 库，我们需要使用它来创建和操作 CartPole 环境。\n",
    "*   `env = gym.make(\"CartPole-v1\", render_mode=\"human\")`:  创建 CartPole-v1 环境。`gym.make()` 是 Gymnasium 库中用于创建环境的函数。`\"CartPole-v1\"` 是 CartPole 环境的 ID，用于指定要创建的环境类型。 `render_mode=\"human\"`  参数告诉 Gymnasium 以人类可观看的模式渲染环境，即会弹出一个窗口显示 CartPole 动画。\n",
    "*   `observation, info = env.reset()`:  重置环境到初始状态。`env.reset()` 函数返回初始的 `observation` (状态)，以及一些额外信息 `info` (通常在初学阶段可以忽略)。 每次开始新回合时，都需要调用 `reset()` 函数。\n",
    "*   `for _ in range(100):`:  循环 100 次，模拟 100 个时间步的环境交互。\n",
    "*   `action = env.action_space.sample()`:  从动作空间中随机采样一个动作。`env.action_space` 表示环境的动作空间，对于 CartPole 环境，动作空间是 `Discrete(2)`，即离散空间，包含两个动作 {0, 1}。 `sample()` 方法用于从动作空间中随机选择一个动作。\n",
    "*   `observation, reward, terminated, truncated, info = env.step(action)`:  执行动作 `action`，与环境交互一步。`env.step(action)` 函数是 Gymnasium 环境的核心函数，它接收一个动作作为输入，并返回四个值：\n",
    "    *   `observation`:  执行动作后，环境返回的新的状态。\n",
    "    *   `reward`:  执行动作后，环境给出的奖励值。对于 CartPole 环境，每保持杆子不倒立一步，奖励为 +1。\n",
    "    *   `terminated`:  布尔值，表示当前回合是否结束。`terminated` 为 `True` 通常表示杆子倒了或者小车超出边界。\n",
    "    *   `truncated`: 布尔值，表示当前回合是否由于达到最大步数限制而提前结束 (被截断)。 CartPole-v1 环境的最大步数为 500 步。\n",
    "    *   `info`:  包含一些额外调试信息的字典。\n",
    "*   `# env.render()`:  在 `render_mode=\"human\"` 模式下，`env.step()` 会自动渲染环境，因此 `env.render()`  这行代码可以省略。 如果你创建环境时没有指定 `render_mode=\"human\"`，则需要调用 `env.render()`  来手动渲染。\n",
    "*   `if terminated or truncated: observation, info = env.reset()`:  判断回合是否结束 (`terminated` 或 `truncated` 为 `True`)。如果结束，则调用 `env.reset()` 重置环境，开始下一个新的回合。\n",
    "*   `env.close()`:  关闭环境，释放资源。 尤其是在启用了可视化窗口后，需要调用 `env.close()` 来关闭窗口。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**运行代码:**  请确保你已经安装了 Gymnasium 和 pygame 库 (`pip install gymnasium pygame`)。运行上述 Python 代码，你将看到 CartPole 游戏的动画界面，小车会随机左右移动，杆子也会随之摆动。\n",
    "\n",
    "**实验:**  尝试运行代码，观察随机控制下 CartPole 的表现。你会发现杆子很快就会倒下，因为随机动作通常无法有效地保持平衡。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**总结:**\n",
    "\n",
    "本周我们学习了强化学习的基本概念、与监督学习和无监督学习的区别，以及强化学习在商业领域的应用案例。同时，我们还初步了解了强化学习的理论框架 MDP，以及策略、价值函数、探索-利用平衡等重要概念。最后，我们搭建了 CartPole 环境，并进行了简单的交互体验。\n",
    "\n",
    "**下周预告:**  我们将开始学习第一个强化学习算法：**Q-Learning**，并使用 Q-Learning 算法来训练智能体玩 CartPole 游戏。\n",
    "\n",
    "---\n",
    "\n",
    "**练习:**\n",
    "\n",
    "1.  **理解 CartPole 环境:**  查阅 Gymnasium 官方文档或其他资源，更深入地了解 CartPole 环境的状态空间、动作空间和奖励函数。\n",
    "2.  **尝试编写简单策略:**  修改示例代码，尝试编写一个简单的策略来控制 CartPole，例如：\n",
    "    *   **固定策略:**  始终向左推 (动作 0) 或始终向右推 (动作 1)，观察杆子的表现。\n",
    "    *   **基于状态的简单策略:**  例如，当杆子向左倾斜时向右推，当杆子向右倾斜时向左推。 (提示：你可以使用 `observation` 中的杆子角度信息 `pole_angle = observation[2]` 来判断杆子的倾斜方向。)\n",
    "3.  **思考题:**  随机策略在 CartPole 环境中表现不佳的原因是什么？你认为一个好的 CartPole 控制策略应该具备哪些特点？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
