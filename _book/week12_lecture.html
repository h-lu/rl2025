<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Week 12: Actor-Critic 方法 – 智能计算</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./week13_lecture.html" rel="next">
<link href="./week11_lecture.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles/custom.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week1_lecture.html">讲义</a></li><li class="breadcrumb-item"><a href="./week12_lecture.html"><span class="chapter-title">Week 12: Actor-Critic 方法</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">智能计算</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程介绍</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">讲义</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week1_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1: 商业决策智能化与强化学习概览</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week2_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2: 序贯决策建模 - 马尔可夫决策过程 (MDP)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week3_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3: 最优决策与 Bellman 最优方程</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week4_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4: 蒙特卡洛方法 - 从完整经验中学习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week5_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5: 时序差分学习 - 从不完整经验中学习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week6_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6: 同策略控制 - SARSA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week7_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7: 异策略控制 - Q-Learning (重点)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./sarsa_vs_qlearning_comparison.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">SARSA 与 Q-Learning 算法详解与探索策略对比</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week8_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8: Q-Learning 应用讨论与中期回顾</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week9_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9: 函数逼近入门</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week10_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10: 深度 Q 网络 (DQN)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week10_lab_dqn_experiment.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10 综合实验：DQN 强化学习综合实践</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week11_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 11: 策略梯度方法 (Policy Gradient Methods)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week12_lecture.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 12: Actor-Critic 方法</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week13_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 13: 商业案例分析 1 - 动态定价与资源优化</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week14_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 14: 商业案例分析 2 - 个性化推荐与营销</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week15_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 15: 实践挑战、伦理规范与项目指导</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week16_lecture.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 16: 课程总结与未来展望</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">练习</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week1_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week2_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week3_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week4_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week5_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week6_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week7_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week8_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week9_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week10_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week11_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 11 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week12_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 12 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week13_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 13 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week14_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 14 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week15_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 15 - 学生练习</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week16_exercise.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 16 - 学生练习</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#回顾策略梯度-policy-gradient-与-reinforce" id="toc-回顾策略梯度-policy-gradient-与-reinforce" class="nav-link active" data-scroll-target="#回顾策略梯度-policy-gradient-与-reinforce">回顾：策略梯度 (Policy Gradient) 与 REINFORCE</a></li>
  <li><a href="#actor-critic-框架" id="toc-actor-critic-框架" class="nav-link" data-scroll-target="#actor-critic-框架">Actor-Critic 框架</a></li>
  <li><a href="#a2c-a3c-算法概念" id="toc-a2c-a3c-算法概念" class="nav-link" data-scroll-target="#a2c-a3c-算法概念">A2C / A3C 算法概念</a></li>
  <li><a href="#lab-7-使用-stable-baselines3-运行-a2c" id="toc-lab-7-使用-stable-baselines3-运行-a2c" class="nav-link" data-scroll-target="#lab-7-使用-stable-baselines3-运行-a2c">Lab 7: 使用 Stable Baselines3 运行 A2C</a>
  <ul class="collapse">
  <li><a href="#目标" id="toc-目标" class="nav-link" data-scroll-target="#目标">目标</a></li>
  <li><a href="#环境选择" id="toc-环境选择" class="nav-link" data-scroll-target="#环境选择">环境选择</a></li>
  <li><a href="#示例代码-sb3-a2c-on-cartpole" id="toc-示例代码-sb3-a2c-on-cartpole" class="nav-link" data-scroll-target="#示例代码-sb3-a2c-on-cartpole">示例代码 (SB3 A2C on CartPole)</a>
  <ul class="collapse">
  <li><a href="#网络结构详解-默认-mlppolicy" id="toc-网络结构详解-默认-mlppolicy" class="nav-link" data-scroll-target="#网络结构详解-默认-mlppolicy">网络结构详解 (默认 <code>MlpPolicy</code>)</a></li>
  <li><a href="#如何自定义网络结构" id="toc-如何自定义网络结构" class="nav-link" data-scroll-target="#如何自定义网络结构">如何自定义网络结构</a></li>
  </ul></li>
  <li><a href="#任务与思考" id="toc-任务与思考" class="nav-link" data-scroll-target="#任务与思考">任务与思考</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week1_lecture.html">讲义</a></li><li class="breadcrumb-item"><a href="./week12_lecture.html"><span class="chapter-title">Week 12: Actor-Critic 方法</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Week 12: Actor-Critic 方法</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="回顾策略梯度-policy-gradient-与-reinforce" class="level1">
<h1>回顾：策略梯度 (Policy Gradient) 与 REINFORCE</h1>
<p>上周我们学习了策略梯度 (PG) 方法：</p>
<ul>
<li><strong>核心思想:</strong> 直接参数化策略 <span class="math inline">\(\pi(a|s, \theta)\)</span> 并优化参数 <span class="math inline">\(\theta\)</span> 以最大化预期回报 <span class="math inline">\(J(\theta)\)</span>。</li>
<li><strong>优化方式:</strong> 梯度上升 <span class="math inline">\(\theta \leftarrow \theta + \alpha \nabla J(\theta)\)</span>。</li>
<li><strong>策略梯度定理:</strong> <span class="math inline">\(\nabla J(\theta) = E_{\pi_{\theta}} [ \nabla \log \pi(A_t|S_t, \theta) * Q_{\pi}(S_t, A_t) ]\)</span> (或使用 <span class="math inline">\(G_t\)</span>)。</li>
<li><strong>REINFORCE 算法:</strong> 使用蒙特卡洛方法估计 <span class="math inline">\(Q_{\pi}\)</span> (即使用完整回报 <span class="math inline">\(G_t\)</span>)。
<ul>
<li><span class="math inline">\(\nabla J(\theta) \approx E_{\pi_{\theta}} [ \nabla \log \pi(A_t|S_t, \theta) * G_t ]\)</span></li>
<li><strong>缺点:</strong> 高方差，收敛慢，需要完整回合。</li>
</ul></li>
<li><strong>基线 (Baseline):</strong> 为了减小方差，从回报中减去一个与动作无关的基线 <span class="math inline">\(b(S_t)\)</span>。
<ul>
<li><span class="math inline">\(\nabla J(\theta) \approx E_{\pi_{\theta}} [ \nabla \log \pi(A_t|S_t, \theta) * (G_t - b(S_t)) ]\)</span></li>
<li>常用的基线是状态值函数 <span class="math inline">\(V_{\pi}(S_t)\)</span>。</li>
<li><strong>优势函数 (Advantage Function):</strong> <span class="math inline">\(A_{\pi}(S_t, A_t) = Q_{\pi}(S_t, A_t) - V_{\pi}(S_t)\)</span>。</li>
<li>梯度变为：<span class="math inline">\(\nabla J(\theta) = E_{\pi_{\theta}} [ \nabla \log \pi(A_t|S_t, \theta) * A_{\pi}(S_t, A_t) ]\)</span></li>
</ul></li>
</ul>
<p><strong>问题:</strong> 如何在不知道 <span class="math inline">\(Q_{\pi}\)</span> 和 <span class="math inline">\(V_{\pi}\)</span> 的情况下，有效地估计优势函数 <span class="math inline">\(A_{\pi}\)</span> 并进行策略更新？</p>
</section>
<section id="actor-critic-框架" class="level1">
<h1>Actor-Critic 框架</h1>
<p>Actor-Critic (AC) 方法提供了一个优雅的解决方案，它结合了<strong>策略梯度</strong>和<strong>TD学习</strong>的思想。</p>
<p><strong>核心思想:</strong> 维护两个参数化的模型（通常是神经网络）：</p>
<ol type="1">
<li><strong>Actor (行动者):</strong>
<ul>
<li>参数化的<strong>策略</strong> <span class="math inline">\(\pi(a|s, \theta)\)</span>。</li>
<li>负责根据当前状态 <span class="math inline">\(s\)</span> <strong>选择动作</strong> <span class="math inline">\(a\)</span>。</li>
<li>目标是优化参数 <span class="math inline">\(\theta\)</span> 以改进策略。</li>
</ul></li>
<li><strong>Critic (评论家):</strong>
<ul>
<li>参数化的<strong>价值函数</strong>（通常是状态值函数 <span class="math inline">\(V(s, w)\)</span> 或动作值函数 <span class="math inline">\(Q(s, a, w)\)</span>）。</li>
<li>负责<strong>评估</strong> Actor 选择的动作有多好。</li>
<li>目标是学习准确的价值估计，参数为 <span class="math inline">\(w\)</span>。</li>
</ul></li>
</ol>
<p><strong>交互流程:</strong></p>
<ol type="1">
<li>Actor 根据当前状态 <span class="math inline">\(S_t\)</span> 和策略 <span class="math inline">\(\pi(·|·, \theta)\)</span> 选择动作 <span class="math inline">\(A_t\)</span>。</li>
<li>执行动作 <span class="math inline">\(A_t\)</span>，观察到奖励 <span class="math inline">\(R_{t+1}\)</span> 和下一个状态 <span class="math inline">\(S_{t+1}\)</span>。</li>
<li>Critic 利用这个转移 <span class="math inline">\((S_t, A_t, R_{t+1}, S_{t+1})\)</span> 来<strong>评估</strong>动作 <span class="math inline">\(A_t\)</span> 的好坏，并<strong>更新</strong>其价值函数参数 <span class="math inline">\(w\)</span>。</li>
<li>Actor 利用 Critic 的评估信息来<strong>更新</strong>其策略参数 <span class="math inline">\(\theta\)</span>。</li>
</ol>
<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20250224190459513673/Actor-Critic-Method.webp" class="img-fluid" alt="Actor-Critic Architecture"> <em>(图片来源: https://www.geeksforgeeks.org/actor-critic-algorithm-in-reinforcement-learning)</em></p>
<p><strong>Critic 如何帮助 Actor？</strong></p>
<p>Critic 的主要作用是提供一个比蒙特卡洛回报 <span class="math inline">\(G_t\)</span> <strong>方差更低</strong>的信号来指导 Actor 的学习。这通常通过以下方式实现：</p>
<ul>
<li><strong>计算 TD 误差:</strong> 如果 Critic 学习的是状态值函数 <span class="math inline">\(V(s, w)\)</span>，它可以计算 TD 误差：
<ul>
<li><span class="math inline">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}, w) - V(S_t, w)\)</span></li>
<li>这个 TD 误差 <span class="math inline">\(\delta_t\)</span> 可以作为优势函数 <span class="math inline">\(A_{\pi}(S_t, A_t)\)</span> 的一个（有偏但低方差的）<strong>估计</strong>。</li>
</ul></li>
<li><strong>更新 Actor:</strong> Actor 使用这个 TD 误差来更新策略参数 <span class="math inline">\(\theta\)</span>：
<ul>
<li><span class="math inline">\(\theta \leftarrow \theta + \alpha * \nabla \log \pi(A_t|S_t, \theta) * \delta_t\)</span></li>
<li><strong>直观理解:</strong>
<ul>
<li>如果 <span class="math inline">\(\delta_t &gt; 0\)</span> (实际回报 <span class="math inline">\(R_{t+1} + \gamma V(S')\)</span> 比当前预期 <span class="math inline">\(V(S)\)</span> 要好)，说明动作 <span class="math inline">\(A_t\)</span> 是个好动作，增加其概率。</li>
<li>如果 <span class="math inline">\(\delta_t &lt; 0\)</span> (实际回报比预期差)，说明动作 <span class="math inline">\(A_t\)</span> 是个坏动作，减小其概率。</li>
</ul></li>
</ul></li>
<li><strong>更新 Critic:</strong> Critic 也需要学习，通常使用 TD 学习来更新其参数 <span class="math inline">\(w\)</span>，目标是最小化 TD 误差（使其对 <span class="math inline">\(V_{\pi}\)</span> 的估计更准确）：
<ul>
<li><span class="math inline">\(w \leftarrow w + \beta * \delta_t * \nabla V(S_t, w)\)</span> (<span class="math inline">\(\beta\)</span> 是 Critic 的学习率)</li>
</ul></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="为什么 TD 误差是优势函数的估计?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
为什么 TD 误差是优势函数的估计?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>TD 误差 <span class="math inline">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\)</span> 可以作为优势函数 <span class="math inline">\(A_{\pi}(S_t, A_t)\)</span> 的估计，原因如下：</p>
<ol type="1">
<li><strong>数学期望关系</strong>:
<ul>
<li>优势函数定义为 <span class="math inline">\(A_{\pi}(s,a) = Q_{\pi}(s,a) - V_{\pi}(s)\)</span></li>
<li>TD 误差的期望满足: <span class="math inline">\(E_{\pi}[\delta_t | S_t, A_t] = Q_{\pi}(S_t, A_t) - V_{\pi}(S_t) = A_{\pi}(S_t, A_t)\)</span></li>
</ul></li>
<li><strong>直观理解</strong>:
<ul>
<li><span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})\)</span> 是 <span class="math inline">\(Q_{\pi}(S_t, A_t)\)</span> 的采样估计</li>
<li>减去 <span class="math inline">\(V(S_t)\)</span> 后，<span class="math inline">\(\delta_t\)</span> 衡量了”实际获得的回报”与”状态平均价值”的差异</li>
<li>正值表示动作比平均好，负值表示比平均差</li>
</ul></li>
<li><strong>实际优势</strong>:
<ul>
<li>相比蒙特卡洛回报 <span class="math inline">\(G_t\)</span>，TD误差方差更低</li>
<li>不需要等待回合结束，支持在线学习</li>
<li>计算简单，只需一步转移 <span class="math inline">\((S_t,A_t,R_{t+1},S_{t+1})\)</span></li>
</ul></li>
</ol>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Actor-Critic 方法的优势">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Actor-Critic 方法的优势
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>结合策略梯度与TD学习:</strong> 巧妙融合了策略梯度(PG)的直接策略优化能力和TD学习的低方差优势，有效改善了传统REINFORCE算法的高方差问题。</li>
<li><strong>训练更稳定高效:</strong> 相比蒙特卡洛式的REINFORCE算法，通常具有更快的收敛速度和更稳定的训练过程。</li>
</ul>
</div>
</div>
</section>
<section id="a2c-a3c-算法概念" class="level1">
<h1>A2C / A3C 算法概念</h1>
<p><strong>A2C (Advantage Actor-Critic):</strong></p>
<ul>
<li>这是 Actor-Critic 的一个<strong>同步 (Synchronous)</strong>、<strong>确定性 (Deterministic)</strong> 版本，其概念与 A3C (由 Mnih 等人于 2016 年提出) 紧密相连。</li>
<li>通常使用<strong>多个并行的环境</strong>实例来收集经验数据。</li>
<li>智能体在所有环境中执行一步，收集一批 (S, A, R, S’) 数据。</li>
<li>使用这批数据计算 TD 误差 δ 和梯度，然后<strong>一次性</strong>更新 Actor 和 Critic 的参数。</li>
<li>由于是同步更新，实现相对简单。Stable Baselines3 中的 <code>A2C</code> 实现的就是这种思想。</li>
</ul>
<p><strong>A3C (Asynchronous Advantage Actor-Critic):</strong></p>
<ul>
<li>这是 Actor-Critic 的一个<strong>异步 (Asynchronous)</strong> 版本，由 Mnih 等人于 2016 年在论文《Asynchronous Methods for Deep Reinforcement Learning》中首次提出，是早期深度强化学习 (DRL) 的一个重要里程碑式算法。</li>
<li><strong>核心思想:</strong> 创建多个并行的 Actor-Learner 线程，每个线程都有自己的环境副本和模型参数副本。</li>
<li>每个线程独立地与环境交互，计算梯度（Actor 和 Critic 的梯度）。</li>
<li><strong>异步更新:</strong> 各个线程<strong>独立地、异步地</strong>将计算出的梯度应用到<strong>全局共享</strong>的模型参数上。</li>
<li><strong>优点:</strong> 不需要经验回放缓冲区（异步性本身提供了数据去相关性）；通过并行化提高了训练速度。</li>
<li><strong>缺点:</strong> 实现相对复杂；异步更新可能导致某些线程使用过时的参数进行计算。</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="A2C vs. A3C 实践对比">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
A2C vs.&nbsp;A3C 实践对比
</div>
</div>
<div class="callout-body-container callout-body">
<p>研究表明，A2C（同步并行版本）在大多数实际任务中表现优于A3C，主要体现在： 1. <strong>性能更优</strong>：同步更新机制使训练更稳定，通常能获得更高的最终回报 2. <strong>实现简单</strong>：避免了异步更新的复杂性和潜在参数冲突 3. <strong>复现性强</strong>：确定性更新过程确保实验结果可重复 4. <strong>资源利用率高</strong>：能更好地利用现代GPU的并行计算能力</p>
<p>因此主流框架(如Stable Baselines3)优先支持A2C实现，A3C更多具有历史研究价值。</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="其他 Actor-Critic 变体与发展">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
其他 Actor-Critic 变体与发展
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>A2C 和 A3C 是 Actor-Critic 思想的重要实现，奠定了基础。在此之后，研究者们提出了许多更先进和性能更强的 Actor-Critic 变体：</p>
<ul>
<li><strong>DDPG (Deep Deterministic Policy Gradient):</strong> 专门为<strong>连续动作空间</strong>设计。Actor 输出确定的动作（而不是概率分布），Critic 学习 Q(s,a)。结合了 DQN 中经验回放和目标网络等技巧。</li>
<li><strong>TRPO (Trust Region Policy Optimization):</strong> 旨在通过限制每次策略更新的幅度（在“信任区域”内进行优化）来保证策略的单调改进，从而提高训练的稳定性。</li>
<li><strong>PPO (Proximal Policy Optimization):</strong> TRPO 的一种简化版本，通过裁剪目标函数或使用 KL 散度惩罚项来实现类似 TRPO 的稳定更新效果，但实现更简单，计算效率更高。PPO 目前是许多应用中非常流行且效果出色的算法。</li>
<li><strong>SAC (Soft Actor-Critic):</strong> 一种基于最大熵强化学习框架的 Actor-Critic 算法。它在最大化累积回报的同时，也最大化策略的熵，从而鼓励探索，并能学习到更鲁棒的策略。SAC 在许多连续控制任务上取得了顶尖的性能。</li>
<li><strong>TD3 (Twin Delayed Deep Deterministic Policy Gradient):</strong> DDPG 的改进版本，通过使用两个 Critic 网络（取较小值）、延迟策略更新和目标策略平滑等技巧来缓解 Q 值过高估计的问题，从而提升了 DDPG 的性能和稳定性。</li>
</ul>
<p><strong>后续发展:</strong></p>
<p>强化学习领域仍然在飞速发展。虽然 PPO 和 SAC 等算法在很多任务上表现优异，但研究者们仍在不断探索新的架构、优化方法和理论，例如：</p>
<ul>
<li><strong>基于模型的强化学习 (Model-Based RL):</strong> 学习环境模型，然后利用模型进行规划或生成模拟经验。</li>
<li><strong>离线强化学习 (Offline RL):</strong> 从固定的数据集中学习策略，而无需与环境进行新的交互。</li>
<li><strong>多智能体强化学习 (Multi-Agent RL):</strong> 多个智能体在共享环境中学习和交互。</li>
<li><strong>与大型语言模型 (LLM) 的结合:</strong> 探索如何利用 LLM 的知识和推理能力来增强 RL 智能体的学习和决策。</li>
</ul>
<p>因此，虽然 A2C/A3C 是重要的里程碑，但后续的 PPO、SAC 等算法在性能和稳定性上通常有更佳表现，并且整个领域仍在不断涌现更为突破性的思想和算法。</p>
</div>
</div>
</div>
</section>
<section id="lab-7-使用-stable-baselines3-运行-a2c" class="level1">
<h1>Lab 7: 使用 Stable Baselines3 运行 A2C</h1>
<section id="目标" class="level2">
<h2 class="anchored" data-anchor-id="目标">目标</h2>
<ol type="1">
<li>使用 Stable Baselines3 (SB3) 运行 A2C 算法。</li>
<li>在 CartPole (离散动作) 或 Pendulum (连续动作) 环境上进行实验。</li>
<li>对比 A2C 和 DQN (在 CartPole 上) 的训练过程和结果。</li>
<li>理解 Actor-Critic 方法相对于 DQN 的优势（尤其是在处理连续动作空间方面）。</li>
</ol>
</section>
<section id="环境选择" class="level2">
<h2 class="anchored" data-anchor-id="环境选择">环境选择</h2>
<ul>
<li><strong>CartPole-v1:</strong> 离散动作空间。可以与上周的 DQN 进行直接比较。</li>
<li><strong>Pendulum-v1:</strong> <strong>连续动作空间</strong>。
<ul>
<li>目标: 通过施加力矩，将倒立摆摆动到最高点并保持稳定。</li>
<li>状态: [cos(杆角度), sin(杆角度), 杆角速度] (连续)。</li>
<li>动作: 施加的力矩 (连续值，通常在 [-2.0, 2.0] 之间)。</li>
<li>奖励: 与杆子角度和角速度有关，目标是最大化奖励（最小化“成本”）。</li>
<li><strong>注意:</strong> DQN 无法直接处理 Pendulum 的连续动作空间，而 A2C 可以。</li>
</ul></li>
</ul>
</section>
<section id="示例代码-sb3-a2c-on-cartpole" class="level2">
<h2 class="anchored" data-anchor-id="示例代码-sb3-a2c-on-cartpole">示例代码 (SB3 A2C on CartPole)</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3 <span class="im">import</span> A2C</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.env_util <span class="im">import</span> make_vec_env</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.evaluation <span class="im">import</span> evaluate_policy</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 创建日志目录</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">"/tmp/gym_a2c/"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>os.makedirs(log_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 创建环境 (A2C 通常需要向量化环境)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>vec_env <span class="op">=</span> make_vec_env(<span class="st">"CartPole-v1"</span>, n_envs<span class="op">=</span><span class="dv">8</span>) <span class="co"># A2C 通常使用更多并行环境</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 定义 A2C 模型</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># A2C 使用 "MlpPolicy" 或 "CnnPolicy"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 关键超参数:</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># n_steps: 每个环境在更新前运行多少步 (影响 TD 估计的长度)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># vf_coef: 值函数损失的系数 (Critic loss weight)</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># ent_coef: 熵正则化系数 (鼓励探索)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> A2C(<span class="st">"MlpPolicy"</span>, vec_env, verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            gamma<span class="op">=</span><span class="fl">0.99</span>,             <span class="co"># 折扣因子</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            n_steps<span class="op">=</span><span class="dv">5</span>,              <span class="co"># 每个环境更新前运行 5 步</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>            vf_coef<span class="op">=</span><span class="fl">0.5</span>,            <span class="co"># 值函数损失系数</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            ent_coef<span class="op">=</span><span class="fl">0.0</span>,           <span class="co"># 熵正则化系数 (CartPole 通常不需要太多探索)</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            learning_rate<span class="op">=</span><span class="fl">7e-4</span>,     <span class="co"># 学习率 (A2C 通常用稍高一点的学习率)</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            tensorboard_log<span class="op">=</span>log_dir</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>           )</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 训练模型</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting A2C training on CartPole..."</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>model.learn(total_timesteps<span class="op">=</span><span class="dv">100000</span>, log_interval<span class="op">=</span><span class="dv">50</span>) <span class="co"># 训练步数与 DQN 保持一致</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training finished in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 保存模型</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>model_path <span class="op">=</span> os.path.join(log_dir, <span class="st">"a2c_cartpole_sb3"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>model.save(model_path)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model saved to </span><span class="sc">{</span>model_path<span class="sc">}</span><span class="ss">.zip"</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. 评估训练好的模型</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Evaluating trained A2C model..."</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>eval_env <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>mean_reward, std_reward <span class="op">=</span> evaluate_policy(model, eval_env, n_eval_episodes<span class="op">=</span><span class="dv">20</span>, deterministic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Evaluation results (A2C): Mean reward = </span><span class="sc">{</span>mean_reward<span class="sc">:.2f}</span><span class="ss"> +/- </span><span class="sc">{</span>std_reward<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>vec_env.close()</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>eval_env.close()</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"To view training logs, run: tensorboard --logdir </span><span class="sc">{</span>log_dir<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co"># --- (可选) 运行 A2C on Pendulum-v1 ---</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># print("\nStarting A2C training on Pendulum...")</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co"># log_dir_pendulum = "/tmp/gym_a2c_pendulum/"</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># os.makedirs(log_dir_pendulum, exist_ok=True)</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co"># vec_env_pendulum = make_vec_env("Pendulum-v1", n_envs=8)</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co"># model_pendulum = A2C("MlpPolicy", vec_env_pendulum, verbose=1,</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="co">#                      gamma=0.99,</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co">#                      n_steps=5,</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co">#                      vf_coef=0.5,</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co">#                      ent_coef=0.0, # Pendulum 可能需要一点熵正则化</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="co">#                      learning_rate=7e-4,</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">#                      tensorboard_log=log_dir_pendulum</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="co">#                     )</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># start_time = time.time()</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="co"># model_pendulum.learn(total_timesteps=200000, log_interval=50) # Pendulum 可能需要更多步数</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="co"># end_time = time.time()</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"Pendulum training finished in {end_time - start_time:.2f} seconds.")</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co"># model_path_pendulum = os.path.join(log_dir_pendulum, "a2c_pendulum_sb3")</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="co"># model_pendulum.save(model_path_pendulum)</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"Pendulum model saved to {model_path_pendulum}.zip")</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Evaluating trained A2C model on Pendulum...")</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co"># eval_env_pendulum = gym.make("Pendulum-v1")</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co"># mean_reward_p, std_reward_p = evaluate_policy(model_pendulum, eval_env_pendulum, n_eval_episodes=10, deterministic=True)</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"Evaluation results (A2C on Pendulum): Mean reward = {mean_reward_p:.2f} +/- {std_reward_p:.2f}")</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="co"># vec_env_pendulum.close()</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co"># eval_env_pendulum.close()</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"To view Pendulum training logs, run: tensorboard --logdir {log_dir_pendulum}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="网络结构详解-默认-mlppolicy" class="level3">
<h3 class="anchored" data-anchor-id="网络结构详解-默认-mlppolicy">网络结构详解 (默认 <code>MlpPolicy</code>)</h3>
<p>在上面的示例代码中，我们使用了 <code>A2C("MlpPolicy", vec_env, ...)</code>。这里的 <code>"MlpPolicy"</code> 是 Stable Baselines3 (SB3) 提供的一个预定义策略，它为 Actor-Critic 架构构建了基于多层感知机 (Multi-Layer Perceptron, MLP) 的神经网络。虽然代码中没有显式定义每一层的具体参数，但 <code>MlpPolicy</code> 会使用一套标准的默认配置。</p>
<p><strong>通用结构:</strong></p>
<ul>
<li><strong>输入层 (Input Layer):</strong> 接收环境的状态观测值。</li>
<li><strong>隐藏层 (Hidden Layers):</strong> Actor (策略网络) 和 Critic (价值网络) 通常各自拥有独立的隐藏层。对于 <code>MlpPolicy</code>，SB3 的默认配置通常是：
<ul>
<li><strong>网络结构 (net_arch):</strong> <code>[dict(pi=[64, 64], vf=[64, 64])]</code>。这意味着 Actor (pi) 和 Critic (vf) 各自有<strong>两个包含64个神经单元的隐藏层</strong>。</li>
<li><strong>激活函数 (activation_fn):</strong> 默认使用 <strong>Tanh</strong> 作为隐藏层的激活函数。</li>
</ul></li>
<li><strong>输出层 (Output Layer):</strong>
<ul>
<li><strong>Actor 网络:</strong> 输出动作或动作的参数。</li>
<li><strong>Critic 网络:</strong> 输出状态的价值估计。</li>
</ul></li>
</ul>
<p><strong>1. CartPole-v1 环境 (离散动作空间)</strong></p>
<ul>
<li><strong>状态观测 (Input):</strong> 4个连续值 (小车位置、小车速度、杆子角度、杆尖速度)。</li>
<li><strong>Actor 网络 (<span class="math inline">\(\pi(a|s, \theta)\)</span>):</strong>
<ul>
<li>输入层: 4 个单元。</li>
<li>隐藏层 1: 64 个单元 (Tanh 激活)。</li>
<li>隐藏层 2: 64 个单元 (Tanh 激活)。</li>
<li>输出层: 2 个单元 (对应向左和向右两个离散动作)，之后通常连接一个 <strong>Softmax</strong> 激活函数，输出每个动作的选择概率。</li>
</ul></li>
<li><strong>Critic 网络 (<span class="math inline">\(V(s, w)\)</span>):</strong>
<ul>
<li>输入层: 4 个单元。</li>
<li>隐藏层 1: 64 个单元 (Tanh 激活)。</li>
<li>隐藏层 2: 64 个单元 (Tanh 激活)。</li>
<li>输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 <span class="math inline">\(V(s)\)</span>。</li>
</ul></li>
</ul>
<p><strong>2. Pendulum-v1 环境 (连续动作空间)</strong></p>
<ul>
<li><strong>状态观测 (Input):</strong> 3个连续值 (<span class="math inline">\(\cos(\text{杆角度})\)</span>, <span class="math inline">\(\sin(\text{杆角度})\)</span>, 杆角速度)。</li>
<li><strong>Actor 网络 (<span class="math inline">\(\pi(a|s, \theta)\)</span>):</strong>
<ul>
<li>输入层: 3 个单元。</li>
<li>隐藏层 1: 64 个单元 (Tanh 激活)。</li>
<li>隐藏层 2: 64 个单元 (Tanh 激活)。</li>
<li>输出层:
<ul>
<li>对于连续动作空间，Actor 网络通常输出动作分布的参数。SB3 中，对于高斯策略 (Gaussian policy)，网络会输出动作的<strong>均值 (mean)</strong>。Pendulum 环境的动作是1个连续值 (力矩)，所以输出层有1个单元代表该均值 (通常是线性激活后接一个 Tanh 来限制动作范围)。</li>
<li>动作分布的<strong>标准差 (standard deviation)</strong> 也是学习的一部分。在 SB3 的 <code>MlpPolicy</code> 中，标准差（或其对数 <code>log_std</code>）通常是独立于状态的可学习参数，并为每个动作维度学习一个。</li>
</ul></li>
</ul></li>
<li><strong>Critic 网络 (<span class="math inline">\(V(s, w)\)</span>):</strong>
<ul>
<li>输入层: 3 个单元。</li>
<li>隐藏层 1: 64 个单元 (Tanh 激活)。</li>
<li>隐藏层 2: 64 个单元 (Tanh 激活)。</li>
<li>输出层: 1 个单元 (线性激活)，输出当前状态的价值估计 <span class="math inline">\(V(s)\)</span>。</li>
</ul></li>
</ul>
</section>
<section id="如何自定义网络结构" class="level3">
<h3 class="anchored" data-anchor-id="如何自定义网络结构">如何自定义网络结构</h3>
<p>如果你想使用不同于默认设置的网络结构，例如改变隐藏层的数量、每层的神经元数量或激活函数，可以通过在创建 <code>A2C</code> 模型时传递 <code>policy_kwargs</code> 参数来实现。</p>
<p>下面是一个示例，展示如何为 Actor 和 Critic 网络自定义隐藏层和激活函数：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3 <span class="im">import</span> A2C</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.env_util <span class="im">import</span> make_vec_env</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.evaluation <span class="im">import</span> evaluate_policy</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn <span class="co"># 引入 torch.nn 来指定激活函数</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 假设 log_dir 已创建</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">"/tmp/gym_a2c_custom/"</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>os.makedirs(log_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. 创建环境</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>vec_env <span class="op">=</span> make_vec_env(<span class="st">"CartPole-v1"</span>, n_envs<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. 定义自定义网络结构的参数</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># policy_kwargs 接受一个字典</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># net_arch: 定义 Actor (pi) 和 Critic (vf) 的网络层结构</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">#   例如 [128, 128] 表示两个包含128个单元的隐藏层</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># activation_fn: 指定激活函数，例如 nn.ReLU, nn.Tanh, nn.ELU</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>policy_kwargs <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    net_arch<span class="op">=</span><span class="bu">dict</span>(pi<span class="op">=</span>[<span class="dv">128</span>, <span class="dv">64</span>], vf<span class="op">=</span>[<span class="dv">128</span>, <span class="dv">64</span>]), <span class="co"># Actor 和 Critic 各自两层，神经元数分别为 128, 64</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    activation_fn<span class="op">=</span>nn.ReLU                     <span class="co"># 使用 ReLU 作为激活函数</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. 定义 A2C 模型，并传入 policy_kwargs</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>model_custom <span class="op">=</span> A2C(</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MlpPolicy"</span>,</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    vec_env,</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    policy_kwargs<span class="op">=</span>policy_kwargs, <span class="co"># 应用自定义网络结构</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    gamma<span class="op">=</span><span class="fl">0.99</span>,</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    n_steps<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    vf_coef<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    ent_coef<span class="op">=</span><span class="fl">0.0</span>,</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">7e-4</span>,</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    tensorboard_log<span class="op">=</span>log_dir</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. 训练模型</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Starting A2C training on CartPole with custom network..."</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>model_custom.learn(total_timesteps<span class="op">=</span><span class="dv">50000</span>, log_interval<span class="op">=</span><span class="dv">50</span>) <span class="co"># 训练少量步数作为演示</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Custom training finished."</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># (可选) 打印模型结构来查看 Actor 和 Critic 的网络</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="co"># 注意：这会打印出 PyTorch 模块的详细信息</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="co"># print("Actor's network architecture:")</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co"># print(model_custom.policy.mlp_extractor.policy_net)</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="co"># print("\nCritic's network architecture:")</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="co"># print(model_custom.policy.mlp_extractor.value_net)</span></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. 保存和评估 (与之前类似)</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>model_path_custom <span class="op">=</span> os.path.join(log_dir, <span class="st">"a2c_cartpole_custom_sb3"</span>)</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>model_custom.save(model_path_custom)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Custom model saved to </span><span class="sc">{</span>model_path_custom<span class="sc">}</span><span class="ss">.zip"</span>)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>eval_env_custom <span class="op">=</span> gym.make(<span class="st">"CartPole-v1"</span>)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>mean_reward_custom, std_reward_custom <span class="op">=</span> evaluate_policy(model_custom, eval_env_custom, n_eval_episodes<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Evaluation results (Custom A2C): Mean reward = </span><span class="sc">{</span>mean_reward_custom<span class="sc">:.2f}</span><span class="ss"> +/- </span><span class="sc">{</span>std_reward_custom<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>vec_env.close()</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>eval_env_custom.close()</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"To view custom training logs, run: tensorboard --logdir </span><span class="sc">{</span>log_dir<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>解释 <code>policy_kwargs</code>:</strong></p>
<ul>
<li><code>net_arch</code>:
<ul>
<li>你可以为 Actor (<code>pi</code>) 和 Critic (<code>vf</code>) 指定不同的结构。例如 <code>dict(pi=[256, 128], vf=[64, 64])</code>。</li>
<li>如果共享层，可以这样定义：<code>[64, 64, dict(pi=[32], vf=[32])]</code>，表示先有两个共享的64单元层，然后 Actor 和 Critic 各自有一个32单元的输出前一层。但对于 A2C 的 <code>MlpPolicy</code>，通常 <code>pi</code> 和 <code>vf</code> 是独立的路径。</li>
</ul></li>
<li><code>activation_fn</code>: 可以从 <code>torch.nn</code> 模块中选择不同的激活函数，如 <code>nn.ReLU</code>, <code>nn.Tanh</code>, <code>nn.LeakyReLU</code>, <code>nn.ELU</code> 等。</li>
</ul>
<p>通过这种方式，你可以更灵活地调整模型结构以适应不同任务的复杂性。</p>
</section>
</section>
<section id="任务与思考" class="level2">
<h2 class="anchored" data-anchor-id="任务与思考">任务与思考</h2>
<ol type="1">
<li><strong>运行 A2C on CartPole:</strong> 运行代码的前半部分（CartPole）。使用 TensorBoard 观察训练曲线 (<code>rollout/ep_rew_mean</code>)。查看最终的评估结果。</li>
<li><strong>对比 A2C 与 DQN (CartPole):</strong>
<ul>
<li>比较 A2C 和上周 DQN 在 CartPole 上的<strong>收敛速度</strong>（达到相似性能所需的步数）和<strong>最终性能</strong>（评估奖励）。哪个表现更好或更快？（注意：超参数可能需要调整才能公平比较）。</li>
<li>考虑两种算法的<strong>样本效率</strong>。哪个算法似乎需要更少的交互步数来学习？（提示：DQN 使用经验回放，A2C 通常是 On-Policy）。</li>
</ul></li>
<li><strong>(可选) 运行 A2C on Pendulum:</strong> 取消注释代码的后半部分，运行 A2C 解决 Pendulum-v1 问题。观察训练曲线和评估结果。思考为什么 DQN 无法直接用于此任务，而 A2C 可以？</li>
<li><strong>分析 Actor-Critic:</strong>
<ul>
<li>解释 Actor-Critic 框架如何结合策略学习和价值学习。</li>
<li>Critic 在 Actor-Critic 中扮演什么角色？它如何帮助 Actor 学习？</li>
<li>什么是优势函数？为什么在策略梯度更新中使用优势函数估计（如 TD 误差）通常比使用原始回报更好？</li>
</ul></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./week11_lecture.html" class="pagination-link" aria-label="Week 11: 策略梯度方法 (Policy Gradient Methods)">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 11: 策略梯度方法 (Policy Gradient Methods)</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./week13_lecture.html" class="pagination-link" aria-label="Week 13: 商业案例分析 1 - 动态定价与资源优化">
        <span class="nav-page-text"><span class="chapter-title">Week 13: 商业案例分析 1 - 动态定价与资源优化</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>