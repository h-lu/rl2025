[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "智能计算",
    "section": "",
    "text": "欢迎来到《智能计算》课程！\n本课程专为经济管理类学生设计，旨在帮助大家快速掌握强化学习的基本概念和应用技能，并结合 AI 辅助编程工具，提升学习效率和实践能力。\n通过本课程，你将：\n\n了解强化学习的核心思想和算法\n掌握使用 Python 和常用库进行强化学习编程\n能够应用强化学习解决实际商业问题\n\n让我们一起开始强化学习的探索之旅吧！\n前往课程大纲\n开始第一周学习",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>课程介绍</span>"
    ]
  },
  {
    "objectID": "syllbus.html",
    "href": "syllbus.html",
    "title": "课程大纲",
    "section": "",
    "text": "课程概述\n本课程为经管学生设计，旨在弱化数学理论和公式，侧重强化学习的实战应用，并结合 AI 辅助编程。课程共计 16 周，每周两次课，每次 90 分钟。课程以项目为中心，贯穿始终。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程目标",
    "href": "syllbus.html#课程目标",
    "title": "课程大纲",
    "section": "课程目标",
    "text": "课程目标\n完成本课程后，学生将能够：\n\n理解核心概念: 解释强化学习的核心概念，并区分其与监督学习和无监督学习的不同。\n掌握编程工具: 熟练使用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，以及 Gymnasium, Stable Baselines3 等 Python 库进行强化学习编程。\n应用经典算法: 运用 Q-Learning, DQN, PPO 等经典强化学习算法，解决至少两种不同类型的商业问题 (例如：动态定价、推荐系统)。\n团队协作: 有效地进行团队合作，完成五个具有实际商业应用价值的强化学习小组项目，并撰写项目报告。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程结构",
    "href": "syllbus.html#课程结构",
    "title": "课程大纲",
    "section": "课程结构",
    "text": "课程结构\n课程分为四个阶段，循序渐进地引导学生从理论到实践，最终完成小组项目，并拓展到更广泛的商业应用场景和伦理考量。\n\n阶段一 (1-2周): 强化学习入门\n\n目标：理解基本概念，体验简单环境。\n\n阶段二 (3-8周): 经典算法实践\n\n目标：掌握 Q-Learning 和深度 Q 网络 (DQN) 算法，解决中等难度问题。\n\n阶段三 (9-13周): 高级算法探索\n\n目标：了解策略梯度方法，尝试更复杂的环境和算法。\n\n阶段四 (14-16周): 小组项目实战与商业应用拓展\n\n目标：团队合作，应用强化学习解决实际商业问题，并拓展到更广泛的商业应用场景和伦理考量。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#每周课程安排",
    "href": "syllbus.html#每周课程安排",
    "title": "课程大纲",
    "section": "每周课程安排",
    "text": "每周课程安排\n以下是详细的每周课程内容和项目安排，每次课程 90 分钟。\n\n第一阶段 (1-2周): 强化学习入门\n\n第1周 (共2次课)\n\n第一次课: 什么是强化学习？\n\n内容：\n\n讲解强化学习的核心概念：智能体 (Agent)、环境 (Environment)、动作 (Action)、状态 (State)、奖励 (Reward)。\n对比强化学习与监督学习、无监督学习。\n展示强化学习在商业领域的应用案例 (例如：推荐系统、动态定价、库存管理)。\n\n\n第二次课: AI 辅助编程入门与平衡杆 (CartPole) 初体验\n\n内容：\n\n介绍并安装 AI 辅助编程工具 (例如：GitHub Copilot, Tabnine)。\n演示如何使用 AI 工具辅助编写 Python 代码，并针对平衡杆游戏 (CartPole) 进行初步的演示和代码演练。\n介绍强化学习常用 Python 库 (例如：Gymnasium, Stable Baselines3)，强调其易用性和对初学者的友好性。\n\n实践：\n\n引导学生使用 AI 工具和库，体验平衡杆 (CartPole) 环境。\n完成简单的 Python 编程练习，为后续强化学习编程打基础。\n\n\n\n\n\n第2周 (共2次课)\n\n第一次课: 强化学习框架与迷宫环境 (Grid World) 搭建\n\n内容：\n\n解释马尔可夫决策过程 (MDP) 的基本思想 (无需深入数学细节，侧重直观理解)。\n讲解策略 (Policy)、价值函数 (Value Function, 包括 V 函数和 Q 函数) 的概念，强调直观理解。\n探讨探索 (Exploration) 与利用 (Exploitation) 的平衡问题，用商业案例 (例如：新市场尝试 vs. 现有市场深耕) 进行类比。\n\n实践：\n\n详细讲解如何使用 Gymnasium 库搭建迷宫环境 (Grid World)。\n\n\n第二次课: 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n内容：\n\n提供迷宫环境 (Grid World) 的代码框架，并指导学生使用 AI 工具进行代码补全和修改。\n\n实践：\n\n学生以小组为单位，搭建迷宫环境 (Grid World)。\n\n答疑：\n\n解答学生在环境搭建过程中遇到的问题，确保所有小组都能成功搭建迷宫环境 (Grid World)。\n布置小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n\n\n\n\n\n\n第二阶段 (3-8周): 经典算法实践\n\n第3周 (共2次课)\n\n第一次课: Q-Learning 算法详解\n\n内容：\n\n直观解释 Q-Table 和 Q-Learning 的更新规则 (避免复杂的数学公式，用表格和例子说明)。\n梳理 Q-Learning 算法的步骤流程。\n结合动态定价案例，演示如何使用 Q-Learning 算法解决简单动态定价问题 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目一：Q-Learning 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Q-Learning 算法代码，应用于迷宫寻宝 (Grid World) 项目。\n\n内容：\n\n讲解超参数 (学习率、折扣因子) 的作用，以及如何进行简单的参数调整 (直观理解)。\n\n检查点：\n\n小组项目一检查点：确保学生小组能够运行基本的 Q-Learning 智能体，在迷宫环境 (Grid World) 中进行探索。\n\n\n\n\n\n第4周 (共2次课)\n\n第一次课: Q-Learning 算法优化与改进 / 小组项目一提交 / 优秀小组项目一讲解 (3组)\n\n内容：\n\n讨论 Q-Learning 算法的局限性 (例如：状态空间爆炸问题)。\n介绍 ε-greedy 策略等探索策略，提升智能体的探索能力。\n讲解 Q-Table 初始化、奖励函数设计等实用技巧，提升 Q-Learning 算法的性能。\n\n管理：\n\n小组项目一：迷宫寻宝 (Grid World) 提交 (第 4 周第一次课前)。\n优秀小组项目一讲解 (3 组)。\n\n\n第二次课: 小组项目一：Q-Learning 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 Q-Learning 算法代码，优化智能体在迷宫环境 (Grid World) 中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第5周 (共2次课)\n\n第一次课: 深度学习的引入 - 深度 Q 网络 (DQN) 初探\n\n内容：\n\n解释 Q-Learning 算法在复杂问题上的瓶颈 (状态空间爆炸)。\n引入深度学习 (神经网络) 的基本概念 (无需深入数学细节，强调神经网络的表示能力)。\n讲解深度 Q 网络 (DQN) 的基本原理，如何使用神经网络近似 Q 函数。\n对比 DQN 与 Q-Learning 算法的异同。\n\n\n第二次课: DQN 算法关键技术 - 经验回放与目标网络\n\n内容：\n\n深入讲解 DQN 算法的两个关键技术：经验回放 (Experience Replay) 和目标网络 (Target Network) 的作用和原理。\n讲解如何使用经验回放和目标网络来提升 DQN 算法的稳定性和性能。\n\n代码：\n\n演示如何在 DQN 算法代码中加入经验回放和目标网络 (使用 AI 辅助编程)。\n\n\n\n\n\n第6周 (共2次课)\n\n第一次课: DQN 算法详解 / 小组项目二提交 / 优秀小组项目二讲解 (3组)\n\n内容：\n\n梳理 DQN 算法的完整步骤流程。\n结合平衡杆 (CartPole) 案例，演示如何使用 DQN 算法解决平衡杆问题 (使用 AI 辅助编程)。\n\n管理：\n\n小组项目二：平衡杆 (CartPole) 提交 (第 6 周第一次课前)。\n优秀小组项目二讲解 (3 组)。\n\n\n第二次课: 小组项目二：DQN 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 DQN 算法代码，应用于平衡杆 (CartPole) 项目。\n\n内容：\n\n讲解神经网络结构设计 (层数、神经元数量) 的基本原则 (无需深入理论，强调试错和调参)。\n\n检查点：\n\n小组项目二检查点：确保学生小组能够运行基本的 DQN 智能体，在平衡杆 (CartPole) 环境中进行探索。\n\n\n\n\n\n第7周 (共2次课)\n\n第一次课: DQN 算法改进与调优\n\n内容：\n\n讨论 DQN 算法的改进方向 (例如：Double DQN, Dueling DQN 等)。\n探讨不同的探索策略 (例如：ε-greedy 退火策略、Noisy Networks 等)，进一步提升智能体的探索能力。\n讲解 DQN 算法的调参技巧 (学习率、batch size、网络结构等)，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n\n第二次课: 小组项目二：DQN 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 DQN 算法代码，优化智能体在平衡杆 (CartPole) 环境中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第8周 (共2次课)\n\n第一次课: 策略梯度方法 - Policy Gradient 算法初步 / 小组项目三提交 / 优秀小组项目三讲解 (3组)\n\n内容：\n\n介绍策略梯度 (Policy Gradient) 方法的基本思想 (直接优化策略)。\n讲解 Policy Gradient 算法 (例如：REINFORCE 算法) 的基本原理和流程 (无需深入数学推导，侧重直观理解)。\n对比策略梯度方法与价值方法 (Q-Learning, DQN) 的异同。\n\n管理：\n\n小组项目三：资源分配 模拟环境 提交 (第 8 周第一次课前)。\n优秀小组项目三讲解 (3 组)。\n\n\n第二次课: 小组项目三：Policy Gradient 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Policy Gradient 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讨论策略梯度方法中的探索问题，以及如何进行有效的探索。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 Policy Gradient 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n\n第三阶段 (9-13周): 高级算法探索\n\n第9周 (共2次课)\n\n第一次课: Actor-Critic 算法 - A2C 算法详解\n\n内容：\n\n讲解 Actor-Critic 算法的基本框架 (结合策略梯度和价值方法)。\n深入讲解 A2C (Advantage Actor-Critic) 算法的原理和优势 (例如：更稳定、更高效)。\n\n代码：\n\n演示 A2C 算法的代码实现 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目三：A2C 算法优化与改进\n\n内容：\n\n讨论 A2C 算法的改进方向 (例如：GAE-优势函数估计)。\n讲解 A2C 算法的调参技巧，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n实践：\n\n指导学生尝试改进 A2C 算法，提升算法性能。\n\n\n\n\n\n第10周 (共2次课)\n\n第一次课: 近端策略优化 (PPO) 算法 - PPO 算法详解\n\n内容：\n\n讲解近端策略优化 (PPO) 算法的原理和优势 (例如：性能稳定、易于调参)。\n深入讲解 PPO 算法的核心机制：Clipping 和 PPO-Penalty。\n对比 PPO 算法与 A2C 算法的异同。\n\n\n第二次课: 小组项目三：PPO 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 PPO 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讲解 PPO 算法的关键超参数 (例如：clip ratio, value function coefficient 等) 的作用，以及如何进行参数调整。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 PPO 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n第11周 (共2次课)\n\n第一次课: PPO 算法的改进与应用\n\n内容：\n\n讨论 PPO 算法的改进方向 (例如：PPO-Clip, PPO-Penalty 的选择)。\n探讨 PPO 算法在不同商业场景中的应用案例 (例如：推荐系统、动态定价、机器人控制等)。\n介绍 PPO 算法的变体和前沿研究方向。\n\n\n第二次课: 小组项目三：PPO 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 PPO 算法代码，优化智能体在资源分配模拟环境中的表现，并尝试应用更高级的探索策略。\n\n指导：\n\n指导学生解决在更复杂算法和环境中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第12周 (共2次课)\n\n第一次课: 强化学习前沿技术与发展趋势 / 小组项目四提交 / 优秀小组项目四讲解 (3组)\n\n内容：\n\n介绍强化学习领域的前沿技术和发展趋势，例如：模仿学习 (Imitation Learning)、逆强化学习 (Inverse Reinforcement Learning)、分层强化学习 (Hierarchical Reinforcement Learning) 等 (简单介绍思想，激发学生兴趣)。\n展望强化学习未来的发展方向和潜在突破。\n引导学生思考强化学习技术可能对商业和社会带来的变革。\n\n管理：\n\n小组项目四：商业案例应用 提交 (第 12 周第一次课前)。\n优秀小组项目四讲解 (3 组)。\n\n\n第二次课: 小组项目四：商业案例应用 项目实践与问题解决\n\n实践：\n\n学生小组进行小组项目四：商业案例应用 的项目实践，进行环境搭建、算法选择、代码编写和实验验证。\n\n指导：\n\n指导学生解决在项目实践中遇到的问题。\n\n讨论：\n\n鼓励学生小组之间交流项目进展和经验，互相学习，共同进步。\n\n\n\n\n\n第13周 (共2次课)\n\n第一次课: 小组项目四成果展示 / 小组综合项目布置与答疑\n\n展示：\n\n小组项目四成果展示 (部分小组，时间允许的情况下，可以放在本次课，也可以放在第 12 周第二次课，根据实际情况灵活安排)。\n\n内容：\n\n布置小组综合项目：开放式商业问题，明确项目要求、评分标准和提交时间。\n\n答疑：\n\n针对小组综合项目进行初步答疑，解答学生关于项目选题、方向等问题。\n\n\n第二次课: 小组综合项目 - 开放式商业问题介绍与选题\n\n内容：\n\n详细介绍小组综合项目：开放式商业问题，鼓励学生结合课程所学知识和技能，以及自身专业背景和兴趣，选择具有实际商业价值和应用前景的项目。\n\n指导：\n\n指导学生小组进行小组综合项目选题，鼓励创新性和个性化。\n\n分组：\n\n学生维持小组队伍，开始讨论小组综合项目选题和分工。\n\n\n\n\n\n\n第四阶段 (14-16周): 小组项目实战与商业应用拓展\n\n第14-15周 (共4次课): 小组综合项目开发与指导\n\n内容：\n\n学生以小组为单位，进行小组综合项目的设计、开发和实验。\n\n指导：\n\n教师在课堂上提供项目指导和技术支持，解答学生在项目开发过程中遇到的问题。\n\n协作：\n\n鼓励学生充分利用 AI 辅助编程工具，提高开发效率，加强团队协作。\n\n检查：\n\n第 15 周进行小组综合项目中期检查，了解项目进展，及时发现和解决问题。\n\n\n\n\n第15周 (共2次课)\n\n第一次课: (内容待定)\n第二次课: 小组综合项目准备与答疑\n\n内容：\n\n学生小组继续进行小组综合项目的开发和完善，准备项目展示材料。\n\n答疑：\n\n教师提供项目答疑和指导，帮助学生解决项目开发中遇到的最后问题。\n\n\n\n\n\n第16周 (共2次课): 小组综合项目展示与总结\n\n第一次课: 小组综合项目展示 / 小组综合项目提交 / 小组综合项目展示 (全部小组)\n\n展示：\n\n各小组进行项目成果展示，讲解项目背景、问题定义、算法选择、实验结果和商业价值。\n\n答辩：\n\n接受教师和同学的提问和点评。\n\n管理：\n\n小组综合项目：开放式商业问题 提交 (第 16 周第一次课前)。\n\n\n第二次课: 课程总结与未来展望\n\n总结：\n\n回顾课程内容，总结强化学习的核心概念、算法和应用。\n\n展望：\n\n展望强化学习在商业领域的未来发展前景，鼓励学生继续深入学习和探索。\n\n伦理讨论：\n\n探讨强化学习在商业应用中可能引发的伦理问题，例如：算法歧视、数据隐私、自动化对就业的影响等，培养学生的社会责任感和批判性思维。\n\n评估：\n\n进行课程评估和反馈，收集学生对课程的意见和建议，为课程改进提供参考。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#项目设置-贯穿课程",
    "href": "syllbus.html#项目设置-贯穿课程",
    "title": "课程大纲",
    "section": "项目设置 (贯穿课程)",
    "text": "项目设置 (贯穿课程)\n\n小组项目 (共5个)\n\n小组项目一 (迷宫寻宝 Grid World)\n\n学习目标：掌握 Q-Learning 算法的原理和基本实现，理解探索与利用的概念。\n周期：2 周\n提交时间：第 4 周第一次课前\n\n小组项目二 (平衡杆 CartPole)\n\n学习目标：掌握 DQN 算法的原理和使用，理解经验回放和目标网络的作用，初步接触神经网络。\n周期：2 周\n提交时间：第 6 周第一次课前\n\n小组项目三 (资源分配 模拟环境)\n\n学习目标：掌握策略梯度方法 (PPO) 的原理和应用，理解策略梯度与价值方法的区别，学习处理连续动作空间问题。\n周期：2 周\n提交时间：第 8 周第一次课前\n\n小组项目四 (商业案例应用)\n\n学习目标：综合运用所学强化学习算法和工具，解决实际商业问题，提升问题建模和方案设计能力。\n周期：2 周\n提交时间：第 12 周第一次课前\n\n小组综合项目 (开放式商业问题)\n\n学习目标：培养学生的团队合作、项目管理和实际问题解决能力，提升综合应用强化学习知识的能力。\n周期：3 周\n提交时间：第 16 周第一次课前\n\n小组人数：建议每组 2-3 人。\n项目形式: 小组合作完成，包括项目方案设计、环境搭建、算法实现、实验验证、结果分析和项目报告。 每次小组项目提交后，选取 3 个优秀小组在课堂上进行项目讲解和展示，分享经验和成果。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#ai-辅助编程工具",
    "href": "syllbus.html#ai-辅助编程工具",
    "title": "课程大纲",
    "section": "AI 辅助编程工具",
    "text": "AI 辅助编程工具\n\n代码辅助: GitHub Copilot, Tabnine 等\n\n功能：代码自动补全、代码片段生成、代码错误检查等，提高编程效率，降低编程难度。\n\n强化学习库: Gymnasium (原 OpenAI Gym)\n\n功能：提供各种强化学习环境，方便学生进行算法测试和验证。\n\n算法库: Stable Baselines3\n\n功能：封装了多种常用的强化学习算法 (Q-Learning, DQN, PPO 等)，易于使用，方便学生快速上手。\n\n可视化工具: TensorBoard 等\n\n功能：方便学生监控强化学习训练过程，分析实验结果，调试算法。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#考核方式",
    "href": "syllbus.html#考核方式",
    "title": "课程大纲",
    "section": "考核方式",
    "text": "考核方式\n\n小组项目一 ~ 四 (40%): 根据项目完成质量、代码质量、实验结果和项目报告进行评分，平均分配到四个项目。\n小组综合项目 (20%): 根据项目创新性、实用性、完成度、展示效果和项目报告进行评分 (小组内成员评分可以考虑组内互评和贡献度)。\n课堂参与 (10%): 根据课堂讨论、提问、作业完成情况等进行评分，鼓励学生积极参与课堂互动。\n期末考试 (30%): 期末考试采用闭卷形式，重点考察学生对强化学习基础理论的掌握程度。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "课程目标",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#课程目标",
    "href": "week1.html#课程目标",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "了解什么是强化学习，以及强化学习的核心概念\n区分强化学习与监督学习、无监督学习\n了解强化学习在商业领域的应用案例\n初步体验 AI 辅助编程，并使用 AI 工具进行简单的 Python 编程练习\n熟悉强化学习常用的 Python 库",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第一次课什么是强化学习",
    "href": "week1.html#第一次课什么是强化学习",
    "title": "第一周：强化学习入门",
    "section": "第一次课：什么是强化学习？",
    "text": "第一次课：什么是强化学习？\n\n1. 强化学习的核心概念\n\n\n\n\n\n\nNote\n\n\n\n强化学习是一种通过与环境交互来学习的机器学习方法。智能体通过不断尝试和犯错来学习最优策略。\n\n\n让我们通过一个电商推荐系统的例子来理解核心概念：\n\n智能体 (Agent)\n\n负责学习和做出决策的实体\n例如：推荐算法就是一个智能体，它决定向用户推荐什么商品\n\n环境 (Environment)\n\n智能体所处的外部世界\n例如：用户群体、商品库、市场状况等\n环境会对智能体的动作做出响应\n\n状态 (State)\n\n环境在某一时刻的描述\n例如：用户的历史浏览记录、搜索关键词、当前页面停留时间等\n智能体根据状态来决定下一步动作\n\n动作 (Action)\n\n智能体可以采取的操作\n例如：推荐特定商品、调整商品展示顺序、发送个性化优惠券等\n\n奖励 (Reward)\n\n环境对智能体动作的反馈\n例如：用户点击推荐商品(正奖励)、直接关闭页面(负奖励)\n奖励信号指导智能体改进决策\n\n\n\n\n2. 机器学习方法对比\n\n\n\n\n\n\nImportant\n\n\n\n强化学习与监督学习和无监督学习的最大区别在于学习方式和反馈机制\n\n\n\n\n\n特征\n强化学习\n监督学习\n无监督学习\n\n\n\n\n学习方式\n通过交互和尝试\n从标注数据中学习\n从数据模式中学习\n\n\n数据需求\n无需标注数据\n需要大量标注数据\n无需标注数据\n\n\n反馈类型\n奖励信号(延迟)\n即时准确的标签\n无直接反馈\n\n\n适用场景\n决策和控制问题\n分类和回归问题\n聚类和降维问题\n\n\n\n\n\n3. 商业应用案例详解\n\n3.1 智能推荐系统\n\n应用场景：电商平台、内容平台\n实现方式：\n\n收集用户行为数据作为状态\n推荐算法作为智能体\n用户反馈作为奖励信号\n\n商业价值：\n\n提高用户转化率\n增加平台收入\n改善用户体验\n\n\n\n\n3.2 动态定价系统\n\n应用场景：酒店预订、网约车平台\n实现方式：\n\n市场供需作为状态\n价格调整作为动作\n成交量和收入作为奖励\n\n商业价值：\n\n优化收入管理\n平衡供需关系\n提高资源利用率",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "href": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "title": "第一周：强化学习入门",
    "section": "第二次课：AI辅助编程入门与平衡杆实践",
    "text": "第二次课：AI辅助编程入门与平衡杆实践\n\n1. AI辅助编程工具入门\n\n\n\n\n\n\nTip\n\n\n\nAI辅助编程工具可以显著提高编程效率，但要注意理解生成的代码逻辑\n\n\n\n1.1 GitHub Copilot 安装与配置\n\nVS Code 插件安装\n1. 打开 VS Code\n2. 转到扩展市场\n3. 搜索 \"GitHub Copilot\"\n4. 点击安装\n基本功能演示\n\n代码自动补全\n注释生成代码\n函数签名提示\n代码解释生成\n\n\n\n\n1.2 实用技巧\n\n使用清晰的注释引导生成\n分步骤编写复杂逻辑\n及时检查生成的代码\n理解并修改不合适的建议\n\n\n\n\n2. 强化学习环境搭建\n\n2.1 Gymnasium 库\n# 安装 Gymnasium\npip install gymnasium\n\n# 基本使用示例\nimport gymnasium as gym\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 获取环境信息\nprint(f\"动作空间: {env.action_space}\")\nprint(f\"状态空间: {env.observation_space}\")\n\n\n2.2 Stable Baselines3 库\n# 安装 Stable Baselines3\npip install stable-baselines3\n\n# 基本使用示例\nfrom stable_baselines3 import DQN\n\n# 创建模型\nmodel = DQN(\"MlpPolicy\", env, verbose=1)\n\n# 训练模型\nmodel.learn(total_timesteps=10000)\n\n\n\n3. CartPole 环境实践\n\n\n\n\n\n\nNote\n\n\n\nCartPole 是强化学习入门的经典环境，目标是通过左右移动小车来保持杆子平衡\n\n\n\n3.1 环境介绍\nimport gymnasium as gym\nimport numpy as np\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 环境参数说明\nprint(\"状态空间包含4个连续值:\")\nprint(\"- 小车位置: [-4.8, 4.8]\")\nprint(\"- 小车速度: [-∞, ∞]\")\nprint(\"- 杆子角度: [-0.418, 0.418]\")\nprint(\"- 杆子角速度: [-∞, ∞]\")\n\nprint(\"\\n动作空间包含2个离散动作:\")\nprint(\"- 0: 向左推\")\nprint(\"- 1: 向右推\")\n\n\n3.2 随机动作示例\n# 运行随机策略\nobservation, info = env.reset()\ntotal_reward = 0\n\nfor step in range(100):\n    # 随机选择动作\n    action = env.action_space.sample()\n    \n    # 执行动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    total_reward += reward\n    \n    # 打印当前状态\n    print(f\"Step {step}: State = {observation}, Reward = {reward}\")\n    \n    # 判断是否需要重置\n    if terminated or truncated:\n        print(f\"Episode finished after {step + 1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()\n\n\n\n课后作业\n\n环境配置\n\n安装 Python 3.8+\n配置 VS Code 和 GitHub Copilot\n安装所需 Python 库\n\n编程练习\n\n修改随机动作示例，尝试实现简单的规则策略\n记录并分析不同策略的表现\n使用 AI 辅助工具优化代码\n\n思考题\n\n强化学习如何应用到你的专业领域？\nCartPole 环境中的状态空间设计有什么特点？\n为什么需要重置环境？\n\n\n\n\n预习资料\n\n阅读材料\n\nGymnasium 官方文档\nStable Baselines3 入门教程\n马尔可夫决策过程基础概念\n\n视频资源\n\nCartPole 环境详解\n强化学习基础概念讲解\n\n下周预习重点\n\n马尔可夫决策过程\n价值函数与策略函数\nGrid World 环境介绍",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "课程目标",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#课程目标",
    "href": "week2.html#课程目标",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "理解马尔可夫决策过程 (MDP) 的基本思想\n掌握策略 (Policy)、价值函数 (Value Function) 的概念\n理解探索 (Exploration) 与利用 (Exploitation) 的平衡\n学习使用 Gymnasium 库搭建迷宫环境 (Grid World)\n掌握使用 AI 辅助工具进行代码补全和修改",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "href": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "第一次课：强化学习框架与迷宫环境 (Grid World) 搭建",
    "text": "第一次课：强化学习框架与迷宫环境 (Grid World) 搭建\n\n1. 马尔可夫决策过程 (MDP) 基础\n\n\n\n\n\n\nNote\n\n\n\n马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习的核心框架，用于形式化描述智能体与环境的交互过程。\n\n\n\n1.1 MDP 的核心要素 (无需数学细节，侧重直观理解)\n\n状态 (State, S)\n\n环境的描述，包含了智能体做出决策所需的信息。\n马尔可夫性质: 当前状态包含了所有历史信息，未来的状态只依赖于当前状态和动作，而与过去的历史无关。\n在迷宫环境中，状态可以是智能体在迷宫中的位置坐标。\n\n动作 (Action, A)\n\n智能体在每个状态下可以采取的行为。\n在迷宫环境中，动作可以是向上、下、左、右移动。\n\n转移概率 (Transition Probability, P)\n\n智能体在状态 \\(s\\) 采取动作 \\(a\\) 后，转移到下一个状态 \\(s'\\) 的概率。\n\\(P(s'|s, a)\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 的概率。\n在确定性迷宫环境中，转移概率是确定的 (例如：向上走，一定到达上方的格子)。在非确定性环境中，转移概率可能存在随机性 (例如：在某些环境中，向上走，可能以 0.8 的概率到达上方格子，0.2 的概率滑到其他格子)。\n\n奖励 (Reward, R)\n\n智能体在与环境交互后获得的反馈信号，用于评价动作的好坏。\n\\(R(s, a, s')\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 后获得的奖励。\n在迷宫寻宝游戏中，到达宝藏位置可以获得正奖励，撞墙或者到达陷阱位置可能获得负奖励，正常移动可能获得小的负奖励 (鼓励尽快寻宝)。\n\n策略 (Policy, \\(\\pi\\))\n\n智能体根据当前状态选择动作的规则，可以是确定性的或随机性的。\n\\(\\pi(a|s)\\) 表示在状态 \\(s\\) 下选择动作 \\(a\\) 的概率。\n强化学习的目标是学习最优策略，使得智能体在环境中获得最大的累积奖励。\n\n\n\n\n1.2 MDP 过程示意\n\n\n\n\n\ngraph LR\n    S --&gt; A(动作 Action)\n    A --&gt; S'(下一个状态 State)\n    S' --&gt; R(奖励 Reward)\n    R --&gt; S\n\n\n\n\n\n\n智能体 (Agent) 处在状态 (State) S，根据策略 (Policy) 选择动作 (Action) A，环境 (Environment) 接收动作后，转移到新的状态 (State) S’，并给智能体返回奖励 (Reward) R。智能体不断与环境交互，目标是学习到最优策略，最大化累积奖励。\n\n\n\n2. 价值函数 (Value Function)\n\n\n\n\n\n\nNote\n\n\n\n价值函数用于评估在给定状态或状态-动作对下，未来预期累积奖励的期望值。价值函数是强化学习算法的核心概念之一。\n\n\n\n2.1 V 函数 (State Value Function)\n\n\\(V_{\\pi}(s)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，未来可以获得的期望累积奖励。\nV 函数评估的是状态的价值，即处于某个状态的好坏程度。\nV 函数越大，表示当前状态越好，未来可以获得的期望奖励越高。\n\n\n\n2.2 Q 函数 (Action Value Function)\n\n\\(Q_{\\pi}(s, a)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，选择动作 \\(a\\) 后，未来可以获得的期望累积奖励。\nQ 函数评估的是状态-动作对的价值，即在某个状态下，采取某个动作的好坏程度。\nQ 函数越大，表示在当前状态下，采取该动作越好，未来可以获得的期望奖励越高。\n\n\n\n2.3 V 函数和 Q 函数的关系\n\nV 函数可以通过 Q 函数计算得到：\n\\(V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) Q_{\\pi}(s, a)\\)\n即状态 \\(s\\) 的价值等于在状态 \\(s\\) 下，所有可能动作的 Q 函数值的期望 (按照策略 \\(\\pi\\) 的动作选择概率进行加权平均)。\nQ 函数可以通过 V 函数和奖励函数以及转移概率计算得到 (Bellman 方程，下周会详细讲解)。\n\n\n\n\n3. 探索 (Exploration) vs. 利用 (Exploitation)\n\n\n\n\n\n\nImportant\n\n\n\n探索与利用是强化学习中一个核心的权衡问题。智能体需要在探索新策略和利用已知最优策略之间找到平衡。\n\n\n\n探索 (Exploration)\n\n尝试新的动作，探索未知的状态和动作空间，以发现潜在的更优策略。\n例如：在迷宫寻宝游戏中，智能体可能会尝试一些之前没有走过的路径，以期找到更短的路径或者隐藏的宝藏。\n\n利用 (Exploitation)\n\n根据已知的经验，选择当前认为最优的动作，以最大化当前的累积奖励。\n例如：在迷宫寻宝游戏中，如果智能体已经知道某条路径可以到达宝藏，它可能会重复选择这条路径，以快速获得奖励。\n\n\n\n3.1 探索与利用的平衡\n\n过度探索：可能导致智能体花费大量时间在探索无用的区域，而错失已知的最优策略。\n过度利用：可能导致智能体陷入局部最优解，而无法发现全局最优策略。\n\\(\\epsilon\\)-greedy 策略 是一种常用的平衡探索与利用的策略：\n\n以概率 \\(\\epsilon\\) (探索率) 随机选择动作 (探索)。\n以概率 \\(1-\\epsilon\\) 选择当前 Q 函数值最大的动作 (利用)。\n\\(\\epsilon\\) 的值通常会随着训练的进行而逐渐减小，从而在训练初期鼓励探索，在训练后期侧重利用。\n\n\n\n\n3.2 商业案例类比\n\n新市场尝试 vs. 现有市场深耕\n\n新市场尝试 (探索)：企业尝试进入新的市场领域，例如：开发新的产品线、拓展新的客户群体、进入新的地理区域。\n\n优点：可能发现新的增长机会，拓展业务范围，提高长期竞争力。\n缺点：风险较高，投入成本较大，短期收益不确定。\n\n现有市场深耕 (利用)：企业在现有市场领域深耕细作，例如：优化现有产品、提高客户满意度、提升市场份额。\n\n优点：风险较低，收益相对稳定，短期回报可观。\n缺点：增长空间有限，可能错失新的市场机会，长期竞争力可能不足。\n\n平衡策略：企业需要在新市场尝试和现有市场深耕之间找到平衡点，例如：\n\n双元策略 (Dual Strategy)：同时进行探索性创新和利用性优化。\n阶段性策略：在不同发展阶段侧重不同的策略，例如：初创期侧重探索，成长期侧重利用，成熟期再次侧重探索。\n\n\n\n\n\n\n4. 迷宫环境 (Grid World) 搭建 (Gymnasium)\n\n\n\n\n\n\nNote\n\n\n\nGrid World 是强化学习中常用的简单环境，非常适合初学者入门。我们将使用 Gymnasium 库来搭建 Grid World 环境。\n\n\n\n4.1 Gymnasium 库安装与基本使用 (回顾)\n# 安装 Gymnasium (如果已安装，可以跳过)\n# pip install gymnasium\n\nimport gymnasium as gym\n\n# 创建 CartPole 环境 (回顾)\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 重置环境\nobservation, info = env.reset()\n\n# 随机选择一个动作\naction = env.action_space.sample()\n\n# 执行动作，获取环境反馈\nobservation, reward, terminated, truncated, info = env.step(action)\n\n# 关闭环境\nenv.close()\n\n\n4.2 自定义 Grid World 环境\n\n创建自定义环境类\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass GridWorldEnv(gym.Env):\n    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n\n    def __init__(self, render_mode=None, size=5):\n        super().__init__()\n        self.size = size  # Grid world size\n        self.window_size = 512  # The size of the PyGame window\n\n        # 动作空间：上下左右 (4个离散动作)\n        self.action_space = spaces.Discrete(4)\n        # 观测空间：智能体在 Grid world 中的位置 (行、列坐标)\n        self.observation_space = spaces.Discrete(size * size)\n\n        \"\"\"\n        Map of the grid world:\n        - 0: 空格 (可以自由移动)\n        - 1: 障碍物 (无法通过)\n        - 2: 宝藏 (目标位置，获得奖励)\n        - 3: 陷阱 (负奖励)\n        - 4: 智能体起始位置\n        \"\"\"\n        self._grid_map = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 2, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0]\n        ])\n\n        # 宝藏位置\n        self._target_location = np.array([2, 2])\n        # 陷阱位置 (可以添加多个陷阱位置)\n        self._trap_location = np.array([4, 4])\n        # 智能体起始位置\n        self._agent_start_location = np.array([0, 0])\n\n        self._agent_location = np.array([0, 0])\n\n        self.render_mode = render_mode\n\n        \"\"\"\n        对于 render_mode=\"human\" 或 \"rgb_array\"，需要初始化 PyGame\n        \"\"\"\n        if render_mode is not None:\n            import pygame\n\n            pygame.init()\n            pygame.font.init()\n            self.window = pygame.display.set_mode(\n                (self.window_size, self.window_size)\n            )\n            self.clock = pygame.time.Clock()\n\n    def _get_obs(self):\n        # 将 2D 坐标转换为 1D 状态表示\n        return np.ravel_multi_index(self._agent_location, (self.size, self.size))\n\n    def _get_info(self):\n        return {\n            \"agent_location\": self._agent_location,\n            \"target_location\": self._target_location,\n            \"distance_to_target\": np.linalg.norm(self._agent_location - self._target_location, ord=1)\n        }\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        # 将智能体放置在起始位置\n        self._agent_location = self._agent_start_location\n\n        observation = self._get_obs()\n        info = self._get_info()\n\n        if self.render_mode == \"human\":\n            self._render_frame()\n\n        return observation, info\n\n    def step(self, action):\n        # 动作编码：0=上, 1=右, 2=下, 3=左\n        direction = np.array([\n            [-1, 0], # 上\n            [0, 1],  # 右\n            [1, 0],  # 下\n            [0, -1]  # 左\n        ])\n        # 智能体移动到下一个位置 (如果超出边界，则保持原位置)\n        proposed_location = self._agent_location + direction[action]\n\n        # 判断是否超出边界\n        if not (0 &lt;= proposed_location[0] &lt; self.size and 0 &lt;= proposed_location[1] &lt; self.size):\n            # 超出边界，保持原位置\n            self._agent_location = self._agent_location\n        elif self._grid_map[proposed_location[0], proposed_location[1]] == 1:\n            # 撞到障碍物，保持原位置\n            self._agent_location = self._agent_location\n        else:\n            # 移动到新位置\n            self._agent_location = proposed_location\n\n        # 获取新的观测\n        observation = self._get_obs()\n        info = self._get_info()\n\n        # 根据智能体位置计算奖励 (稀疏奖励)\n        if np.array_equal(self._agent_location, self._target_location):\n            reward = 10  # 到达宝藏，正奖励\n            terminated = True # 游戏结束\n        elif np.array_equal(self._agent_location, self._trap_location):\n            reward = -10 # 到达陷阱，负奖励\n            terminated = True # 游戏结束\n        else:\n            reward = -1 # 正常移动，小负奖励 (鼓励尽快寻宝)\n            terminated = False\n\n        truncated = False #  Grid world 环境没有时间限制，所以 truncated 始终为 False\n\n        if self.render_mode == \"human\":\n            self._render_frame()\n\n        return observation, reward, terminated, truncated, info\n\n    def render(self):\n        if self.render_mode == \"rgb_array\":\n            return self._render_frame()\n        elif self.render_mode == \"human\":\n            self._render_frame()\n            return None\n\n    def _render_frame(self):\n        if self.window is None and self.render_mode == \"human\":\n            import pygame\n            pygame.init()\n            pygame.font.init()\n            self.window = pygame.display.set_mode(\n                (self.window_size, self.window_size)\n            )\n        if self.clock is None and self.render_mode == \"human\":\n            import pygame\n            self.clock = pygame.time.Clock()\n\n        canvas = pygame.Surface((self.window_size, self.window_size))\n        canvas.fill((255, 255, 255)) # 白色背景\n        pix_square_size = (\n            self.window_size / self.size\n        )  # 一格像素大小\n\n        # 绘制 Grid world 地图\n        for row in range(self.size):\n            for col in range(self.size):\n                if self._grid_map[row, col] == 1: # 障碍物\n                    pygame.draw.rect(\n                        canvas,\n                        (0, 0, 0), # 黑色\n                        pygame.Rect(\n                            pix_square_size * col,\n                            pix_square_size * row,\n                            pix_square_size,\n                            pix_square_size,\n                        ),\n                    )\n                elif self._grid_map[row, col] == 2: # 宝藏\n                    pygame.draw.rect(\n                        canvas,\n                        (255, 255, 0), # 黄色\n                        pygame.Rect(\n                            pix_square_size * col,\n                            pix_square_size * row,\n                            pix_square_size,\n                            pix_square_size,\n                        ),\n                    )\n                elif self._grid_map[row, col] == 3: # 陷阱\n                    pygame.draw.rect(\n                        canvas,\n                        (255, 0, 0), # 红色\n                        pygame.Rect(\n                            pix_square_size * col,\n                            pix_square_size * row,\n                            pix_square_size,\n                            pix_square_size,\n                        ),\n                    )\n\n        # 绘制智能体\n        import pygame\n        pygame.draw.circle(\n            canvas,\n            (0, 0, 255), # 蓝色\n            (self._agent_location + 0.5) * pix_square_size,\n            min(pix_square_size, pix_square_size) / 3,\n        )\n\n        if self.render_mode == \"human\":\n            # The following line copies our drawings from `canvas` to the visible window\n            self.window.blit(canvas, canvas.get_rect())\n            pygame.event.pump()\n            pygame.display.flip()\n\n            # We need to ensure that human-rendering occurs at the predefined FPS.\n            # The following line will automatically add a delay to keep the FPS stable.\n            self.clock.tick(self.metadata[\"render_fps\"])\n        else:  # rgb_array\n            return np.transpose(\n                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n            )\n\n    def close(self):\n        if self.window is not None:\n            import pygame\n            pygame.display.quit()\n            pygame.quit()\n注册自定义环境\nfrom gymnasium.envs.registration import register\n\nregister(\n    id=\"GridWorld-v0\", # 环境 ID\n    entry_point=\"__main__:GridWorldEnv\", #  指向自定义环境类\n    kwargs={\"render_mode\": \"human\", \"size\": 5}, #  环境初始化参数\n    max_episode_steps=300, #  最大 episode 步数\n)\n注意: entry_point=\"__main__:GridWorldEnv\" 假设你的 GridWorldEnv 类定义在当前脚本文件中。如果你的环境类定义在单独的文件中，需要修改 entry_point 指向你的环境类。 例如，如果你的环境类定义在 envs/grid_world.py 文件中，并且类名为 CustomGridWorldEnv, 则 entry_point 应该修改为 entry_point=\"envs.grid_world:CustomGridWorldEnv\"。\n测试自定义环境\nimport gymnasium as gym\n# 导入注册自定义环境 (确保注册代码在运行前被执行)\nimport week2\n\n# 创建自定义 Grid World 环境\nenv = gym.make(\"GridWorld-v0\")\nobservation, info = env.reset()\n\nfor _ in range(10):\n    action = env.action_space.sample() # 随机动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    if terminated or truncated:\n        observation, info = env.reset()\n    env.render() # 渲染环境 (human 模式下会显示窗口)\n\nenv.close()\n\n\n\n4.3 AI 辅助编程实践\n\n使用 GitHub Copilot 或 Tabnine 等 AI 工具，辅助完成 Grid World 环境代码的编写。\n例如：\n\n可以尝试输入注释 # 创建 GridWorldEnv 类，让 AI 工具自动补全类定义代码。\n可以尝试输入函数签名 def step(self, action):，让 AI 工具自动生成 step 函数的代码框架。\n可以利用 AI 工具的代码补全、代码片段生成、代码解释等功能，提高代码编写效率，并理解代码逻辑。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "href": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建",
    "text": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n1. 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n项目目标：\n\n以小组为单位，基于第一次课的代码框架，独立完成迷宫环境 (Grid World) 的搭建。\n扩展迷宫地图，设计更复杂的迷宫场景 (例如：增加更多障碍物、宝藏、陷阱等)。\n实现基本的环境渲染，能够可视化智能体在迷宫中的探索过程。\n\n代码框架：\n\n提供第一次课中 GridWorldEnv 类的代码框架 (作为项目的基础)。\n小组需要自行完成代码的补全、修改和扩展。\n\nAI 辅助工具：\n\n鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高开发效率。\n但强调：AI 工具是辅助手段，学生需要理解代码逻辑，不能完全依赖 AI 工具生成代码，而忽略代码理解和调试。\n\n项目提交：\n\n小组提交完整的 Grid World 环境代码 (Python 文件)。\n无需提交项目报告。\n\n\n\n\n2. 答疑与指导\n\n解答学生在环境搭建过程中遇到的问题。\n重点关注：\n\nGymnasium 库的使用方法。\n自定义环境类的结构和接口。\n状态空间、动作空间、奖励函数的设计。\n环境渲染的实现。\nAI 辅助工具的使用技巧。\n\n\n\n\n3. 布置小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n明确项目要求、提交时间和评分标准 (本次项目不评分，作为后续项目的基础)。\n鼓励小组积极探索、尝试，遇到问题及时提问。\n建议小组：\n\n提前开始项目，预留充足的开发和调试时间。\n分工合作，提高开发效率。\n充分利用 AI 辅助工具，但也要注重代码理解和调试能力。\n相互交流、学习，共同解决问题。\n\n\n\n\n课后作业\n\n完成小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n思考题：\n\n在 Grid World 环境中，如何设计更有效的奖励函数，以引导智能体更快地找到宝藏？\n如果迷宫地图非常大，状态空间会变得很大，对强化学习算法会产生什么影响？\n如何使用 AI 辅助工具更高效地进行强化学习代码开发？\n\n\n\n\n预习资料\n\n阅读材料：\n\nGymnasium 官方文档 (自定义环境部分)。\n强化学习算法基础：Q-Learning 算法初步。\n探索-利用平衡的更多策略 (例如：\\(\\epsilon\\)-greedy 退火策略、UCB 算法等)。\n\n视频资源：\n\nGrid World 环境搭建详解。\nQ-Learning 算法原理讲解 (初步了解)。\n探索与利用的平衡策略讲解。\n\n下周预习重点：\n\nQ-Learning 算法原理和步骤。\nQ-Table 的更新规则。\n使用 Q-Learning 算法解决 Grid World 迷宫寻宝问题。\n\n\n\n请注意:\n\n代码框架: 第一次课的代码框架，指的是第一次课中 GridWorldEnv 类的代码。学生需要基于这个代码框架进行扩展和修改，完成小组项目一。\n环境注册: 请确保在运行 Grid World 环境代码之前，已经执行了环境注册代码 (register(...))。可以将注册代码放在单独的文件中，或者放在运行环境代码的脚本文件的开头。\nAI 辅助工具: 鼓励学生使用 AI 辅助工具，但务必强调理解代码逻辑的重要性，避免过度依赖 AI 工具。\n小组项目: 小组项目一旨在让学生熟悉 Gymnasium 库和自定义环境的流程，为后续的强化学习算法实践打下基础。",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "课程目标",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#课程目标",
    "href": "week3.html#课程目标",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "理解 Q-Learning 算法的核心思想和原理\n掌握 Q-Table 的概念和更新规则\n学习使用 Q-Learning 算法解决迷宫寻宝 (Grid World) 问题\n了解超参数 (学习率、折扣因子) 的作用，并进行简单调整\n掌握 Q-Learning 算法的基本编程实现",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第一次课q-learning-算法详解",
    "href": "week3.html#第一次课q-learning-算法详解",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "第一次课：Q-Learning 算法详解",
    "text": "第一次课：Q-Learning 算法详解\n\n1. Q-Learning 算法核心思想\n\n\n\n\n\n\nNote\n\n\n\nQ-Learning 是一种基于价值 (Value-based) 的离线 (Off-policy) 强化学习算法，用于学习最优 Q 函数，从而得到最优策略。\n\n\n\n核心思想: 通过不断试错 (Trial-and-Error) 和更新 Q-Table，逐步逼近最优 Q 函数。\n离线 (Off-policy): 学习的策略 (Q 函数对应的策略) 与实际执行的策略 (探索策略，例如 \\(\\epsilon\\)-greedy 策略) 可以不同。\n最优 Q 函数 (\\(Q^*(s, a)\\)): 表示在状态 \\(s\\) 下，执行动作 \\(a\\)，并之后都采取最优策略所能获得的最大期望累积奖励。\n最优策略 (\\(\\pi^*(s)\\)): 在每个状态 \\(s\\) 下，选择能使 Q 函数值 \\(Q^*(s, a)\\) 最大的动作 \\(a\\)。\n\n\n\n2. Q-Table (Q 值表)\n\n\n\n\n\n\nNote\n\n\n\nQ-Table 是 Q-Learning 算法中用于存储 Q 函数值的表格。表格的行表示状态 (State)，列表示动作 (Action)，单元格存储的是 Q 值 \\(Q(s, a)\\)。\n\n\n\n表格结构:\n\n\n\n\n\n\n\n\n\n\n状态 (State)\n动作 1 (Action 1)\n动作 2 (Action 2)\n…\n动作 n (Action n)\n\n\n\n\n状态 1 (S1)\n\\(Q(S1, A1)\\)\n\\(Q(S1, A2)\\)\n…\n\\(Q(S1, An)\\)\n\n\n状态 2 (S2)\n\\(Q(S2, A1)\\)\n\\(Q(S2, A2)\\)\n…\n\\(Q(S2, An)\\)\n\n\n…\n…\n…\n…\n…\n\n\n状态 m (Sm)\n\\(Q(Sm, A1)\\)\n\\(Q(Sm, A2)\\)\n…\n\\(Q(Sm, An)\\)\n\n\n\n初始化: Q-Table 通常初始化为 0 或者小的随机值。\n更新: 在智能体与环境交互的过程中，不断更新 Q-Table 中的 Q 值，使其逐渐逼近真实的最优 Q 函数值。\n查询: 在决策时，根据当前状态 \\(s\\)，查询 Q-Table 中对应行的 Q 值，选择 Q 值最大的动作 (或者根据探索策略选择动作)。\n\n\n\n3. Q-Learning 更新规则 (时序差分学习)\n\n\n\n\n\n\nNote\n\n\n\nQ-Learning 使用时序差分 (Temporal Difference, TD) 学习方法来更新 Q 值。TD 学习是一种无模型 (Model-free) 的强化学习方法，无需事先知道环境的转移概率和奖励函数，通过采样和迭代的方式进行学习。\n\n\n\n更新公式 (无需数学公式，侧重直观理解):\n新的 Q(s, a)  &lt;-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))\n\n学习率 (\\(\\alpha\\)): 控制每次更新的幅度，取值范围通常为 \\([0, 1]\\)。\n\n\\(\\alpha\\) 较大: 更新幅度大，学习速度快，但容易不稳定，可能震荡。\n\\(\\alpha\\) 较小: 更新幅度小，学习速度慢，但稳定，收敛性好。\n\nTD 目标 (TD Target): 表示我们期望的 Q 值，是对未来累积奖励的估计。\n\nTD 目标 = 即时奖励 (Reward) + 折扣因子 (\\(\\gamma\\)) * 未来最优 Q 值 (下一状态的最大 Q 值)\n未来最优 Q 值: 在下一个状态 \\(s'\\) 下，所有可能动作 \\(a'\\) 中，Q 值最大的那个值，即 \\(\\max_{a'} Q(s', a')\\)。\n\n折扣因子 (\\(\\gamma\\)): 控制未来奖励的重要性，取值范围通常为 \\([0, 1]\\)。\n\n\\(\\gamma\\) 接近 0: 更关注即时奖励，短视。\n\\(\\gamma\\) 接近 1: 更关注未来奖励，有远见。\n\n\n更新过程:\n\n智能体在状态 \\(s\\) 下，根据策略 (例如 \\(\\epsilon\\)-greedy 策略) 选择动作 \\(a\\)。\n智能体执行动作 \\(a\\)，环境转移到下一个状态 \\(s'\\)，并返回奖励 \\(r\\)。\n根据 Q-Learning 更新规则，更新 Q-Table 中 \\(Q(s, a)\\) 的值。\n将当前状态更新为 \\(s'\\)，重复步骤 1-3，直到 episode 结束。\n\nQ-Learning 更新规则图示:\n\n\n\n\n\n\ngraph LR\n    S[\"状态 s\"] --&gt; A[\"动作 a\"]\n    A --&gt; E[\"环境\"]\n    E --&gt; SP[\"新状态 s'\"]\n    E --&gt; R[\"奖励 r\"]\n    SP --&gt; QP[\"最大Q值\"]\n    R --&gt; TD[\"TD目标\"]\n    QP --&gt; TD\n    TD --&gt; U[\"更新Q表\"]\n\n\n\n\n\n\n\n\n4. Q-Learning 算法步骤流程\n\n\n\n\n\n\nNote\n\n\n\nQ-Learning 算法的步骤流程可以总结为：初始化 Q-Table，循环迭代 episodes，在每个 episode 中，循环迭代 steps，选择动作，执行动作，更新 Q-Table。\n\n\n\n初始化 Q-Table: 创建一个 Q-Table，行数为状态空间大小，列数为动作空间大小，所有 Q 值初始化为 0 (或其他小值)。\n循环迭代 Episodes: 进行多次 episodes 训练，让智能体不断与环境交互，学习优化策略。\n\nFor each episode:\n\n初始化环境: 重置环境到初始状态 \\(s\\)。\n循环迭代 Steps: 在每个 episode 中，进行多步交互，直到 episode 结束 (例如到达目标状态或达到最大步数)。\n\nFor each step:\n\n选择动作: 根据当前状态 \\(s\\)，使用探索策略 (例如 \\(\\epsilon\\)-greedy 策略) 从 Q-Table 中选择一个动作 \\(a\\)。\n执行动作: 智能体在环境中执行动作 \\(a\\)，环境返回下一个状态 \\(s'\\) 和奖励 \\(r\\)。\n更新 Q-Table: 使用 Q-Learning 更新规则，根据 \\((s, a, r, s')\\) 更新 Q-Table 中 \\(Q(s, a)\\) 的值。\n更新状态: 将当前状态更新为 \\(s'，s \\leftarrow s'\\)。\n判断 Episode 结束: 判断是否到达终止状态 (terminated) 或截断状态 (truncated)，如果 episode 结束，则跳出 step 循环。\n\n\n\n\n训练结束: 当 Q-Table 收敛 (Q 值变化很小) 或者达到预设的训练 episodes 数量时，训练结束。\n策略提取: 训练结束后，可以从 Q-Table 中提取最优策略。对于每个状态 \\(s\\)，最优策略 \\(\\pi^*(s)\\) 是选择能使 Q 值 \\(Q^*(s, a)\\) 最大的动作 \\(a\\)。\n\n\n\n5. 动态定价案例 (结合商业案例，演示 Q-Learning 应用)\n\n\n\n\n\n\nNote\n\n\n\n动态定价是一种根据市场供需变化，实时调整商品或服务价格的定价策略。强化学习可以用于学习最优的动态定价策略，以最大化收益或利润。\n\n\n\n5.1 动态定价场景简化\n\n场景: 在线零售平台，销售单一商品 (例如：某品牌的热门手机)。\n状态 (State): 商品库存水平 (例如：高库存、中库存、低库存)。\n动作 (Action): 价格调整 (例如：涨价、降价、维持原价)。\n奖励 (Reward): 销售利润 (例如：销售额 - 成本)。\n目标: 最大化一段时间内的累积利润。\n\n\n\n5.2 Q-Learning 动态定价步骤 (简化版)\n\n状态空间: 假设商品库存水平分为 3 个状态：高库存 (High)，中库存 (Medium)，低库存 (Low)。\n动作空间: 假设价格调整分为 3 个动作：涨价 (Increase)，降价 (Decrease)，维持原价 (Maintain)。\nQ-Table: 创建一个 \\(3 \\times 3\\) 的 Q-Table，初始化为 0。\n奖励函数: 假设奖励函数如下 (简化示例)：\n\n\n\n状态 (库存)\n动作 (价格调整)\n奖励 (利润)\n\n\n\n\n高库存\n降价\n+5 (销量增加)\n\n\n高库存\n维持原价\n+2 (正常销量)\n\n\n高库存\n涨价\n-1 (销量减少)\n\n\n中库存\n降价\n+3 (销量略增)\n\n\n中库存\n维持原价\n+4 (正常销量)\n\n\n中库存\n涨价\n+1 (销量略减)\n\n\n低库存\n降价\n-2 (缺货风险)\n\n\n低库存\n维持原价\n+6 (高利润率)\n\n\n低库存\n涨价\n+8 (更高利润率)\n\n\n\n注意: 这只是一个简化的奖励函数示例，实际应用中奖励函数会更复杂，需要根据具体业务场景进行设计。\nQ-Learning 训练: 进行 episodes 训练，使用 \\(\\epsilon\\)-greedy 策略选择动作，并根据奖励函数和 Q-Learning 更新规则更新 Q-Table。\n最优策略: 训练结束后，Q-Table 会学习到最优的动态定价策略。例如，可能学习到：\n\n高库存: 应该 降价 以快速清理库存。\n中库存: 应该 维持原价 以获得稳定利润。\n低库存: 可以 涨价 以提高利润率 (但需注意缺货风险)。\n\n\n\n\n5.3 AI 辅助编程演示 (使用 Python 和 AI 工具，演示动态定价代码)\n\n可以使用 Python 模拟动态定价环境 (状态、动作、奖励函数等)。\n使用 AI 辅助编程工具 (例如 GitHub Copilot)，辅助编写 Q-Learning 算法代码，应用于动态定价问题。\n演示代码 (伪代码示例，仅供参考)：\n# 初始化 Q-Table (字典或 NumPy 数组)\nq_table = {} #  状态为 (库存状态)，动作为 (价格调整动作)\n\n# 定义状态空间和动作空间 (例如使用枚举类型)\nstates = [\"High\", \"Medium\", \"Low\"]\nactions = [\"Increase\", \"Maintain\", \"Decrease\"]\n\n# 定义奖励函数 (根据状态和动作返回奖励值)\ndef get_reward(state, action):\n    # ... (根据奖励函数表格返回奖励值)\n    pass\n\n# Q-Learning 算法训练循环\nepisodes = 1000\nlearning_rate = 0.1\ndiscount_factor = 0.9\nepsilon = 0.1 #  epsilon-greedy 策略的探索率\n\nfor episode in range(episodes):\n    # 初始化状态 (随机选择初始库存状态)\n    current_state = random.choice(states)\n\n    for step in range(steps_per_episode): #  每个 episode 的步数\n        # 使用 epsilon-greedy 策略选择动作\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.choice(actions) #  探索：随机选择动作\n        else:\n            # 利用：选择 Q 值最大的动作\n            action = max(actions, key=lambda a: q_table.get((current_state, a), 0))\n\n        # 执行动作，获取下一个状态和奖励 (模拟环境交互)\n        next_state = ... #  根据当前状态和动作，模拟状态转移\n        reward = get_reward(current_state, action)\n\n        # Q-Learning 更新规则\n        old_q_value = q_table.get((current_state, action), 0)\n        next_max_q = max([q_table.get((next_state, a), 0) for a in actions]) #  下一个状态的最大 Q 值\n        new_q_value = old_q_value + learning_rate * (reward + discount_factor * next_max_q - old_q_value)\n        q_table[(current_state, action)] = new_q_value #  更新 Q-Table\n\n        # 更新状态\n        current_state = next_state\n\n# 训练结束，输出学习到的 Q-Table 和最优策略\nprint(\"Q-Table:\")\nprint(q_table)\n\n#  提取最优策略 (对于每个状态，选择 Q 值最大的动作)\noptimal_policy = {}\nfor state in states:\n    optimal_action = max(actions, key=lambda a: q_table.get((state, a), 0))\n    optimal_policy[state] = optimal_action\nprint(\"\\nOptimal Policy:\")\nprint(optimal_policy)\n注意: 这只是一个非常简化的动态定价示例，用于演示 Q-Learning 的基本应用思路。实际商业场景中的动态定价问题会更加复杂，需要考虑更多因素 (例如：竞争对手价格、季节性因素、促销活动等)，并使用更复杂的强化学习算法。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "href": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "第二次课：小组项目一：Q-Learning 算法编程实践",
    "text": "第二次课：小组项目一：Q-Learning 算法编程实践\n\n1. 小组项目一：Q-Learning 算法编程实践 (迷宫寻宝 Grid World)\n\n项目目标:\n\n以小组为单位，使用 Python 和 AI 工具，编写 Q-Learning 算法代码，应用于迷宫寻宝 (Grid World) 项目。\n实现 Q-Learning 智能体，使其能够在迷宫环境中自主探索，并学习找到宝藏的最优路径。\n可视化智能体在迷宫中的探索过程 (例如：绘制智能体路径、Q-Table 热力图等，可选)。\n\n代码框架:\n\n可以使用第一次课提供的 Grid World 环境代码 (或者小组自行搭建的 Grid World 环境)。\n小组需要自行编写 Q-Learning 算法代码，并与 Grid World 环境进行集成。\n\nAI 辅助工具:\n\n鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高开发效率。\n但强调: AI 工具是辅助手段，学生需要理解 Q-Learning 算法原理和代码逻辑，不能完全依赖 AI 工具生成代码，而忽略算法理解和调试。\n\n项目提交:\n\n小组提交完整的 Q-Learning 算法代码 (Python 文件)，以及修改后的 Grid World 环境代码 (如果环境代码有修改)。\n无需提交项目报告。\n\n\n\n\n2. 超参数讲解与调整 (学习率、折扣因子)\n\n\n\n\n\n\nNote\n\n\n\n超参数 (Hyperparameters) 是强化学习算法中需要手动设置的参数，例如学习率 (\\(\\alpha\\))、折扣因子 (\\(\\gamma\\))、探索率 (\\(\\epsilon\\)) 等。超参数的选择会直接影响算法的性能和收敛速度。\n\n\n\n常用超参数:\n\n学习率 (\\(\\alpha\\)): 已在第一次课中讲解。\n折扣因子 (\\(\\gamma\\)): 已在第一次课中讲解。\n探索率 (\\(\\epsilon\\)): \\(\\epsilon\\)-greedy 策略中的探索概率，已在第二次课中讲解。\n\n超参数调整:\n\n经验调整: 根据经验和直觉进行调整 (trial-and-error)。\n网格搜索 (Grid Search): 在超参数空间中，预先定义一组候选值，遍历所有可能的组合，训练模型并评估性能，选择性能最佳的超参数组合。\n随机搜索 (Random Search): 在超参数空间中，随机采样一定数量的超参数组合，训练模型并评估性能，选择性能最佳的超参数组合。\n自动化超参数优化方法: 例如 Bayesian Optimization, Hyperband 等 (更高级的方法，本课程不深入讲解)。\n\n超参数调整建议 (针对 Q-Learning 和 Grid World):\n\n学习率 (\\(\\alpha\\)): 可以尝试 0.1, 0.3, 0.5, 0.7 等值。\n折扣因子 (\\(\\gamma\\)): 可以尝试 0.8, 0.9, 0.95, 0.99 等值。\n探索率 (\\(\\epsilon\\)): 初始值可以设置为 1.0 (完全探索)，然后逐渐衰减到较小的值 (例如 0.1 或 0.01)。衰减方式可以是线性衰减、指数衰减等。\n\n超参数调整实践:\n\n在 Q-Learning 代码中，将超参数设置为可调节的变量。\n尝试不同的超参数组合，观察智能体在 Grid World 环境中的表现 (例如：是否能更快找到宝藏，是否能避免陷阱，平均 episode 奖励等)。\n记录实验结果，分析超参数对算法性能的影响。\n\n\n\n\n3. 小组项目一检查点：Q-Learning 智能体探索\n\n检查点目标: 确保学生小组能够运行基本的 Q-Learning 智能体，在迷宫环境 (Grid World) 中进行探索。\n检查内容:\n\nQ-Learning 算法代码是否能够正常运行，没有明显的 bug。\n智能体是否能够在迷宫环境中移动，并与环境进行交互。\nQ-Table 是否能够正常更新 (可以通过打印 Q-Table 或 Q 值变化来观察)。\n智能体是否能够进行初步的探索 (例如：在迷宫中随机移动，尝试不同的路径)。\n不需要智能体达到最优性能，重点是代码能够跑起来，并且智能体能够进行基本的探索。\n\n检查方式:\n\n小组演示: 每个小组简单演示 Q-Learning 智能体在 Grid World 环境中的运行情况。\n代码 review (可选): 教师可以抽查部分小组的代码，进行简单的代码 review，指出潜在问题和改进方向。\n答疑: 解答学生在 Q-Learning 算法编程和环境集成过程中遇到的问题。\n\n\n\n\n课后作业\n\n完成小组项目一：Q-Learning 算法编程实践，实现 Q-Learning 智能体在迷宫环境中寻宝。\n调整 Q-Learning 算法的超参数 (学习率、折扣因子、探索率等)，观察超参数对智能体性能的影响，并尝试找到一组较好的超参数组合。\n思考题:\n\nQ-Learning 算法有什么优点和缺点？\nQ-Learning 算法适用于什么类型的问题？有什么局限性？\n如何改进 Q-Learning 算法，以提高其性能和适用范围？ (例如：Double Q-Learning, Prioritized Experience Replay 等，可以查阅资料了解)\n\n\n\n\n预习资料\n\n阅读材料:\n\nQ-Learning 算法的改进版本：Double Q-Learning, Dueling Q-Network 等 (初步了解思想)。\nε-greedy 退火策略、UCB 算法等更高级的探索策略。\n动态定价的更多商业应用案例。\n\n视频资源:\n\nQ-Learning 算法代码实现详解 (Python)。\n超参数调整技巧和经验分享。\n强化学习算法的探索策略进阶。\n\n下周预习重点:\n\nQ-Learning 算法的优化和改进方向。\n探索策略的进一步探讨 (例如 ε-greedy 退火策略)。\nQ-Table 初始化、奖励函数设计等实用技巧。\n小组项目一提交和优秀项目讲解准备。\n\n\n\n请注意:\n\n代码框架: 可以使用第二次课提供的 Grid World 环境代码，或者小组自行搭建的环境。重点是自行编写 Q-Learning 算法代码，并与环境集成。\nAI 辅助工具: 鼓励使用 AI 工具，但务必强调理解算法原理和代码逻辑的重要性，避免过度依赖 AI 工具。\n小组项目: 小组项目一旨在让学生实践 Q-Learning 算法，解决迷宫寻宝问题，并初步了解超参数调整。为后续更复杂的强化学习算法和项目打下基础。\n检查点: 本次课设有小组项目一检查点，旨在及时发现学生在编程实践中遇到的问题，并提供指导和帮助，确保所有小组都能顺利进行后续的学习和项目。",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "课程目标",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#课程目标",
    "href": "week4.html#课程目标",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "了解 Q-Learning 算法的局限性，例如状态空间爆炸问题。\n掌握 ε-greedy 策略等探索策略，提升智能体的探索能力。\n学习 Q-Table 初始化、奖励函数设计等实用技巧，提升 Q-Learning 算法的性能。\n学习使用调试工具和 AI 工具，解决 Q-Learning 算法编程中遇到的问题。\n通过小组项目一的实践，巩固 Q-Learning 算法的理解和应用。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第一次课q-learning-算法优化与改进-小组项目一提交-优秀小组项目一讲解-3组",
    "href": "week4.html#第一次课q-learning-算法优化与改进-小组项目一提交-优秀小组项目一讲解-3组",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "第一次课：Q-Learning 算法优化与改进 / 小组项目一提交 / 优秀小组项目一讲解 (3组)",
    "text": "第一次课：Q-Learning 算法优化与改进 / 小组项目一提交 / 优秀小组项目一讲解 (3组)\n\n1. Q-Learning 算法的局限性\n\n\n\n\n\n\nNote\n\n\n\nQ-Learning 算法虽然简单有效，但也存在一些局限性，在实际应用中需要注意和改进。\n\n\n\n状态空间爆炸问题 (Curse of Dimensionality):\n\n当状态空间非常大或者连续时，Q-Table 的规模会变得非常庞大，难以存储和计算。\n例如，如果状态由多个离散特征组成，Q-Table 的大小会随着特征数量呈指数级增长。\n对于连续状态空间，Q-Table 无法直接应用，需要进行离散化或者使用函数逼近等方法。\n\n探索-利用困境 (Exploration-Exploitation Dilemma):\n\nQ-Learning 算法需要在探索 (Exploration) 和 利用 (Exploitation) 之间进行权衡。\n探索: 智能体尝试新的动作，发现新的状态和奖励，提高对环境的理解。\n利用: 智能体选择当前已知最优的动作，最大化累积奖励。\n\\(\\epsilon\\)-greedy 策略 是一种简单的平衡探索和利用的方法，但探索效率较低，可能浪费大量时间在探索不必要的区域。\n\n收敛性问题:\n\nQ-Learning 算法在满足一定条件下可以收敛到最优 Q 函数，例如：\n\n状态空间和动作空间是有限的。\n环境是马尔可夫决策过程 (MDP)。\n学习率 (\\(\\alpha\\)) 逐渐衰减到 0，但衰减速度不能太快。\n充分的探索，保证所有状态-动作对都被访问到足够多次。\n\n在实际应用中，环境可能不完全满足 MDP 假设，超参数调整不当，探索不足等因素都可能导致 Q-Learning 算法不收敛 或者 收敛到局部最优解。\n\n对超参数敏感:\n\nQ-Learning 算法的性能受到超参数 (学习率 \\(\\alpha\\), 折扣因子 \\(\\gamma\\), 探索率 \\(\\epsilon\\) 等) 的影响较大。\n超参数选择不当可能导致 学习速度慢、收敛性差、甚至 算法不稳定。\n手动调整超参数 需要经验和技巧，耗时耗力。\n\n\n\n\n2. 探索策略：\\(\\epsilon\\)-greedy 策略及改进\n\n\n\n\n\n\nNote\n\n\n\n探索策略 (Exploration Strategy) 决定了智能体在与环境交互时，如何选择动作，以平衡探索和利用。\n\n\n\n\\(\\epsilon\\)-greedy 策略回顾:\n\n以 \\(\\epsilon\\) 的概率 随机选择一个动作 (探索)。\n以 \\(1-\\epsilon\\) 的概率 选择当前 Q 值最大的动作 (利用)。\n优点: 简单易实现，应用广泛。\n缺点: 探索效率较低，随机探索可能探索到很多无用的状态和动作，浪费时间。\\(\\epsilon\\) 值通常固定不变，无法根据探索情况动态调整。\n\n\\(\\epsilon\\)-greedy 退火策略 (Epsilon-Greedy Annealing):\n\n思想: 在训练初期，增加探索，\\(\\epsilon\\) 值较大，鼓励智能体探索更多状态空间。随着训练的进行，逐渐减少探索，\\(\\epsilon\\) 值逐渐减小，更多地利用已学到的知识，选择最优动作。\n实现: 线性衰减、指数衰减 等方式。\n\n线性衰减: \\(\\epsilon = \\epsilon_{start} - \\frac{(\\epsilon_{start} - \\epsilon_{end})}{episodes} \\times episode\\)\n指数衰减: \\(\\epsilon = \\epsilon_{start} \\times decay\\_rate^{episode}\\)\n\n优点: 提高探索效率，在训练初期充分探索，后期逐渐收敛到最优策略。\n\n其他探索策略 (了解):\n\nUCB (Upper Confidence Bound) 算法: 在选择动作时，不仅考虑 Q 值的大小，还考虑 Q 值的不确定性 (例如：被访问次数较少的动作，Q 值不确定性较高，应该增加探索)。\nThompson Sampling 算法: 将 Q 值看作是随机变量，服从一定的概率分布。在选择动作时，从每个动作的 Q 值分布中采样，选择采样值最大的动作。\nSoftmax 策略 (Boltzmann 策略): 根据 Q 值的概率分布 来选择动作，Q 值越大，被选择的概率越高，但Q 值小的动作也有一定的概率被选择，保持一定的探索性。\n\n\n\n\n3. Q-Table 初始化技巧\n\n\n\n\n\n\nNote\n\n\n\nQ-Table 的初始化方式 会影响 Q-Learning 算法的学习效率和收敛性。\n\n\n\n初始化为 0:\n\n优点: 简单易实现，常用默认初始化方式。\n缺点: 可能导致初始探索不足，算法收敛速度慢。因为初始 Q 值都为 0，智能体可能倾向于停留在初始状态，难以开始探索。\n\n初始化为小的随机值:\n\n优点: 鼓励初始探索，打破对称性，加速学习。\n缺点: 随机值范围 需要谨慎选择，过大可能导致算法不稳定，过小则效果不明显。\n\n乐观初始化 (Optimistic Initialization):\n\n思想: 将 Q-Table 初始化为较大的值 (例如：奖励函数的最大可能值，或者一个较大的常数)。\n原理: 鼓励智能体探索。因为初始 Q 值较大，智能体会倾向于选择未探索过的动作，以期望获得更高的奖励。随着训练的进行，Q 值会逐渐更新为真实值。\n适用场景: 稀疏奖励环境 (奖励信号很少的环境)。\n\n基于领域知识的初始化:\n\n思想: 如果对问题领域有一定的先验知识，可以根据先验知识初始化 Q-Table。\n例如: 在迷宫寻宝问题中，如果已知宝藏的大概位置，可以将靠近宝藏的状态-动作对的 Q 值初始化为较大值，引导智能体更快地找到宝藏。\n优点: 加速学习，提高算法性能。\n缺点: 依赖于领域知识，通用性较差。\n\n\n\n\n4. 奖励函数设计技巧\n\n\n\n\n\n\nNote\n\n\n\n奖励函数 (Reward Function) 是强化学习算法的核心，直接决定了智能体的学习目标和行为策略。设计合理的奖励函数 至关重要。\n\n\n\n奖励函数的原则:\n\n与目标一致: 奖励函数应该准确反映任务的目标。智能体最大化累积奖励的过程，应该等价于完成任务的过程。\n稀疏奖励 vs. 密集奖励:\n\n稀疏奖励 (Sparse Reward): 只有达到目标状态或者完成任务时才有奖励，中间过程没有奖励。例如，迷宫寻宝问题，只有到达宝藏位置才有奖励，其他状态奖励为 0。\n密集奖励 (Dense Reward): 在中间过程也提供奖励，引导智能体逐步学习。例如，迷宫寻宝问题，可以根据智能体离宝藏的距离，设计一个与距离相关的奖励函数，距离越近奖励越大。\n选择: 稀疏奖励 更符合真实场景，但学习难度大，收敛速度慢。密集奖励 可以加速学习，但需要仔细设计，不当的密集奖励 可能引导智能体学习到非期望的策略。\n\n奖励的尺度 (Reward Shaping):\n\n奖励值的尺度 会影响算法的学习速度和稳定性。\n奖励值过大 可能导致 Q 值过大，算法不稳定。\n奖励值过小 可能导致 Q 值更新缓慢，学习速度慢。\n需要根据具体问题调整奖励值的尺度，通常需要进行实验和调参。\n\n\n奖励函数设计示例 (迷宫寻宝 Grid World):\n\n稀疏奖励:\n\n到达宝藏位置: +10\n撞墙或到达陷阱: -10\n其他状态: 0\n\n密集奖励 (示例 1，基于距离):\n\n到达宝藏位置: +10\n撞墙或到达陷阱: -10\n每走一步: -0.1 (鼓励尽快到达宝藏)\n根据当前位置与宝藏位置的距离，设计奖励值，距离越近奖励越大 (例如：负距离的倒数，或者高斯函数等)。\n\n密集奖励 (示例 2，分阶段奖励):\n\n到达宝藏位置: +10\n撞墙或到达陷阱: -10\n到达迷宫的某个关键位置 (checkpoint): +1 (引导智能体按特定路线探索)\n其他状态: -0.1 (每走一步的惩罚)\n\n\n奖励函数调试:\n\n可视化智能体的行为: 观察智能体在环境中的行为是否符合预期。\n分析 Q-Table: 检查 Q-Table 中的 Q 值是否合理，是否能反映状态-动作的价值。\n调整奖励函数: 根据观察和分析结果，迭代调整奖励函数，直到智能体学习到期望的策略。\n\n\n\n\n5. 小组项目一：迷宫寻宝 (Grid World) 提交 / 优秀小组项目一讲解 (3 组)\n\n小组项目一提交:\n\n请各小组在第一次课前，提交小组项目一：迷宫寻宝 (Grid World) 的 Q-Learning 算法代码 (Python 文件) 和 修改后的 Grid World 环境代码 (如有修改)。\n提交方式: [待定，例如：通过网络教学平台提交，或者发送到指定邮箱]。\n提交内容:\n\nQ-Learning 算法代码 (Python 文件，例如 q_learning_agent.py)\nGrid World 环境代码 (Python 文件，例如 grid_world.py，如有修改则提交，如无修改可不提交)\n简要说明: 小组分工，使用的 AI 辅助工具，遇到的问题及解决方案，超参数调整结果 (例如：最佳超参数组合，性能指标)。\n\n\n优秀小组项目一讲解 (3 组):\n\n从提交的项目中，挑选 3 组优秀小组，在课堂上进行项目讲解和演示。\n讲解内容:\n\n项目目标和实现思路\nQ-Learning 算法代码讲解 (重点)\nGrid World 环境演示\n超参数调整和实验结果\n项目亮点和创新之处\n遇到的问题和解决方案\n项目总结和反思\n\n讲解时间: 每组 15-20 分钟 (包括提问环节)。\n评分标准: 算法实现 (40%)，代码质量 (20%)，环境演示 (10%)，讲解清晰度 (20%)，创新性 (10%)。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第二次课小组项目一q-learning-算法优化与问题解决",
    "href": "week4.html#第二次课小组项目一q-learning-算法优化与问题解决",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "第二次课：小组项目一：Q-Learning 算法优化与问题解决",
    "text": "第二次课：小组项目一：Q-Learning 算法优化与问题解决\n\n1. 小组项目一：Q-Learning 算法优化与问题解决 (迷宫寻宝 Grid World)\n\n实践:\n\n各小组继续完善小组项目一：迷宫寻宝 (Grid World) 的 Q-Learning 算法代码，优化智能体在迷宫环境中的表现。\n优化方向:\n\n探索策略优化: 尝试使用 \\(\\epsilon\\)-greedy 退火策略 或其他更高级的探索策略 (UCB, Thompson Sampling 等，可选)。\nQ-Table 初始化优化: 尝试 乐观初始化 或 基于领域知识的初始化 (如果适用)。\n奖励函数优化: 调整奖励函数，尝试 密集奖励，引导智能体更快更有效地找到宝藏。\n超参数调整: 系统地调整超参数 (学习率 \\(\\alpha\\), 折扣因子 \\(\\gamma\\), 探索率 \\(\\epsilon\\) 等)，找到最佳超参数组合。\n代码效率优化: 提高代码运行效率，例如使用 NumPy 向量化计算，减少循环 等。\n可视化: 可视化智能体在迷宫中的探索过程 (例如：绘制智能体路径、Q-Table 热力图等，可选，作为加分项)。\n\n\n指导:\n\n教师巡回指导，解答学生在项目优化过程中遇到的问题。\n重点指导:\n\n调试技巧: 使用 print 语句、debug 工具 (例如 Python 的 pdb)，检查代码运行过程，定位 bug。\nAI 工具应用: 利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高调试效率。例如，使用 AI 工具快速生成测试代码、查找 bug、优化代码 等。\n问题解决思路: 引导学生分析问题，分解问题，逐步解决。例如，如果智能体无法找到宝藏，可以先检查环境代码是否正确，再检查 Q-Learning 算法代码是否逻辑正确，然后检查超参数是否合适，奖励函数是否合理 等。\n\n\n\n\n\n2. 小组项目进展分享与问题讨论\n\n小组分享:\n\n各小组轮流分享小组项目一的优化进展和遇到的问题。\n分享内容:\n\n优化目标: 例如，尝试了哪些优化策略，目标是提高哪些性能指标 (例如：平均 episode 奖励，成功率，收敛速度等)。\n优化方法: 例如，如何实现 \\(\\epsilon\\)-greedy 退火策略，如何调整奖励函数，如何使用 AI 工具进行调试等。\n优化结果: 展示优化后的实验结果，例如，绘制学习曲线 (episode 奖励 vs. episode)，对比优化前后的性能指标。\n遇到的问题: 详细描述遇到的问题，例如，代码 bug，算法不收敛，超参数调整困难等。\n解决方案: 分享已尝试的解决方案，以及是否有效。\n\n\n集体讨论与解答:\n\n教师组织学生进行集体讨论，共同解答各小组提出的问题。\n鼓励学生互相帮助，分享经验和技巧。\n教师进行总结和点评，指出常见问题和解决方法，提供进一步优化的建议。\n\n\n\n\n课后作业\n\n继续优化小组项目一：Q-Learning 算法编程实践，争取在迷宫环境中获得更好的寻宝效果。\n准备小组项目一的项目报告 (下周第一次课前提交)。项目报告模板和具体要求见下周通知。\n预习资料:\n\n阅读材料: Q-Learning 算法的改进版本：Double Q-Learning, Prioritized Experience Replay 等 (深入理解原理)。\n视频资源: Q-Learning 算法优化技巧详解，强化学习算法的调试方法。\n思考题: 如何将 Q-Learning 算法应用于更复杂的游戏或实际问题？Q-Learning 算法在哪些场景下会失效？如何解决？\n\n\n\n\n下周预习重点\n\n小组项目一项目报告提交。\n小组项目一项目答辩准备 (PPT 制作，内容组织，演示准备)。\n开始学习新的强化学习算法 (例如：Sarsa 算法)。\n了解深度强化学习 (Deep Reinforcement Learning) 的基本概念。\n\n\n请注意:\n\n小组项目一提交: 请各小组务必在本周第一次课前提交项目代码和简要说明。\n优秀项目讲解: 入选优秀项目讲解的小组，请认真准备讲解内容和演示，争取在课堂上展示最佳水平。\n项目优化: 鼓励各小组充分利用第二次课时间，积极进行项目优化和问题解决，争取在项目报告和答辩中取得优异成绩。\n互相学习: 鼓励各小组在项目分享和讨论环节，积极参与，互相学习，共同进步。",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  }
]