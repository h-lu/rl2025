[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "智能计算",
    "section": "",
    "text": "欢迎来到《智能计算》课程！\n本课程专为经济管理类学生设计，旨在帮助大家快速掌握强化学习的基本概念和应用技能，并结合 AI 辅助编程工具，提升学习效率和实践能力。\n通过本课程，你将：\n\n了解强化学习的核心思想和算法\n掌握使用 Python 和常用库进行强化学习编程\n能够应用强化学习解决实际商业问题\n\n让我们一起开始强化学习的探索之旅吧！\n前往课程大纲\n开始第一周学习",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>课程介绍</span>"
    ]
  },
  {
    "objectID": "syllbus.html",
    "href": "syllbus.html",
    "title": "课程大纲",
    "section": "",
    "text": "课程概述\n本课程为经管学生设计，旨在弱化数学理论和公式，侧重强化学习的实战应用，并结合 AI 辅助编程。课程共计 16 周，每周两次课，每次 90 分钟。课程以项目为中心，贯穿始终。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程目标",
    "href": "syllbus.html#课程目标",
    "title": "课程大纲",
    "section": "课程目标",
    "text": "课程目标\n完成本课程后，学生将能够：\n\n理解核心概念: 解释强化学习的核心概念，并区分其与监督学习和无监督学习的不同。\n掌握编程工具: 熟练使用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，以及 Gymnasium, Stable Baselines3 等 Python 库进行强化学习编程。\n应用经典算法: 运用 Q-Learning, DQN, PPO 等经典强化学习算法，解决至少两种不同类型的商业问题 (例如：动态定价、推荐系统)。\n团队协作: 有效地进行团队合作，完成五个具有实际商业应用价值的强化学习小组项目，并撰写项目报告。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程结构",
    "href": "syllbus.html#课程结构",
    "title": "课程大纲",
    "section": "课程结构",
    "text": "课程结构\n课程分为四个阶段，循序渐进地引导学生从理论到实践，最终完成小组项目，并拓展到更广泛的商业应用场景和伦理考量。\n\n阶段一 (1-2周): 强化学习入门\n\n目标：理解基本概念，体验简单环境。\n\n阶段二 (3-8周): 经典算法实践\n\n目标：掌握 Q-Learning 和深度 Q 网络 (DQN) 算法，解决中等难度问题。\n\n阶段三 (9-13周): 高级算法探索\n\n目标：了解策略梯度方法，尝试更复杂的环境和算法。\n\n阶段四 (14-16周): 小组项目实战与商业应用拓展\n\n目标：团队合作，应用强化学习解决实际商业问题，并拓展到更广泛的商业应用场景和伦理考量。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#每周课程安排",
    "href": "syllbus.html#每周课程安排",
    "title": "课程大纲",
    "section": "每周课程安排",
    "text": "每周课程安排\n以下是详细的每周课程内容和项目安排，每次课程 90 分钟。\n\n第一阶段 (1-2周): 强化学习入门\n\n第1周 (共2次课)\n\n第一次课: 什么是强化学习？\n\n内容：\n\n讲解强化学习的核心概念：智能体 (Agent)、环境 (Environment)、动作 (Action)、状态 (State)、奖励 (Reward)。\n对比强化学习与监督学习、无监督学习。\n展示强化学习在商业领域的应用案例 (例如：推荐系统、动态定价、库存管理)。\n\n\n第二次课: AI 辅助编程入门与平衡杆 (CartPole) 初体验\n\n内容：\n\n介绍并安装 AI 辅助编程工具 (例如：GitHub Copilot, Tabnine)。\n演示如何使用 AI 工具辅助编写 Python 代码，并针对平衡杆游戏 (CartPole) 进行初步的演示和代码演练。\n介绍强化学习常用 Python 库 (例如：Gymnasium, Stable Baselines3)，强调其易用性和对初学者的友好性。\n\n实践：\n\n引导学生使用 AI 工具和库，体验平衡杆 (CartPole) 环境。\n完成简单的 Python 编程练习，为后续强化学习编程打基础。\n\n\n\n\n\n第2周 (共2次课)\n\n第一次课: 强化学习框架与迷宫环境 (Grid World) 搭建\n\n内容：\n\n解释马尔可夫决策过程 (MDP) 的基本思想 (无需深入数学细节，侧重直观理解)。\n讲解策略 (Policy)、价值函数 (Value Function, 包括 V 函数和 Q 函数) 的概念，强调直观理解。\n探讨探索 (Exploration) 与利用 (Exploitation) 的平衡问题，用商业案例 (例如：新市场尝试 vs. 现有市场深耕) 进行类比。\n\n实践：\n\n详细讲解如何使用 Gymnasium 库搭建迷宫环境 (Grid World)。\n\n\n第二次课: 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n内容：\n\n提供迷宫环境 (Grid World) 的代码框架，并指导学生使用 AI 工具进行代码补全和修改。\n\n实践：\n\n学生以小组为单位，搭建迷宫环境 (Grid World)。\n\n答疑：\n\n解答学生在环境搭建过程中遇到的问题，确保所有小组都能成功搭建迷宫环境 (Grid World)。\n布置小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n\n\n\n\n\n\n第二阶段 (3-8周): 经典算法实践\n\n第3周 (共2次课)\n\n第一次课: Q-Learning 算法详解\n\n内容：\n\n直观解释 Q-Table 和 Q-Learning 的更新规则 (避免复杂的数学公式，用表格和例子说明)。\n梳理 Q-Learning 算法的步骤流程。\n结合动态定价案例，演示如何使用 Q-Learning 算法解决简单动态定价问题 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目一：Q-Learning 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Q-Learning 算法代码，应用于迷宫寻宝 (Grid World) 项目。\n\n内容：\n\n讲解超参数 (学习率、折扣因子) 的作用，以及如何进行简单的参数调整 (直观理解)。\n\n检查点：\n\n小组项目一检查点：确保学生小组能够运行基本的 Q-Learning 智能体，在迷宫环境 (Grid World) 中进行探索。\n\n\n\n\n\n第4周 (共2次课)\n\n第一次课: Q-Learning 算法优化与改进 / 小组项目一提交 / 优秀小组项目一讲解 (3组)\n\n内容：\n\n讨论 Q-Learning 算法的局限性 (例如：状态空间爆炸问题)。\n介绍 ε-greedy 策略等探索策略，提升智能体的探索能力。\n讲解 Q-Table 初始化、奖励函数设计等实用技巧，提升 Q-Learning 算法的性能。\n\n管理：\n\n小组项目一：迷宫寻宝 (Grid World) 提交 (第 4 周第一次课前)。\n优秀小组项目一讲解 (3 组)。\n\n\n第二次课: 小组项目一：Q-Learning 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 Q-Learning 算法代码，优化智能体在迷宫环境 (Grid World) 中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第5周 (共2次课)\n\n第一次课: 深度学习的引入 - 深度 Q 网络 (DQN) 初探\n\n内容：\n\n解释 Q-Learning 算法在复杂问题上的瓶颈 (状态空间爆炸)。\n引入深度学习 (神经网络) 的基本概念 (无需深入数学细节，强调神经网络的表示能力)。\n讲解深度 Q 网络 (DQN) 的基本原理，如何使用神经网络近似 Q 函数。\n对比 DQN 与 Q-Learning 算法的异同。\n\n\n第二次课: DQN 算法关键技术 - 经验回放与目标网络\n\n内容：\n\n深入讲解 DQN 算法的两个关键技术：经验回放 (Experience Replay) 和目标网络 (Target Network) 的作用和原理。\n讲解如何使用经验回放和目标网络来提升 DQN 算法的稳定性和性能。\n\n代码：\n\n演示如何在 DQN 算法代码中加入经验回放和目标网络 (使用 AI 辅助编程)。\n\n\n\n\n\n第6周 (共2次课)\n\n第一次课: DQN 算法详解 / 小组项目二提交 / 优秀小组项目二讲解 (3组)\n\n内容：\n\n梳理 DQN 算法的完整步骤流程。\n结合平衡杆 (CartPole) 案例，演示如何使用 DQN 算法解决平衡杆问题 (使用 AI 辅助编程)。\n\n管理：\n\n小组项目二：平衡杆 (CartPole) 提交 (第 6 周第一次课前)。\n优秀小组项目二讲解 (3 组)。\n\n\n第二次课: 小组项目二：DQN 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 DQN 算法代码，应用于平衡杆 (CartPole) 项目。\n\n内容：\n\n讲解神经网络结构设计 (层数、神经元数量) 的基本原则 (无需深入理论，强调试错和调参)。\n\n检查点：\n\n小组项目二检查点：确保学生小组能够运行基本的 DQN 智能体，在平衡杆 (CartPole) 环境中进行探索。\n\n\n\n\n\n第7周 (共2次课)\n\n第一次课: DQN 算法改进与调优\n\n内容：\n\n讨论 DQN 算法的改进方向 (例如：Double DQN, Dueling DQN 等)。\n探讨不同的探索策略 (例如：ε-greedy 退火策略、Noisy Networks 等)，进一步提升智能体的探索能力。\n讲解 DQN 算法的调参技巧 (学习率、batch size、网络结构等)，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n\n第二次课: 小组项目二：DQN 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 DQN 算法代码，优化智能体在平衡杆 (CartPole) 环境中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第8周 (共2次课)\n\n第一次课: 策略梯度方法 - Policy Gradient 算法初步 / 小组项目三提交 / 优秀小组项目三讲解 (3组)\n\n内容：\n\n介绍策略梯度 (Policy Gradient) 方法的基本思想 (直接优化策略)。\n讲解 Policy Gradient 算法 (例如：REINFORCE 算法) 的基本原理和流程 (无需深入数学推导，侧重直观理解)。\n对比策略梯度方法与价值方法 (Q-Learning, DQN) 的异同。\n\n管理：\n\n小组项目三：资源分配 模拟环境 提交 (第 8 周第一次课前)。\n优秀小组项目三讲解 (3 组)。\n\n\n第二次课: 小组项目三：Policy Gradient 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Policy Gradient 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讨论策略梯度方法中的探索问题，以及如何进行有效的探索。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 Policy Gradient 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n\n第三阶段 (9-13周): 高级算法探索\n\n第9周 (共2次课)\n\n第一次课: Actor-Critic 算法 - A2C 算法详解\n\n内容：\n\n讲解 Actor-Critic 算法的基本框架 (结合策略梯度和价值方法)。\n深入讲解 A2C (Advantage Actor-Critic) 算法的原理和优势 (例如：更稳定、更高效)。\n\n代码：\n\n演示 A2C 算法的代码实现 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目三：A2C 算法优化与改进\n\n内容：\n\n讨论 A2C 算法的改进方向 (例如：GAE-优势函数估计)。\n讲解 A2C 算法的调参技巧，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n实践：\n\n指导学生尝试改进 A2C 算法，提升算法性能。\n\n\n\n\n\n第10周 (共2次课)\n\n第一次课: 近端策略优化 (PPO) 算法 - PPO 算法详解\n\n内容：\n\n讲解近端策略优化 (PPO) 算法的原理和优势 (例如：性能稳定、易于调参)。\n深入讲解 PPO 算法的核心机制：Clipping 和 PPO-Penalty。\n对比 PPO 算法与 A2C 算法的异同。\n\n\n第二次课: 小组项目三：PPO 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 PPO 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讲解 PPO 算法的关键超参数 (例如：clip ratio, value function coefficient 等) 的作用，以及如何进行参数调整。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 PPO 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n第11周 (共2次课)\n\n第一次课: PPO 算法的改进与应用\n\n内容：\n\n讨论 PPO 算法的改进方向 (例如：PPO-Clip, PPO-Penalty 的选择)。\n探讨 PPO 算法在不同商业场景中的应用案例 (例如：推荐系统、动态定价、机器人控制等)。\n介绍 PPO 算法的变体和前沿研究方向。\n\n\n第二次课: 小组项目三：PPO 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 PPO 算法代码，优化智能体在资源分配模拟环境中的表现，并尝试应用更高级的探索策略。\n\n指导：\n\n指导学生解决在更复杂算法和环境中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第12周 (共2次课)\n\n第一次课: 强化学习前沿技术与发展趋势 / 小组项目四提交 / 优秀小组项目四讲解 (3组)\n\n内容：\n\n介绍强化学习领域的前沿技术和发展趋势，例如：模仿学习 (Imitation Learning)、逆强化学习 (Inverse Reinforcement Learning)、分层强化学习 (Hierarchical Reinforcement Learning) 等 (简单介绍思想，激发学生兴趣)。\n展望强化学习未来的发展方向和潜在突破。\n引导学生思考强化学习技术可能对商业和社会带来的变革。\n\n管理：\n\n小组项目四：商业案例应用 提交 (第 12 周第一次课前)。\n优秀小组项目四讲解 (3 组)。\n\n\n第二次课: 小组项目四：商业案例应用 项目实践与问题解决\n\n实践：\n\n学生小组进行小组项目四：商业案例应用 的项目实践，进行环境搭建、算法选择、代码编写和实验验证。\n\n指导：\n\n指导学生解决在项目实践中遇到的问题。\n\n讨论：\n\n鼓励学生小组之间交流项目进展和经验，互相学习，共同进步。\n\n\n\n\n\n第13周 (共2次课)\n\n第一次课: 小组项目四成果展示 / 小组综合项目布置与答疑\n\n展示：\n\n小组项目四成果展示 (部分小组，时间允许的情况下，可以放在本次课，也可以放在第 12 周第二次课，根据实际情况灵活安排)。\n\n内容：\n\n布置小组综合项目：开放式商业问题，明确项目要求、评分标准和提交时间。\n\n答疑：\n\n针对小组综合项目进行初步答疑，解答学生关于项目选题、方向等问题。\n\n\n第二次课: 小组综合项目 - 开放式商业问题介绍与选题\n\n内容：\n\n详细介绍小组综合项目：开放式商业问题，鼓励学生结合课程所学知识和技能，以及自身专业背景和兴趣，选择具有实际商业价值和应用前景的项目。\n\n指导：\n\n指导学生小组进行小组综合项目选题，鼓励创新性和个性化。\n\n分组：\n\n学生维持小组队伍，开始讨论小组综合项目选题和分工。\n\n\n\n\n\n\n第四阶段 (14-16周): 小组项目实战与商业应用拓展\n\n第14-15周 (共4次课): 小组综合项目开发与指导\n\n内容：\n\n学生以小组为单位，进行小组综合项目的设计、开发和实验。\n\n指导：\n\n教师在课堂上提供项目指导和技术支持，解答学生在项目开发过程中遇到的问题。\n\n协作：\n\n鼓励学生充分利用 AI 辅助编程工具，提高开发效率，加强团队协作。\n\n检查：\n\n第 15 周进行小组综合项目中期检查，了解项目进展，及时发现和解决问题。\n\n\n\n\n第15周 (共2次课)\n\n第一次课: (内容待定)\n第二次课: 小组综合项目准备与答疑\n\n内容：\n\n学生小组继续进行小组综合项目的开发和完善，准备项目展示材料。\n\n答疑：\n\n教师提供项目答疑和指导，帮助学生解决项目开发中遇到的最后问题。\n\n\n\n\n\n第16周 (共2次课): 小组综合项目展示与总结\n\n第一次课: 小组综合项目展示 / 小组综合项目提交 / 小组综合项目展示 (全部小组)\n\n展示：\n\n各小组进行项目成果展示，讲解项目背景、问题定义、算法选择、实验结果和商业价值。\n\n答辩：\n\n接受教师和同学的提问和点评。\n\n管理：\n\n小组综合项目：开放式商业问题 提交 (第 16 周第一次课前)。\n\n\n第二次课: 课程总结与未来展望\n\n总结：\n\n回顾课程内容，总结强化学习的核心概念、算法和应用。\n\n展望：\n\n展望强化学习在商业领域的未来发展前景，鼓励学生继续深入学习和探索。\n\n伦理讨论：\n\n探讨强化学习在商业应用中可能引发的伦理问题，例如：算法歧视、数据隐私、自动化对就业的影响等，培养学生的社会责任感和批判性思维。\n\n评估：\n\n进行课程评估和反馈，收集学生对课程的意见和建议，为课程改进提供参考。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#项目设置-贯穿课程",
    "href": "syllbus.html#项目设置-贯穿课程",
    "title": "课程大纲",
    "section": "项目设置 (贯穿课程)",
    "text": "项目设置 (贯穿课程)\n\n小组项目 (共5个)\n\n小组项目一 (迷宫寻宝 Grid World)\n\n学习目标：掌握 Q-Learning 算法的原理和基本实现，理解探索与利用的概念。\n周期：2 周\n提交时间：第 4 周第一次课前\n\n小组项目二 (平衡杆 CartPole)\n\n学习目标：掌握 DQN 算法的原理和使用，理解经验回放和目标网络的作用，初步接触神经网络。\n周期：2 周\n提交时间：第 6 周第一次课前\n\n小组项目三 (资源分配 模拟环境)\n\n学习目标：掌握策略梯度方法 (PPO) 的原理和应用，理解策略梯度与价值方法的区别，学习处理连续动作空间问题。\n周期：2 周\n提交时间：第 8 周第一次课前\n\n小组项目四 (商业案例应用)\n\n学习目标：综合运用所学强化学习算法和工具，解决实际商业问题，提升问题建模和方案设计能力。\n周期：2 周\n提交时间：第 12 周第一次课前\n\n小组综合项目 (开放式商业问题)\n\n学习目标：培养学生的团队合作、项目管理和实际问题解决能力，提升综合应用强化学习知识的能力。\n周期：3 周\n提交时间：第 16 周第一次课前\n\n小组人数：建议每组 2-3 人。\n项目形式: 小组合作完成，包括项目方案设计、环境搭建、算法实现、实验验证、结果分析和项目报告。 每次小组项目提交后，选取 3 个优秀小组在课堂上进行项目讲解和展示，分享经验和成果。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#ai-辅助编程工具",
    "href": "syllbus.html#ai-辅助编程工具",
    "title": "课程大纲",
    "section": "AI 辅助编程工具",
    "text": "AI 辅助编程工具\n\n代码辅助: GitHub Copilot, Tabnine 等\n\n功能：代码自动补全、代码片段生成、代码错误检查等，提高编程效率，降低编程难度。\n\n强化学习库: Gymnasium (原 OpenAI Gym)\n\n功能：提供各种强化学习环境，方便学生进行算法测试和验证。\n\n算法库: Stable Baselines3\n\n功能：封装了多种常用的强化学习算法 (Q-Learning, DQN, PPO 等)，易于使用，方便学生快速上手。\n\n可视化工具: TensorBoard 等\n\n功能：方便学生监控强化学习训练过程，分析实验结果，调试算法。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#考核方式",
    "href": "syllbus.html#考核方式",
    "title": "课程大纲",
    "section": "考核方式",
    "text": "考核方式\n\n小组项目一 ~ 四 (40%): 根据项目完成质量、代码质量、实验结果和项目报告进行评分，平均分配到四个项目。\n小组综合项目 (20%): 根据项目创新性、实用性、完成度、展示效果和项目报告进行评分 (小组内成员评分可以考虑组内互评和贡献度)。\n课堂参与 (10%): 根据课堂讨论、提问、作业完成情况等进行评分，鼓励学生积极参与课堂互动。\n期末考试 (30%): 期末考试采用闭卷形式，重点考察学生对强化学习基础理论的掌握程度。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "第一次课：什么是强化学习？",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第一次课什么是强化学习",
    "href": "week1.html#第一次课什么是强化学习",
    "title": "第一周：强化学习入门",
    "section": "强化学习基本流程",
    "text": "核心定义\n\n\n\n强化学习是一种通过与环境交互来学习的机器学习方法。智能体通过不断尝试和犯错来学习最优策略。\n\n\n\n1. 强化学习的核心概念\n\n\n\n\n\n\n以电商推荐系统为例理解核心概念\n\n\n\n\n智能体 (Agent)\n\n负责学习和做出决策的实体\n例如：推荐算法就是一个智能体，它决定向用户推荐什么商品\n\n\n\n环境 (Environment)\n\n智能体所处的外部世界\n例如：用户群体、商品库、市场状况等\n环境会对智能体的动作做出响应\n\n\n\n状态 (State)\n\n环境在某一时刻的描述\n例如：用户的历史浏览记录、搜索关键词、当前页面停留时间等\n智能体根据状态来决定下一步动作\n\n\n\n动作 (Action)\n\n智能体可以采取的操作\n例如：推荐特定商品、调整商品展示顺序、发送个性化优惠券等\n\n\n\n奖励 (Reward)\n\n环境对智能体动作的反馈\n例如：用户点击推荐商品(正奖励)、直接关闭页面(负奖励)\n奖励信号指导智能体改进决策\n\n\n\n\n\n\n\n\n\n\n强化学习交互过程可视化\n\n\n\n\n\n\n强化学习交互过程\n\n\n\n强化学习基本流程\n\n智能体观察环境，获取当前状态\n基于当前状态和策略选择一个动作\n执行动作，与环境交互\n环境转移到新状态，并给予智能体奖励\n智能体根据获得的奖励更新其策略\n重复上述过程，直到学习收敛或达到终止条件\n\n\n\n\n\n\n2. 机器学习方法对比\n\n\n\n\n\n\n学习方法对比\n\n\n\n\n\n\n机器学习方法对比\n\n\n\n\n\n\n\n\n\n\n三种学习方法的本质差异\n\n\n\n监督学习：有”老师”（标签）直接告诉模型正确答案，模型通过对比其预测与正确答案之间的差距来学习。\n无监督学习：没有”老师”，模型需要自己从数据中寻找规律和模式，不依赖外部反馈。\n强化学习：有”评价者”而非”老师”，评价者只告诉模型其行为的好坏（奖励信号），但不直接告诉正确答案，模型需要通过尝试和探索来找到获取最大奖励的策略。\n\n\n\n\n3. 商业应用案例详解\n\n\n\n\n\n\n智能推荐系统\n\n\n\n\n应用场景：电商平台、内容平台\n实现方式：\n\n收集用户行为数据作为状态\n推荐算法作为智能体\n用户反馈作为奖励信号\n\n商业价值：\n\n提高用户转化率\n增加平台收入\n改善用户体验\n\n\n\n\n\n\n\n\n\n\n动态定价系统\n\n\n\n\n应用场景：酒店预订、网约车平台\n实现方式：\n\n市场供需作为状态\n价格调整作为动作\n成交量和收入作为奖励\n\n商业价值：\n\n优化收入管理\n平衡供需关系\n提高资源利用率",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#强化学习基本流程",
    "href": "week1.html#强化学习基本流程",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "智能体观察环境，获取当前状态\n基于当前状态和策略选择一个动作\n执行动作，与环境交互\n环境转移到新状态，并给予智能体奖励\n智能体根据获得的奖励更新其策略\n重复上述过程，直到学习收敛或达到终止条件",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "href": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "title": "第一周：强化学习入门",
    "section": "第二次课：AI辅助编程入门与平衡杆实践",
    "text": "第二次课：AI辅助编程入门与平衡杆实践\n\n1. AI辅助编程工具入门\n\n\n\n\n\n\n注意事项\n\n\n\nAI辅助编程工具可以显著提高编程效率，但要注意理解生成的代码逻辑\n\n\n\n\n\n\n\n\nGitHub Copilot 安装步骤\n\n\n\n\n打开 VS Code\n转到扩展市场\n搜索 “GitHub Copilot”\n点击安装\n\n\n\n\n\n\n\n\n\n实用技巧\n\n\n\n\n使用清晰的注释引导生成\n分步骤编写复杂逻辑\n及时检查生成的代码\n理解并修改不合适的建议\n\n\n\n\n\n2. 强化学习环境搭建\n\n\n\n\n\n\nGymnasium 库基础使用\n\n\n\n# 安装 Gymnasium\npip install gymnasium\n\n# 基本使用示例\nimport gymnasium as gym\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 获取环境信息\nprint(f\"动作空间: {env.action_space}\")\nprint(f\"状态空间: {env.observation_space}\")\n\n\n\n\n\n\n\n\nStable Baselines3 库基础使用\n\n\n\n# 安装 Stable Baselines3\npip install stable-baselines3\n\n# 基本使用示例\nfrom stable_baselines3 import DQN\n\n# 创建模型\nmodel = DQN(\"MlpPolicy\", env, verbose=1)\n\n# 训练模型\nmodel.learn(total_timesteps=10000)\n\n\n\n\n3. CartPole 环境实践\n\n\n\n\n\n\nCartPole 环境说明\n\n\n\nCartPole 是强化学习入门的经典环境，目标是通过左右移动小车来保持杆子平衡\n\n\n\n\n\n\n\n\nCartPole 环境可视化\n\n\n\n\n\n\nCartPole环境\n\n\n\n\n\n\n\n\n\n\n环境参数\n\n\n\nimport gymnasium as gym\nimport numpy as np\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 环境参数说明\nprint(\"状态空间包含4个连续值:\")\nprint(\"- 小车位置: [-4.8, 4.8]\")\nprint(\"- 小车速度: [-∞, ∞]\")\nprint(\"- 杆子角度: [-0.418, 0.418]\")\nprint(\"- 杆子角速度: [-∞, ∞]\")\n\nprint(\"\\n动作空间包含2个离散动作:\")\nprint(\"- 0: 向左推\")\nprint(\"- 1: 向右推\")\n\n\n\n\n\n\n\n\n随机动作示例代码\n\n\n\n# 运行随机策略\nobservation, info = env.reset()\ntotal_reward = 0\n\nfor step in range(100):\n    # 随机选择动作\n    action = env.action_space.sample()\n    \n    # 执行动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    total_reward += reward\n    \n    # 打印当前状态\n    print(f\"Step {step}: State = {observation}, Reward = {reward}\")\n    \n    # 判断是否需要重置\n    if terminated or truncated:\n        print(f\"Episode finished after {step + 1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()\n\n\n\n\n\n\n\n\n简单规则策略实现示例\n\n\n\n对于CartPole任务，我们可以实现一个基于当前状态的简单规则策略，而不是随机选择动作：\ndef simple_rule_policy(observation):\n    \"\"\"\n    基于杆子角度和角速度的简单规则策略\n    \n    参数：\n        observation: 环境观测值，包含4个状态变量\n        \n    返回：\n        action: 0(向左推)或1(向右推)\n    \"\"\"\n    # 解析观测值\n    cart_position, cart_velocity, pole_angle, pole_velocity = observation\n    \n    # 如果杆子正在向右倾斜，则向右推车（让杆子回到中心）\n    if pole_angle &gt; 0:\n        return 1\n    # 如果杆子正在向左倾斜，则向左推车\n    else:\n        return 0\n\n# 使用规则策略运行环境\nobservation, info = env.reset()\ntotal_reward = 0\n\nfor step in range(1000):  # 设置更大的步数上限\n    # 使用规则策略选择动作\n    action = simple_rule_policy(observation)\n    \n    # 执行动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    total_reward += reward\n    \n    # 判断是否需要重置\n    if terminated or truncated:\n        print(f\"Episode finished after {step + 1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()\n这个简单的规则策略基于杆子的倾斜角度来决定将小车向哪个方向推动，虽然简单，但通常比随机策略能获得更好的性能。在强化学习中，我们的目标是通过学习来自动发现这样的策略，甚至找到更好的策略。\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n环境配置\n\n安装 Python 3.8+\n配置 VS Code 和 GitHub Copilot\n安装所需 Python 库\n\n编程练习\n\n修改随机动作示例，尝试实现简单的规则策略\n记录并分析不同策略的表现\n使用 AI 辅助工具优化代码\n\n思考题\n\n强化学习如何应用到你的专业领域？\nCartPole 环境中的状态空间设计有什么特点？\n为什么需要重置环境？\n\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料\n\nGymnasium 官方文档\nStable Baselines3 入门教程\n马尔可夫决策过程基础概念\n\n视频资源\n\nCartPole 环境详解\n强化学习基础概念讲解\n\n下周预习重点\n\n马尔可夫决策过程\n价值函数与策略函数\nGrid World 环境介绍",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "第一次课：强化学习框架与迷宫环境 (Grid World) 搭建",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "href": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "马尔可夫决策过程 (MDP)\n\n\n\n马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习的核心框架，用于形式化描述智能体与环境的交互过程。\n\n\n\n\n\n\n\n\nMDP 的核心要素\n\n\n\n\n状态 (State, S)\n\n环境的描述，包含了智能体做出决策所需的信息\n马尔可夫性质: 当前状态包含了所有历史信息，未来的状态只依赖于当前状态和动作，而与过去的历史无关\n在迷宫环境中，状态可以是智能体在迷宫中的位置坐标\n\n\n\n动作 (Action, A)\n\n智能体在每个状态下可以采取的行为\n在迷宫环境中，动作可以是向上、下、左、右移动\n\n\n\n转移概率 (Transition Probability, P)\n\n智能体在状态 \\(s\\) 采取动作 \\(a\\) 后，转移到下一个状态 \\(s'\\) 的概率\n\\(P(s'|s, a)\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 的概率\n在确定性迷宫环境中，转移概率是确定的；在非确定性环境中可能存在随机性\n\n\n\n奖励 (Reward, R)\n\n智能体在与环境交互后获得的反馈信号，用于评价动作的好坏\n\\(R(s, a, s')\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 后获得的奖励\n在迷宫寻宝游戏中，到达宝藏获得正奖励，撞墙或陷阱获得负奖励\n\n\n\n策略 (Policy, \\(\\pi\\))\n\n智能体根据当前状态选择动作的规则，可以是确定性的或随机性的\n\\(\\pi(a|s)\\) 表示在状态 \\(s\\) 下选择动作 \\(a\\) 的概率\n强化学习的目标是学习最优策略，使得智能体获得最大的累积奖励\n\n\n\n\n\n\n\n\n\n\nMDP 过程示意图\n\n\n\n\n\n\n马尔可夫决策过程\n\n\n\n\n\n\n\n\n\n\n价值函数 (Value Function)\n\n\n\n价值函数用于评估在给定状态或状态-动作对下，未来预期累积奖励的期望值。价值函数是强化学习算法的核心概念之一。\n\nV 函数 (State Value Function)\n\n\\(V_{\\pi}(s)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，未来可以获得的期望累积奖励\nV 函数评估的是状态的价值，即处于某个状态的好坏程度\nV 函数越大，表示当前状态越好，未来可以获得的期望奖励越高\n\n\n\nQ 函数 (Action Value Function)\n\n\\(Q_{\\pi}(s, a)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，选择动作 \\(a\\) 后的期望累积奖励\nQ 函数评估的是状态-动作对的价值\nQ 函数越大，表示在当前状态下，该动作越好\n\n\n\nV 函数和 Q 函数的关系\n\\(V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) Q_{\\pi}(s, a)\\)\n\n\n\n\n\n\n\n\n\n价值函数的数学表达\n\n\n\n\nV 函数的数学定义\n\\[V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s \\right]\\]\n\n\\(\\gamma\\) 是折扣因子，取值范围 [0, 1]\n\\(R_t\\) 是在时间步 t 获得的奖励\n\\(\\mathbb{E}_{\\pi}\\) 表示在策略 \\(\\pi\\) 下的期望\n\n\n\nQ 函数的数学定义\n\\[Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a \\right]\\]\n\n\n贝尔曼方程\n贝尔曼方程是强化学习中的核心等式，描述了当前状态的价值与下一个状态价值之间的关系：\nV 函数的贝尔曼方程： \\[V_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]\\]\nQ 函数的贝尔曼方程： \\[Q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q_{\\pi}(s',a')]\\]\n\n\n最优价值函数\n强化学习的目标是找到最优策略，对应的最优价值函数定义为：\n最优状态价值函数： \\[V^*(s) = \\max_{\\pi} V_{\\pi}(s) = \\max_{a} Q^*(s,a)\\]\n最优动作价值函数： \\[Q^*(s,a) = \\max_{\\pi} Q_{\\pi}(s,a) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1},a') | S_t=s, A_t=a\\right]\\]\n如果我们知道最优价值函数，最优策略可以通过选择每个状态下价值最高的动作得到： \\[\\pi^*(s) = \\arg\\max_{a} Q^*(s,a)\\]\n\n\n\n\n\n\n\n\n\n探索与利用的平衡\n\n\n\n探索 (Exploration) 与利用 (Exploitation) 是强化学习中一个核心的权衡问题：\n\n探索 (Exploration)\n\n尝试新的动作，探索未知的状态和动作空间\n可能发现更优策略，但也可能浪费时间在无用区域\n\n\n\n利用 (Exploitation)\n\n根据已知经验选择当前最优动作\n保证稳定收益，但可能错过更好的策略\n\n\n\n\\(\\epsilon\\)-greedy 策略\n\n以概率 \\(\\epsilon\\) 随机选择动作 (探索)\n以概率 \\(1-\\epsilon\\) 选择当前最优动作 (利用)\n\\(\\epsilon\\) 随训练进行逐渐减小\n\n\n\n\n\n\n\n\n\n\n探索与利用的平衡图示\n\n\n\n\n\n\n探索与利用的平衡\n\n\n\n\n\n\n\n\n\n\n商业案例：新市场尝试 vs. 现有市场深耕\n\n\n\n\n新市场尝试 (探索)\n\n进入新的市场领域\n开发新的产品线\n拓展新的客户群体\n\n\n\n现有市场深耕 (利用)\n\n优化现有产品\n提高客户满意度\n提升市场份额\n\n\n\n平衡策略\n\n双元策略：同时进行探索和利用\n阶段性策略：不同发展阶段侧重不同策略\n\n\n\n\n\n\n\n\n\n\nGrid World 环境搭建\n\n\n\n\nGymnasium 基础使用\n# 安装 Gymnasium\n# pip install gymnasium\n\nimport gymnasium as gym\n\n# 创建 CartPole 环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 重置环境\nobservation, info = env.reset()\n\n# 随机选择动作\naction = env.action_space.sample()\n\n# 执行动作\nobservation, reward, terminated, truncated, info = env.step(action)\n\n# 关闭环境\nenv.close()\n\n\n自定义 Grid World 环境\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass GridWorldEnv(gym.Env):\n    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n\n    def __init__(self, render_mode=None, size=5):\n        super().__init__()\n        self.size = size  # Grid world size\n        self.window_size = 512  # PyGame window size\n\n        # 动作空间：上下左右\n        self.action_space = spaces.Discrete(4)\n        # 观测空间：智能体位置\n        self.observation_space = spaces.Discrete(size * size)\n\n        # Grid world 地图\n        self._grid_map = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 2, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0]\n        ])\n\n        # 初始化位置信息\n        self._target_location = np.array([2, 2])\n        self._trap_location = np.array([4, 4])\n        self._agent_start_location = np.array([0, 0])\n        self._agent_location = np.array([0, 0])\n\n        self.render_mode = render_mode\n\n        # 初始化渲染\n        if render_mode is not None:\n            import pygame\n            pygame.init()\n            pygame.font.init()\n            self.window = pygame.display.set_mode(\n                (self.window_size, self.window_size)\n            )\n            self.clock = pygame.time.Clock()\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完成 Grid World 环境的其他方法实现：\n\nstep() 方法：处理动作执行和奖励计算\nrender() 方法：可视化当前环境状态\nclose() 方法：清理资源\n\n实现一个简单的策略：\n\n使用 \\(\\epsilon\\)-greedy 策略\n记录并分析智能体表现\n\n思考题：\n\nMDP 的马尔可夫性质在实际应用中是否总是成立？\n如何在你的专业领域应用探索与利用的概念？\nGrid World 环境中，不同的奖励设计会如何影响智能体的行为？\n\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料：\n\nSutton & Barto 强化学习教材第3章\nGymnasium 自定义环境教程\nPyGame 基础教程\n\n视频资源：\n\nDavid Silver 强化学习课程第2讲\nGrid World 环境实现教学视频\n\n下周预习重点：\n\n动态规划算法\n值迭代与策略迭代\nQ-learning 算法基础",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "href": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建",
    "text": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n1. 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n项目目标：\n\n以小组为单位，基于第一次课的代码框架，独立完成迷宫环境 (Grid World) 的搭建。\n扩展迷宫地图，设计更复杂的迷宫场景 (例如：增加更多障碍物、宝藏、陷阱等)。\n实现基本的环境渲染，能够可视化智能体在迷宫中的探索过程。\n\n代码框架：\n\n提供第一次课中 GridWorldEnv 类的代码框架 (作为项目的基础)。\n小组需要自行完成代码的补全、修改和扩展。\n\nAI 辅助工具：\n\n鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高开发效率。\n但强调：AI 工具是辅助手段，学生需要理解代码逻辑，不能完全依赖 AI 工具生成代码，而忽略代码理解和调试。\n\n项目提交：\n\n小组提交完整的 Grid World 环境代码 (Python 文件)。\n无需提交项目报告。\n\n\n\n\n2. 答疑与指导\n\n解答学生在环境搭建过程中遇到的问题。\n重点关注：\n\nGymnasium 库的使用方法。\n自定义环境类的结构和接口。\n状态空间、动作空间、奖励函数的设计。\n环境渲染的实现。\nAI 辅助工具的使用技巧。\n\n\n\n\n3. 布置小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n明确项目要求、提交时间和评分标准 (本次项目不评分，作为后续项目的基础)。\n鼓励小组积极探索、尝试，遇到问题及时提问。\n建议小组：\n\n提前开始项目，预留充足的开发和调试时间。\n分工合作，提高开发效率。\n充分利用 AI 辅助工具，但也要注重代码理解和调试能力。\n相互交流、学习，共同解决问题。\n\n\n\n\n课后作业\n\n完成小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n思考题：\n\n在 Grid World 环境中，如何设计更有效的奖励函数，以引导智能体更快地找到宝藏？\n如果迷宫地图非常大，状态空间会变得很大，对强化学习算法会产生什么影响？\n如何使用 AI 辅助工具更高效地进行强化学习代码开发？\n\n\n\n\n预习资料\n\n阅读材料：\n\nGymnasium 官方文档 (自定义环境部分)。\n强化学习算法基础：Q-Learning 算法初步。\n探索-利用平衡的更多策略 (例如：\\(\\epsilon\\)-greedy 退火策略、UCB 算法等)。\n\n视频资源：\n\nGrid World 环境搭建详解。\nQ-Learning 算法原理讲解 (初步了解)。\n探索与利用的平衡策略讲解。\n\n下周预习重点：\n\nQ-Learning 算法原理和步骤。\nQ-Table 的更新规则。\n使用 Q-Learning 算法解决 Grid World 迷宫寻宝问题。\n\n\n\n请注意:\n\n代码框架: 第一次课的代码框架，指的是第一次课中 GridWorldEnv 类的代码。学生需要基于这个代码框架进行扩展和修改，完成小组项目一。\n环境注册: 请确保在运行 Grid World 环境代码之前，已经执行了环境注册代码 (register(...))。可以将注册代码放在单独的文件中，或者放在运行环境代码的脚本文件的开头。\nAI 辅助工具: 鼓励学生使用 AI 辅助工具，但务必强调理解代码逻辑的重要性，避免过度依赖 AI 工具。\n小组项目: 小组项目一旨在让学生熟悉 Gymnasium 库和自定义环境的流程，为后续的强化学习算法实践打下基础。\n\n\n\n\n\n\n\nGrid World 环境可视化\n\n\n\n\n\n\nGrid World 环境\n\n\n\n\n\n\n\n\n\n\n完整的Grid World环境实现 - step()方法\n\n\n\ndef step(self, action):\n    \"\"\"\n    执行一步动作，返回下一个状态、奖励、是否终止等信息\n    \n    参数:\n        action: 动作，0=上, 1=右, 2=下, 3=左\n        \n    返回:\n        observation: 新的状态\n        reward: 获得的奖励\n        terminated: 是否到达终止状态（目标或陷阱）\n        truncated: 是否达到最大步数\n        info: 额外信息\n    \"\"\"\n    # 动作映射到方向变化 (行,列)\n    direction = {\n        0: (-1, 0),  # 上\n        1: (0, 1),   # 右\n        2: (1, 0),   # 下\n        3: (0, -1)   # 左\n    }\n    \n    # 计算新位置\n    delta_row, delta_col = direction[action]\n    new_position = self._agent_location + np.array([delta_row, delta_col])\n    \n    # 检查是否越界或撞墙\n    if (\n        0 &lt;= new_position[0] &lt; self.size \n        and 0 &lt;= new_position[1] &lt; self.size \n        and self._grid_map[new_position[0], new_position[1]] != 1\n    ):\n        self._agent_location = new_position\n    \n    # 获取当前位置的单元格类型\n    current_cell = self._grid_map[self._agent_location[0], self._agent_location[1]]\n    \n    # 初始化奖励和终止状态\n    reward = -0.1  # 每一步的小惩罚，鼓励快速到达目标\n    terminated = False\n    truncated = False\n    \n    # 根据当前位置计算奖励和是否终止\n    if np.array_equal(self._agent_location, self._target_location):\n        # 到达目标\n        reward = 1.0\n        terminated = True\n    elif np.array_equal(self._agent_location, self._trap_location):\n        # 掉入陷阱\n        reward = -1.0\n        terminated = True\n    \n    # 将智能体位置转换为离散观测空间的索引\n    observation = self._agent_location[0] * self.size + self._agent_location[1]\n    \n    # 如果需要渲染，渲染当前帧\n    if self.render_mode == \"human\":\n        self.render()\n    \n    return observation, reward, terminated, truncated, {}\n\n\n\n\n\n\n\n\nQ-Learning 算法流程\n\n\n\n\n\n\nQ-Learning 算法流程\n\n\n\n\n\n\n\n\n\n\n价值迭代算法\n\n\n\n\n\n\n价值迭代算法",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "第一次课：Q-Learning 算法详解",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第一次课q-learning-算法详解",
    "href": "week3.html#第一次课q-learning-算法详解",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "Q-Learning 算法简介\n\n\n\nQ-Learning 是一种基于价值的离线强化学习算法，用于学习最优 Q 函数，从而得到最优策略。\n\n核心特点\n\n核心思想: 通过不断试错和更新 Q-Table，逐步逼近最优 Q 函数\n离线特性: 学习的策略与实际执行的策略可以不同\n最优 Q 函数: 表示在状态 \\(s\\) 下，执行动作 \\(a\\) 后的最大期望累积奖励\n最优策略: 在每个状态下选择 Q 函数值最大的动作\n\n\n\n\n\n\n\n\n\n\nQ-Table 详解\n\n\n\nQ-Table 是 Q-Learning 算法中用于存储 Q 函数值的表格。\n\n表格结构\n\n\n\n\n\n\n\n\n\n\n状态 (State)\n动作 1 (Action 1)\n动作 2 (Action 2)\n…\n动作 n (Action n)\n\n\n\n\n状态 1 (S1)\n\\(Q(S1, A1)\\)\n\\(Q(S1, A2)\\)\n…\n\\(Q(S1, An)\\)\n\n\n状态 2 (S2)\n\\(Q(S2, A1)\\)\n\\(Q(S2, A2)\\)\n…\n\\(Q(S2, An)\\)\n\n\n…\n…\n…\n…\n…\n\n\n状态 m (Sm)\n\\(Q(Sm, A1)\\)\n\\(Q(Sm, A2)\\)\n…\n\\(Q(Sm, An)\\)\n\n\n\n\n\n关键操作\n\n初始化: 通常初始化为 0 或小的随机值\n更新: 在交互过程中不断更新 Q 值\n查询: 根据当前状态查询对应的 Q 值\n\n\n\n\n\n\n\n\n\n\nQ-Learning 更新规则\n\n\n\nQ-Learning 使用时序差分学习方法来更新 Q 值，是一种无模型的强化学习方法。\n\n更新公式\n新的 Q(s, a)  &lt;-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))\n\n\n关键参数\n\n学习率 (\\(\\alpha\\)): 控制更新幅度 [0, 1]\n\n\\(\\alpha\\) 较大：学习快但不稳定\n\\(\\alpha\\) 较小：学习慢但稳定\n\n折扣因子 (\\(\\gamma\\)): 控制未来奖励重要性 [0, 1]\n\n\\(\\gamma\\) 接近 0：重视即时奖励\n\\(\\gamma\\) 接近 1：重视未来奖励\n\n\n\n\n\n\n\n\n\n\n\n算法流程图\n\n\n\n\n\n\n\n\ngraph LR\n    S[\"状态 s\"] --&gt; A[\"动作 a\"]\n    A --&gt; E[\"环境\"]\n    E --&gt; SP[\"新状态 s'\"]\n    E --&gt; R[\"奖励 r\"]\n    SP --&gt; QP[\"最大Q值\"]\n    R --&gt; TD[\"TD目标\"]\n    QP --&gt; TD\n    TD --&gt; U[\"更新Q表\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n算法步骤详解\n\n\n\n\n初始化 Q-Table\n\n创建状态-动作表格\n初始化所有 Q 值为 0\n\n循环训练 Episodes\n\n重置环境到初始状态\n在每个 episode 中进行多步交互\n直到达到目标或最大步数\n\n每步操作\n\n选择动作（探索策略）\n执行动作获取反馈\n更新 Q-Table\n更新当前状态\n\n训练完成\n\nQ-Table 收敛或达到预设次数\n提取最优策略\n\n\n\n\n\n\n\n\n\n\n动态定价案例\n\n\n\n\n场景设定\n\n商品: 单一商品销售\n状态: 库存水平（高/中/低）\n动作: 价格调整（涨/跌/维持）\n目标: 最大化累积利润\n\n\n\n奖励设计\n\n\n\n状态 (库存)\n动作 (价格调整)\n奖励 (利润)\n\n\n\n\n高库存\n降价\n+5 (销量增加)\n\n\n高库存\n维持原价\n+2 (正常销量)\n\n\n高库存\n涨价\n-1 (销量减少)\n\n\n中库存\n降价\n+3 (销量略增)\n\n\n中库存\n维持原价\n+4 (正常销量)\n\n\n中库存\n涨价\n+1 (销量略减)\n\n\n低库存\n降价\n-2 (缺货风险)\n\n\n低库存\n维持原价\n+6 (高利润率)\n\n\n低库存\n涨价\n+8 (更高利润率)\n\n\n\n\n\n\n\n\n\n\n\n\n动态定价代码示例\n\n\n\n# 初始化 Q-Table\nq_table = {}\n\n# 定义状态空间和动作空间\nstates = [\"High\", \"Medium\", \"Low\"]\nactions = [\"Increase\", \"Maintain\", \"Decrease\"]\n\n# Q-Learning 算法训练循环\nepisodes = 1000\nlearning_rate = 0.1\ndiscount_factor = 0.9\nepsilon = 0.1\n\nfor episode in range(episodes):\n    current_state = random.choice(states)\n    \n    for step in range(steps_per_episode):\n        # epsilon-greedy 策略选择动作\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.choice(actions)\n        else:\n            action = max(actions, key=lambda a: q_table.get((current_state, a), 0))\n            \n        # 获取奖励和下一状态\n        next_state = ...  # 状态转移\n        reward = get_reward(current_state, action)\n        \n        # 更新 Q 值\n        old_q = q_table.get((current_state, action), 0)\n        next_max_q = max([q_table.get((next_state, a), 0) for a in actions])\n        new_q = old_q + learning_rate * (reward + discount_factor * next_max_q - old_q)\n        q_table[(current_state, action)] = new_q\n        \n        current_state = next_state",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "href": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "第二次课：小组项目一：Q-Learning 算法编程实践",
    "text": "第二次课：小组项目一：Q-Learning 算法编程实践\n\n\n\n\n\n\n项目要求\n\n\n\n\n目标\n\n使用 Python 和 AI 工具编写 Q-Learning 算法\n应用于迷宫寻宝 (Grid World) 项目\n实现智能体自主探索和学习\n\n\n\n提交内容\n\n完整的 Q-Learning 算法代码\n修改后的 Grid World 环境代码（如有）\n可视化结果（可选）\n\n\n\n\n\n\n\n\n\n\n超参数调整指南\n\n\n\n\n关键超参数\n\n学习率 (\\(\\alpha\\))\n折扣因子 (\\(\\gamma\\))\n探索率 (\\(\\epsilon\\))\n\n\n\n调整方法\n\n经验调整\n\n基于经验和直觉\n快速但可能不是最优\n\n网格搜索\n\n遍历预定义参数组合\n全面但耗时\n\n随机搜索\n\n随机采样参数组合\n平衡效率和效果\n\n\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完成 Q-Learning 算法实现\n调整超参数并记录效果\n分析不同参数对学习效果的影响\n尝试改进算法性能\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料\n\nSutton & Barto 第6章\nQ-Learning 相关论文\n\n视频资源\n\nDavid Silver RL Course\nQ-Learning 实现教程\n\n下周预习重点\n\nDQN 算法原理\n神经网络基础\nPyTorch 入门",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "第一次课：Q-Learning 算法优化与改进",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第一次课q-learning-算法优化与改进",
    "href": "week4.html#第一次课q-learning-算法优化与改进",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "Q-Learning 算法的局限性\n\n\n\n\n状态空间爆炸问题\n\n状态空间过大时，Q-Table 规模庞大\n连续状态空间需要离散化处理\n存储和计算成本高\n\n\n\n探索-利用困境\n\n需要平衡探索新动作和利用已知最优动作\n\\(\\epsilon\\)-greedy 策略探索效率较低\n可能浪费时间在无用区域\n\n\n\n收敛性问题\n\n需要满足特定条件才能收敛\n环境可能不满足 MDP 假设\n可能收敛到局部最优解\n\n\n\n超参数敏感性\n\n性能受超参数影响大\n需要经验和技巧调整\n调整过程耗时耗力\n\n\n\n\n\n\n\n\n\n\n探索策略详解\n\n\n\n\n\\(\\epsilon\\)-greedy 策略\n\n以 \\(\\epsilon\\) 概率随机探索\n以 \\(1-\\epsilon\\) 概率选择最优动作\n简单但探索效率低\n\n\n\n\\(\\epsilon\\)-greedy 退火策略\n\n训练初期增加探索（大 \\(\\epsilon\\)）\n训练后期减少探索（小 \\(\\epsilon\\)）\n线性或指数衰减方式\n\n\n\n高级探索策略\n\nUCB 算法：考虑动作不确定性\nThompson Sampling：概率采样\nSoftmax 策略：基于 Q 值分布\n\n\n\n\n\n\n\n\n\n\nQ-Table 初始化技巧\n\n\n\n\n零值初始化\n\n简单易实现\n可能导致初始探索不足\n收敛速度较慢\n\n\n\n随机值初始化\n\n鼓励初始探索\n打破对称性\n需谨慎选择范围\n\n\n\n乐观初始化\n\n初始化为较大值\n鼓励探索未知动作\n适用于稀疏奖励环境\n\n\n\n基于领域知识\n\n利用先验知识\n加速学习过程\n通用性较差\n\n\n\n\n\n\n\n\n\n\n奖励函数设计原则\n\n\n\n\n基本原则\n\n与任务目标一致\n考虑稀疏vs密集奖励\n注意奖励尺度\n\n\n\n奖励类型\n\n稀疏奖励\n\n只在目标状态给予奖励\n更符合真实场景\n学习难度大\n\n密集奖励\n\n提供中间过程奖励\n加速学习过程\n需要careful设计\n\n\n\n\n示例（迷宫寻宝）\n# 稀疏奖励\nrewards = {\n    'goal': 10,      # 到达宝藏\n    'trap': -10,     # 陷阱\n    'default': 0     # 其他状态\n}\n\n# 密集奖励\nrewards = {\n    'goal': 10,      # 到达宝藏\n    'trap': -10,     # 陷阱\n    'step': -0.1,    # 每步惩罚\n    'distance': lambda d: 1/d  # 距离奖励\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第二次课小组项目实践",
    "href": "week4.html#第二次课小组项目实践",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "第二次课：小组项目实践",
    "text": "第二次课：小组项目实践\n\n\n\n\n\n\n项目优化方向\n\n\n\n\n算法优化\n\n实现退火探索策略\n优化 Q-Table 初始化\n改进奖励函数设计\n\n\n\n代码优化\n\n提高运行效率\n使用 NumPy 向量化\n添加可视化功能\n\n\n\n性能优化\n\n系统调整超参数\n记录实验结果\n分析优化效果\n\n\n\n\n\n\n\n\n\n\n调试技巧\n\n\n\n\n基本工具\n\nprint 语句跟踪\npdb 调试器使用\n日志记录分析\n\n\n\nAI 辅助\n\n使用 GitHub Copilot\n代码自动补全\n错误快速定位\n\n\n\n问题解决\n\n环境代码检查\n算法逻辑验证\n超参数调整\n奖励设计分析\n\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n继续优化项目代码\n准备项目报告\n预习下周内容：\n\nDouble Q-Learning\nExperience Replay\n深度强化学习基础\n\n\n\n\n\n\n\n\n\n\n下周预习重点\n\n\n\n\n项目报告准备\n答辩 PPT 制作\nSarsa 算法学习\n深度强化学习入门",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "",
    "text": "项目一回顾与Q-Learning的局限\n在完成并展示了第一个项目后，我们对Q-Learning有了更深入的实践体会。它在解决状态和动作空间有限的问题上表现出色。但同时，我们也可能遇到了它的瓶颈。\n思考一下： 在你的项目一中，如果状态空间变得极其庞大（例如，从简单的网格世界变成高分辨率的图像输入），或者状态变成连续的（例如，机器人的关节角度），你认为基于Q表格的方法会遇到什么问题？",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#项目一回顾与q-learning的局限",
    "href": "week5.html#项目一回顾与q-learning的局限",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "",
    "text": "Q-Learning面临的主要挑战\n\n状态空间爆炸 (State Space Explosion):\n\n对于具有大量状态的环境，维护一个巨大的Q表格变得不现实。想象一下，即使是一个简单的84x84像素的黑白图像，状态数量也超过 (2^{84 })！彩色图像或更复杂的输入会让这个数字变得更加天文。\n现实世界中的许多问题都涉及高维或连续的状态空间，表格方法难以适用。\n\n缺乏泛化能力 (Lack of Generalization):\n\n表格型Q-Learning为每一个状态独立学习Q值，无法利用状态之间的相似性。\n它无法将在某个状态学到的经验泛化到”相似但未见过”的状态。\n这意味着智能体需要访问并学习几乎所有可能的状态，导致样本效率低下。\n\n\n为了克服这些限制，我们需要引入更强大的工具——深度学习。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#神经网络强化学习的新引擎",
    "href": "week5.html#神经网络强化学习的新引擎",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "神经网络：强化学习的新引擎",
    "text": "神经网络：强化学习的新引擎\n神经网络（特别是深度神经网络）以其强大的函数近似 (Function Approximation) 能力，为解决Q-Learning的局限性提供了可能。\n\n为什么是神经网络？\n\n自动特征提取: 神经网络可以从原始输入（如图像像素、传感器读数）中自动学习有用的特征表示，无需人工设计。\n泛化能力: 通过在不同状态间共享参数（权重），神经网络可以将学到的知识泛化到相似的、未曾明确访问过的状态。\n处理高维输入: 神经网络天然适合处理高维数据，如图像、文本等。\n\n\n\n在强化学习中的应用\n我们可以用一个神经网络来近似Q函数，记作 (Q(s, a; ))，其中 () 代表网络的所有可学习参数（权重和偏置）。\n\n输入: 状态 (s) (例如，游戏画面的像素矩阵，或状态特征向量)。\n输出: 对于给定的状态 (s)，网络输出所有可能动作 (a) 的Q值估计。\n学习: 通过优化算法（如随机梯度下降及其变种）调整参数 ()，使得网络的输出 (Q(s, a; )) 尽可能接近目标Q值。\n\n这种用神经网络来近似Q函数的方法，正是深度Q网络 (Deep Q-Network, DQN) 的核心思想。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#深度q网络-dqn-原理详解",
    "href": "week5.html#深度q网络-dqn-原理详解",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "深度Q网络 (DQN) 原理详解",
    "text": "深度Q网络 (DQN) 原理详解\nDQN由DeepMind提出，首次成功地将深度学习与强化学习结合，并在Atari游戏上达到了超越人类水平的表现。\n\n基本思想\n用一个深度神经网络（通常是卷积神经网络CNN处理图像输入，或全连接网络MLP处理向量输入）来代替巨大的Q表格，即 (Q(s, a) Q(s, a; ))。\n\n\n关键挑战与创新\n直接用神经网络替代Q表进行训练并不稳定。主要因为： 1. 样本相关性: 强化学习中连续采集的样本 (s, a, r, s’) 之间高度相关，违反了许多机器学习算法的独立同分布假设，导致训练不稳定。 2. 目标值不稳定: Q值的目标 (y = r + _{a’} Q(s’, a’; )) 依赖于正在更新的网络参数 ()。目标值随着网络的更新而不断变化，就像追逐一个移动的目标，容易导致训练发散。\nDQN引入了两大关键技术来解决这些问题：\n\n经验回放 (Experience Replay):\n\n是什么？ 创建一个存储经验的缓冲区（通常是一个固定大小的队列），将智能体与环境交互产生的经验元组 ((s_t, a_t, r_{t+1}, s_{t+1}, )) 存储起来。\n怎么用？ 在训练时，不直接使用刚产生的经验，而是从缓冲区中随机采样一个小批量 (mini-batch) 的经验来训练Q网络。\n为什么有效？\n\n打破相关性: 随机采样打乱了经验的时间顺序，降低了样本间的相关性。\n提高数据效率: 每个经验可能被多次采样和学习，充分利用了来之不易的交互数据。\n平滑学习: 通过批次学习，梯度的更新更加平稳。\n\n\n目标网络 (Target Network):\n\n是什么？ 使用两个结构相同但参数不同的神经网络：\n\nQ网络 (或称主网络): 参数为 ()，这个网络在训练过程中频繁更新，并且用于实际的动作选择。\n目标网络: 参数为 (^-)，这个网络的参数定期从Q网络复制而来（例如，每 C 步），在计算目标Q值时保持固定。\n\n怎么用？ 在计算TD目标 (y) 时，使用目标网络来估计下一状态的最大Q值：\n\n如果 (s’) 是终止状态: (y = r)\n否则: (y = r + _{a’} Q(s’, a’; ^-)) （注意这里用的是 (^-)）\n\n为什么有效？ 通过固定目标网络一段时间，使得TD目标 (y) 相对稳定，降低了Q网络更新目标与自身变化之间的耦合，从而提高了训练的稳定性。\n\n\n\n\nDQN 算法流程概览\n# 初始化\n创建 Q网络 Q(s, a; θ) 和 目标网络 Q'(s, a; θ⁻)\n将 Q网络 的参数复制给 目标网络 (θ⁻ ← θ)\n初始化 经验回放缓冲区 D，容量为 N\n\nfor 每个回合 (episode):\n    获取初始状态 s\n    \n    for 每个时间步 t:\n        # 1. 与环境交互\n        根据当前 Q网络 Q(s, ·; θ) 和 ε-greedy 策略选择动作 a\n        执行动作 a，观察奖励 r 和下一个状态 s'\n        将经验转移 (s, a, r, s', done) 存储到缓冲区 D\n        \n        # 2. 从缓冲区采样\n        如果 D 中经验数量足够:\n            从 D 中随机采样一个 小批量 (mini-batch) 的经验 (sⱼ, aⱼ, rⱼ₊₁, sⱼ₊₁, doneⱼ)\n            \n            # 3. 计算目标 Q 值\n            计算目标 yⱼ:\n                if doneⱼ:\n                    yⱼ = rⱼ₊₁\n                else:\n                    # 使用目标网络计算下一状态的最大Q值\n                    yⱼ = rⱼ₊₁ + γ * maxₐ' Q'(sⱼ₊₁, a'; θ⁻)\n            \n            # 4. 计算损失并更新 Q 网络\n            计算损失 L = ( Q(sⱼ, aⱼ; θ) - yⱼ )²  (通常是均方误差或 Huber loss)\n            执行一步梯度下降，更新 Q网络 的参数 θ\n            \n        # 5. 更新状态\n        s = s'\n        \n        # 6. 定期更新目标网络\n        每隔 C 步:\n            将 Q网络 的参数复制给 目标网络 (θ⁻ ← θ)\n        \n        if done: # 如果回合结束\n            break\n\n\nDQN 架构图\n为了更直观地理解各组件如何协同工作，请看下面的架构图：\n\n    \n    \n    \n    \n    深度Q网络(DQN)架构图\n    \n    \n    \n    DQN智能体\n    \n    \n    \n    环境\n    (CartPole/Atari)\n    \n    \n    \n    Q网络\n    (频繁更新)\n    \n    \n    \n    目标网络\n    (定期更新)\n    \n    \n    \n    经验回放缓冲区\n    (存储和采样经验)\n    \n    \n    \n    选择动作 (a)\n    \n    \n    \n    存储经验\n    (s,a,r,s',done)\n    \n    \n    \n    采样批次 (sⱼ,aⱼ,rⱼ₊₁,sⱼ₊₁)\n    \n    \n    \n    计算目标值 yⱼ\n    (使用 Q')\n    \n    \n    \n    定期复制参数 (θ⁻ ← θ)\n    \n    \n    \n    计算损失\n    (Q(sⱼ,aⱼ;θ) - yⱼ)²\n    \n    \n    \n        \n            \n        \n    \n    \n    \n    \n    \n        ■ 神经网络\n        ■ 环境\n        ■ 经验回放\n        ■ 损失计算\n    \n\n这张图清晰地展示了DQN的各个部分如何交互：Q网络与环境互动并产生经验，经验存入缓冲区，然后从缓冲区采样用于训练Q网络，而目标网络则提供稳定的目标值计算，并定期从Q网络同步参数。",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#深入理解两大关键技术",
    "href": "week5.html#深入理解两大关键技术",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "深入理解两大关键技术",
    "text": "深入理解两大关键技术\n让我们更详细地探讨经验回放和目标网络。\n\n经验回放 (Experience Replay)目标网络 (Target Network)\n\n\n\n核心作用:\n\n打破样本相关性: 强化学习的序贯决策过程导致相邻样本高度相关。随机采样打破了这种时间上的依赖，使得样本更接近独立同分布，有利于神经网络训练。\n提高数据效率: 环境交互通常是昂贵的。经验回放使得智能体可以反复利用过去的经验进行学习，极大提高了样本的利用率。\n稳定训练过程: 对整个缓冲区进行采样，可以平滑数据分布的变化，避免训练过程因遇到连续的相似或极端样本而剧烈波动。\n\n实现要点:\nfrom collections import deque\nimport random\nimport numpy as np\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        # 使用双端队列存储经验，自动丢弃旧经验\n        self.buffer = deque(maxlen=capacity)\n\n    def add(self, state, action, reward, next_state, done):\n        # 将经验元组添加到缓冲区\n        experience = (state, action, reward, next_state, done)\n        self.buffer.append(experience)\n\n    def sample(self, batch_size):\n        # 从缓冲区中随机采样指定数量的经验\n        # 确保缓冲区内有足够经验才进行采样\n        if len(self.buffer) &lt; batch_size:\n            return None # 或者抛出异常，或返回空值\n\n        batch = random.sample(self.buffer, batch_size)\n\n        # 将批次中的经验解压，方便后续处理\n        # 注意：这里假设状态等是numpy数组或其他可以直接堆叠的格式\n        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n        return states, actions, rewards, next_states, dones\n\n    def __len__(self):\n        # 返回当前缓冲区中的经验数量\n        return len(self.buffer)\n思考与讨论: 缓冲区大小 (capacity) 如何选择？太大或太小会有什么影响？除了随机采样，还有哪些更高级的采样策略？（提示：第七周会讲到优先经验回放）\n\n\n\n\n核心原理: 解决”移动目标”问题。\n\n在标准的Q-Learning更新 (Q(s,a) Q(s,a) + ) 中，如果用神经网络近似Q函数，目标值 (r + _{a’} Q(s’,a’; )) 会随着 () 的更新而不断变化。\n这意味着我们用来计算更新目标的”尺子”本身也在不停地变动，导致学习过程不稳定，容易震荡或发散。\n目标网络通过”冻结”目标Q值计算中使用的网络参数 (^-) 一段时间，提供了一个相对稳定的学习目标。\n\n关键实现:\nimport tensorflow as tf # 或 torch\n\n# 假设 model_builder() 函数能创建我们的Q网络模型\nq_network = model_builder()\ntarget_network = model_builder()\n\n# 初始化：确保目标网络权重与Q网络一致\ntarget_network.set_weights(q_network.get_weights())\n\n# 在学习/训练函数中:\ndef train_step(batch):\n    states, actions, rewards, next_states, dones = batch\n\n    # 1. 使用 *目标网络* 预测下一状态的Q值\n    next_q_values_target = target_network.predict(next_states) \n    # 2. 计算下一状态的最大Q值\n    max_next_q = np.max(next_q_values_target, axis=1)\n    # 3. 计算 TD 目标 y\n    target_y = rewards + gamma * max_next_q * (1 - dones) \n\n    # 4. 使用 *Q网络* 计算损失并执行梯度更新\n    with tf.GradientTape() as tape:\n        # 获取当前状态下，实际采取动作的Q值预测\n        q_values = q_network(states)\n        action_indices = tf.stack([tf.range(len(actions)), actions], axis=1)\n        current_q_pred = tf.gather_nd(q_values, action_indices)\n\n        # 计算损失 (例如 MSE)\n        loss = tf.keras.losses.mean_squared_error(target_y, current_q_pred)\n\n    # 计算梯度并更新 Q 网络\n    grads = tape.gradient(loss, q_network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, q_network.trainable_variables))\n\n    return loss\n\n# 在主训练循环中定期更新目标网络:\nif global_step % target_update_frequency == 0:\n    print(f\"Step {global_step}: Updating target network.\")\n    target_network.set_weights(q_network.get_weights())\n更新策略:\n\n硬更新 (Hard Update): 每隔 C 步，将Q网络的参数 () 完全复制给目标网络 (^-) (如上代码所示)。这是原始DQN论文中使用的方法。\n软更新 (Soft Update): 每一步（或每隔几步）都进行微小的更新： (^- + (1-) ^-) 。其中 () 是一个很小的数（例如 0.001 或 0.01）。这种方式更新更平滑，在一些算法（如DDPG）中更常用。\n思考: 硬更新的更新频率 C 如何选择？软更新的 () 如何选择？它们各有什么优缺点？",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#dqn-与-q-learning-对比总结",
    "href": "week5.html#dqn-与-q-learning-对比总结",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "DQN 与 Q-Learning 对比总结",
    "text": "DQN 与 Q-Learning 对比总结\n\n\n\n\n\n\n\n\n特性\n表格型 Q-Learning\n深度 Q 网络 (DQN)\n\n\n\n\n价值表示\nQ表格 (显式存储每个状态-动作对的值)\n神经网络 (隐式表示，通过参数近似)\n\n\n状态空间\n仅限小规模、离散状态\n可处理大规模、高维甚至连续状态 (通过离散化或特定架构)\n\n\n价值更新\n单个样本更新 Q(s,a)\n从经验回放中采样批量样本进行更新\n\n\n参数数量\n状态数 × 动作数\n神经网络的权重和偏置数量 (通常远小于前者)\n\n\n稳定性技巧\n无特定技巧\n经验回放 + 目标网络\n\n\n泛化能力\n基本无\n强 (神经网络的核心优势)\n\n\n样本效率\n低 (需要大量探索覆盖状态空间)\n相对较高 (经验回放提高了利用率)\n\n\n计算复杂度\n更新快，查询快 (表格查找)\n训练慢 (神经网络前向/反向传播)，查询相对快\n\n\n适用场景\n状态/动作空间小的简单问题\n状态空间复杂、需要泛化能力的问题 (如游戏)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week5.html#展望dqn的扩展与改进",
    "href": "week5.html#展望dqn的扩展与改进",
    "title": "第五周：从Q-Learning到深度Q网络",
    "section": "展望：DQN的扩展与改进",
    "text": "展望：DQN的扩展与改进\nDQN是一个里程碑，但并非终点。后续研究提出了许多重要的改进，进一步提升了其性能和稳定性，我们将在后续课程中深入探讨其中一些：\n\n双重DQN (Double DQN): 旨在解决标准DQN中的Q值过高估计问题。\n决斗网络架构 (Dueling Network Architecture): 将Q值分解为状态价值和动作优势，可能在某些环境中学习更高效。\n优先经验回放 (Prioritized Experience Replay): 更智能地从经验回放缓冲区中采样，优先学习”有价值”的经验。\n分布式DQN (Distributional DQN): 不再只学习Q值的期望，而是学习其完整的值分布。\n噪声网络 (Noisy Nets): 一种更有效的探索策略。\n彩虹DQN (Rainbow): 集成了上述多种改进技术，达到了非常高的性能。\n\n\n\n\n\n\n\n\n课后活动与实践\n\n\n\n\n文献阅读: 尝试阅读DQN的原始论文 (Playing Atari with Deep Reinforcement Learning by Mnih et al., 2013/2015 或 Nature 版本 Human-level control through deep reinforcement learning)，重点理解引言、方法（特别是经验回放和目标网络部分）和实验设置。\n代码探索: 下载并运行课程提供的DQN代码示例。仔细阅读代码，理解ReplayBuffer类和目标网络的创建、使用及更新逻辑。\n动手实践: 选择一个你熟悉且相对简单的环境（例如Gym库中的CartPole-v1），尝试实现一个基础版本的DQN算法。\n\n网络结构: 可以从简单的全连接网络开始（例如，输入层 -&gt; 隐藏层(ReLU) -&gt; 输出层(线性)）。\n核心组件: 确保正确实现了经验回放缓冲区和目标网络的定期更新。\n超参数: 尝试调整一些基本超参数，如学习率、ε-greedy的衰减率、缓冲区大小、目标网络更新频率、批次大小等，观察它们对训练过程和最终性能的影响。\n\n思考: 在CartPole环境中，状态是4维的连续向量。DQN如何处理这种连续状态输入？与处理离散状态的Q-Learning相比有何不同？\n\n\n\n\n\n\n\n\n\n下周预习重点\n\n\n\n本周我们学习了DQN的基础，下周将深入探讨其重要的变体和调优技巧。请提前思考和了解：\n\nDouble DQN 和 Dueling DQN 分别试图解决标准DQN的什么问题？它们的核心思想是什么？\n除了ε-greedy，还有哪些探索策略？它们相比ε-greedy有何优劣？（例如，了解一下Noisy Networks或Boltzmann探索）\n训练深度强化学习模型时，有哪些常见的超参数需要调整？（例如，学习率、批次大小、缓冲区大小、目标网络更新频率等）它们各自的作用是什么？\n如何有效地监控和调试深度强化学习的训练过程？（了解一下TensorBoard等工具的作用）",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：从Q-Learning到深度Q网络</span>"
    ]
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "",
    "text": "温故知新：为何要改进DQN？\n在上周的学习中，我们掌握了标准DQN算法的核心思想及其在解决离散动作空间问题上的优势。然而，标准DQN并非完美，它在实际应用中也暴露出一些局限性。\n思考一下： 结合你上周的学习和实验经验，标准DQN在哪些方面可能存在不足？（提示：可以从Q值估计的准确性、学习效率、探索方式等方面考虑。）\n本周，我们将深入探讨标准DQN的主要局限性，并学习一系列强大的改进方法，为你的DQN武器库增添更多利器。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  },
  {
    "objectID": "week7.html#dqn的核心改进方法",
    "href": "week7.html#dqn的核心改进方法",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "DQN的核心改进方法",
    "text": "DQN的核心改进方法\n为了克服标准DQN的局限性，研究者们提出了多种有效的改进策略。下面我们来学习其中最核心的几种：\n\nDouble DQN：告别Q值过高估计Dueling DQN：状态价值与动作优势的分离优先经验回放 (PER)：让学习更聚焦多步学习：看得更远一点\n\n\n\n核心问题: 标准DQN在计算目标Q值时，使用同一个网络既选择最大Q值对应的动作，又评估该动作的Q值 ( \\(r + \\gamma \\max_{a'} Q(s', a'; \\theta^-)\\) )。这容易导致对Q值的过高估计 (overestimation)，尤其是在学习初期或存在噪声时，某个被随机高估的Q值更容易被max操作选中，导致策略学习产生偏差。\n解决方案: Double DQN巧妙地将动作选择和动作评估解耦。它使用当前的主网络 \\(Q(s, a; \\theta)\\) 来选择下一个状态 \\(s'\\) 的最优动作 \\(a^* = \\arg\\max_{a'} Q(s', a'; \\theta)\\)，然后使用目标网络 \\(Q(s', a; \\theta^-)\\) 来评估这个选定动作的价值。\n\nDouble DQN 目标: \\(r + \\gamma Q(s', a^*; \\theta^-) = r + \\gamma Q(s', \\arg\\max_{a'} Q(s', a'; \\theta); \\theta^-)\\)\n\n代码关键点: 在计算目标Q值时，需要先用主网络预测下一个状态的所有动作的Q值，找出最大Q值对应的动作索引，然后用目标网络预测下一个状态的所有动作的Q值，并取出该索引对应的Q值用于计算目标。\n思考: Double DQN是否在所有情况下都优于标准DQN？它在哪些场景下能发挥更大优势？\n\n\n\n\n思考: 想象一个场景，无论你向左还是向右移动，最终的奖励可能都差不多，因为当前所处的状态本身就很有价值（或很危险）。这启发我们：状态本身的价值 (Value) 和在特定状态下采取某个动作相对于其他动作的优势 (Advantage) 是不是可以分开考虑？\n原理: Dueling DQN正是基于这种思想。它将Q值函数 \\(Q(s,a)\\) 分解为两部分：\n\n状态价值函数 \\(V(s)\\): 表示处于状态 \\(s\\) 本身的价值，与具体动作无关。\n动作优势函数 \\(A(s,a)\\): 表示在状态 \\(s\\) 下，采取动作 \\(a\\) 相对于采取平均动作的好坏程度。\n\n网络结构上，通常有一个共享的特征提取层，然后分叉为两个流：一个输出标量 \\(V(s)\\)，另一个输出每个动作的优势 \\(A(s,a)\\)。最后将两者结合得到Q值。为了保证 \\(V(s)\\) 真正学到的是状态价值，并解决 \\(V\\) 和 \\(A\\) 的不可辨识问题 (identifiability issue)，通常采用以下聚合方式：\n\n\\(Q(s,a) = V(s) + (A(s,a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a' \\in \\mathcal{A}} A(s,a'))\\) （减去优势函数的均值可以使优势函数的均值为零，让 \\(V(s)\\) 更好地代表状态价值。）\n\n代码关键点: 需要设计一个包含共享层、价值流分支和优势流分支的网络结构，并根据上述公式在forward方法中组合输出。\n讨论: Dueling DQN在哪些类型的环境中可能特别有效？为什么？（提示：考虑那些状态价值比动作选择更重要的环境，或者动作空间很大但很多动作影响相似的环境。）\n\n\n\n\n核心问题: 标准DQN的经验回放是均匀采样，这意味着所有经验（无论重要与否）被选中的概率都相同。但直觉上，那些带来”惊喜”（即预测误差很大）的经验应该包含更多值得学习的信息。\n解决方案: 优先经验回放 (Prioritized Experience Replay, PER) 根据经验的重要性来赋予不同的采样概率，让智能体更频繁地从重要的经验中学习。\n\n重要性度量: 通常使用TD误差 (Temporal Difference error) 的绝对值 \\(|\\delta_i| = |y_i - Q(s_i, a_i; \\theta)|\\) 来衡量。TD误差越大，表示当前的Q值预测与目标值差距越大，该经验越”令人惊讶”，学习价值可能越高。\n采样概率: 经验 (i) 被采样的概率 (P(i)) 与其优先级 (p_i) 成正比。优先级通常基于TD误差计算：\\(p_i = (|\\delta_i| + \\epsilon)^\\alpha\\)，其中 \\(\\epsilon\\) 是一个很小的正数（防止优先级为0），\\(\\alpha \\in [0, 1]\\) 是一个超参数，控制优先级的程度（\\(\\alpha=0\\) 时退化为均匀采样）。\n$ P(i) = $\n偏差修正: 优先采样会引入偏差，因为重要的样本被重复学习的次数更多。为了修正这种偏差，PER引入了重要性采样 (Importance Sampling, IS) 权重 \\(w_i\\) 来调整这些样本在梯度更新中的贡献：\n\n\\(w_i = \\left(\\frac{1}{N} \\cdot \\frac{1}{P(i)}\\right)^\\beta = \\left(\\frac{\\sum_k p_k}{N \\cdot p_i}\\right)^\\beta\\) 其中 \\(N\\) 是缓冲区大小，\\(\\beta \\in [0, 1]\\) 是另一个超参数，用于控制权重的大小（通常从一个较小的值开始，在训练过程中逐渐增加到1）。这个权重 \\(w_i\\) 会被用来缩放对应样本的损失。\n\n\n思考: PER如何提高样本效率？它在实现上可能引入哪些新的复杂性和计算开销？如何选择合适的 \\(\\alpha\\) 和 \\(\\beta\\) 值？\n选学/挑战: 为了高效地实现按优先级采样和更新优先级，PER通常使用一种称为SumTree的数据结构。尝试理解SumTree的工作原理。\n\n\n\n\n核心问题: 标准DQN使用单步TD目标 \\(r_t + \\gamma \\max_a Q(s_{t+1}, a)\\)，只利用了一步的真实奖励 \\(r_t\\)，而后续的价值完全依赖于估计值 \\(Q(s_{t+1}, a)\\)。这可能导致学习信号传播较慢，并且过于依赖可能有偏差的Q值估计。\n解决方案: 多步学习 (Multi-step Learning) 使用未来 \\(n\\) 步的累计折扣奖励和第 \\(n\\) 步之后的Q值估计来构建目标值，看得更”远”。\n\nn步回报 (n-step return): \\(R_t^{(n)} = r_{t+1} + \\gamma r_{t+2} + \\dots + \\gamma^{n-1} r_{t+n} + \\gamma^n \\max_{a'} Q(s_{t+n}, a'; \\theta^-)\\) （注意：这里用 \\(r_{t+1}\\) 表示 \\(s_t, a_t\\) 得到的奖励，与之前公式的 \\(r\\) 对应）\n这个 \\(R_t^{(n)}\\) 将替代单步TD目标 \\(r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^-)\\) 用于计算TD误差和更新Q网络。\n\n优势:\n\n更快地传播奖励信息，特别是对于奖励稀疏的环境。\n减少了对早期不准确的Q值估计的依赖。\n\n讨论: 选择n步学习的步数 \\(n\\) 时需要考虑哪些因素？ \\(n\\) 越大越好吗？（提示：考虑偏差和方差的权衡。更大的 \\(n\\) 通常有更低的偏差但更高的方差。）",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  },
  {
    "objectID": "week7.html#探索的艺术如何更聪明地探索",
    "href": "week7.html#探索的艺术如何更聪明地探索",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "探索的艺术：如何更聪明地探索？",
    "text": "探索的艺术：如何更聪明地探索？\n探索 (Exploration) 与利用 (Exploitation) 的平衡是强化学习中的经典问题。除了我们熟悉的ε-greedy策略及其退火方法，还有更”聪明”的探索策略。\n\n回顾: ε-greedy策略是如何工作的？它的主要缺点是什么？（提示：随机性、与状态无关、需要手动调整ε衰减）\n思考与对比:\n\nNoisy Networks (噪声网络):\n\n核心思想: 不再依赖于外部的随机决策（如ε-greedy），而是直接在网络的权重中引入可学习的参数化噪声。智能体通过学习噪声的大小来自适应地进行探索。\n实现: 将网络中的线性层（nn.Linear）替换为NoisyLinear层。在训练时，层的权重和偏置会加入通过网络学习到的噪声；在评估时，则关闭噪声，使用确定的权重进行决策。\n优势: 探索是状态相关的 (因为噪声通过网络传播，受输入状态影响)，并且探索的程度是自适应学习的，无需手动设计复杂的ε衰减方案。通常比ε-greedy更有效地探索环境。\n\nBoltzmann探索 (Softmax Exploration):\n\n原理: 根据当前状态下各个动作的Q值估计，计算一个概率分布，然后根据这个概率分布来随机选择动作。Q值越高的动作被选中的概率越大。\n公式: \\(P(a|s) = \\frac{\\exp(Q(s,a)/\\tau)}{\\sum_{a'}\\exp(Q(s,a')/\\tau)}\\)\n\\(\\tau\\) 称为温度 (temperature) 参数。\\(\\tau\\) 越大，概率分布越接近均匀分布（探索性越强）；\\(\\tau\\) 越小，概率分布越集中在Q值最高的动作上（利用性越强）。通常也需要对 \\(\\tau\\) 进行退火。\n与ε-greedy对比: Boltzmann探索不是完全随机地选择非最优动作，而是根据Q值的相对大小来决定探索的方向，可能比ε-greedy更倾向于探索”看起来有潜力”的次优动作。\n\n\n为你的项目选择: 考虑你的CartPole项目或其他正在进行的项目，你会选择哪种探索策略（ε-greedy退火, Noisy Networks, Boltzmann）？说明你的理由。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  },
  {
    "objectID": "week7.html#dqn训练调优与监控技巧",
    "href": "week7.html#dqn训练调优与监控技巧",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "DQN训练调优与监控技巧",
    "text": "DQN训练调优与监控技巧\n获得良好性能的DQN模型往往需要细致的调优和有效的监控。\n\n网络结构:\n\n对于简单问题（如CartPole），通常2-3个隐藏层，每层64-256个神经元就足够了。\n对于更复杂的视觉输入问题（如Atari游戏），通常使用卷积层提取特征，再接全连接层。\n并非越深或越宽的网络越好，需要根据问题复杂度进行选择。\n激活函数: 隐藏层常用ReLU及其变种（如LeakyReLU），输出层（计算Q值）通常不需要激活函数。\n\n关键超参数:\n\n学习率 (Learning Rate): 常用范围 1e-4 到 1e-3。太大会导致训练不稳定，太小则收敛缓慢。Adam等自适应优化器通常效果较好，也可以配合学习率调度（逐步降低学习率）。\n批次大小 (Batch Size): 常用范围 32 到 256。更大的批次通常提供更稳定的梯度估计，但也更消耗内存和计算资源。\n经验回放缓冲区大小 (Replay Buffer Size): 常用范围 1万到100万。缓冲区需要足够大以保证样本多样性，打破数据相关性，但过大可能包含太多陈旧的、不再符合当前策略的经验。\n目标网络更新频率 (Target Network Update Frequency): 对于硬更新，常用每隔几百到几万个训练步更新一次。更新太频繁可能导致目标不稳定，太慢则可能导致学习滞后。也可以使用软更新 (Soft Update)：每次训练步都按一个小的比例 \\(\\tau\\) (e.g., 0.001, 0.01) 更新目标网络参数：\\(\\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-\\)。\n\n奖励设计 (Reward Engineering):\n\n奖励塑形 (Reward Shaping): 设计中间奖励引导智能体学习，但要小心引入不期望的偏差。\n奖励归一化/裁剪 (Reward Normalization/Clipping): 将奖励缩放到一个固定范围（如[-1, 1]）通常有助于稳定训练，防止Q值爆炸。\n\n使用TensorBoard监控训练:\n\n可视化是理解和调试强化学习过程的关键。TensorBoard是一个强大的工具。\n为什么要监控？ 观察学习进展，发现潜在问题（如不收敛、震荡、梯度消失/爆炸），比较不同超参数/算法的效果。\n关键监控指标:\n\nEpisode Reward: 每个回合的总奖励，是我们最关心的性能指标。\nLoss: TD误差或损失函数的值，观察其是否逐渐下降并稳定。\nEpsilon (如果使用ε-greedy): 观察探索率是否按预期衰减。\nQ-values: 观察平均或最大Q值的变化趋势，可以帮助判断是否存在过高估计等问题。\nGradients/Weights: 监控梯度范数和权重分布有助于发现训练不稳定的问题。\n\n实践: 学习如何在你的代码中集成TensorBoard (torch.utils.tensorboard.SummaryWriter) 来记录上述指标。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  },
  {
    "objectID": "week7.html#小组项目实战挑战与优化",
    "href": "week7.html#小组项目实战挑战与优化",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "小组项目实战：挑战与优化",
    "text": "小组项目实战：挑战与优化\n现在，是时候将我们学到的新知识应用到实际项目中了！第二次课我们将聚焦于解决你在项目中遇到的问题，并尝试应用各种改进和调优技巧来提升模型性能。\n\n常见挑战与应对策略\n在训练DQN（尤其是在优化如CartPole这样的项目时）可能会遇到各种问题。以下是一些常见问题及其可能的分析方向和解决思路：\n\n模型不收敛 / 性能很差:\n\n检查清单: 奖励函数设计是否合理？状态或奖励是否进行了归一化？学习率设置是否合适（过大或过小）？探索策略是否有效（探索不足或过早停止探索）？网络结构是否过于复杂或简单？经验回放缓冲区是否太小？目标网络更新频率是否恰当？代码实现是否存在Bug？\n调试工具: 打印关键变量（如状态、奖励、Q值、损失）；单步调试；利用TensorBoard监控指标变化；编写简单的测试用例验证代码逻辑。\n尝试方案: 调整学习率；更换或调整探索策略参数；简化或复杂化网络结构；调整缓冲区大小和更新频率；尝试Double/Dueling DQN等改进；仔细检查环境交互和数据处理代码。\n\n训练不稳定 / 奖励大幅震荡:\n\n可能原因: 学习率过大；目标网络更新过于频繁（硬更新）或 \\(\\tau\\) 过大（软更新）；批次大小过小导致梯度方差大；经验回放缓冲区未能有效打破数据相关性；奖励尺度过大。\n尝试方案: 降低学习率；降低目标网络更新频率或减小软更新的 \\(\\tau\\)；尝试梯度裁剪 (Gradient Clipping) 限制梯度范数；增大批次大小；确保缓冲区足够大且采样有效；对奖励进行归一化或裁剪。\n\n探索不足 / 过早收敛到次优策略:\n\n可能原因: 初始探索率 \\(\\epsilon\\) 或温度 \\(\\tau\\) 设置过低；探索率衰减过快；探索策略本身效率不高。\n尝试方案: 提高初始探索率；减缓探索率衰减速度或调整衰减方式；尝试Noisy Networks或Boltzmann探索等更高级的探索策略；检查奖励函数是否会过早惩罚探索行为。\n\n\n分享与讨论: 你在项目中遇到了哪些具体问题？尝试过哪些解决方法？和其他同学交流你的经验和困惑。",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  },
  {
    "objectID": "week7.html#进阶视野彩虹dqn-rainbow",
    "href": "week7.html#进阶视野彩虹dqn-rainbow",
    "title": "第七周：深入探索DQN——改进、调优与实战",
    "section": "进阶视野：彩虹DQN (Rainbow)",
    "text": "进阶视野：彩虹DQN (Rainbow)\nDQN的研究并未停止。一个里程碑式的工作是Rainbow DQN，它并非提出全新的思想，而是巧妙地将之前我们讨论过的多种DQN改进技术（以及一种称为分布式RL的技术）集成到了一个统一的框架中。\n\n核心组件: Rainbow通常集成了：\n\nDouble DQN\n优先经验回放 (PER)\nDueling Networks\n多步学习 (Multi-step Learning)\n分布式RL (Distributional RL): 不再只学习Q值的期望，而是学习Q值的完整分布。\nNoisy Nets\n\n核心思想: “集大成者”。研究者发现，这些改进之间存在一定的协同效应，将它们组合在一起的效果往往优于单个改进或简单的两两组合。\n意义: Rainbow展示了组合已有改进可以大幅提升性能和样本效率，为后续研究提供了基准。它也启发我们，在解决复杂问题时，可以考虑融合多种方法的优点。虽然我们不要求完整实现Rainbow，但了解它的构成有助于我们理解各项改进的重要性和相互关系。\n\n\n\n\n\n\n\n\n课后活动与作业\n\n\n\n\n代码实践: 在你的小组项目中（如CartPole或其他环境），选择并完整实现至少一项本周学习的DQN改进（Double DQN, Dueling DQN, PER, Multi-step Learning, Noisy Nets），并与你之前的基线DQN进行实验对比。\n实验报告: 撰写一份简单的实验报告，包含以下内容：\n\n清晰描述你选择并实现的改进方法的原理。\n展示你的对比实验结果（必须包含使用TensorBoard或其他绘图工具生成的关键性能图表，如：每个回合奖励随训练步数/回合数的变化曲线，损失函数变化曲线等）。清晰标注图中不同曲线对应的算法。\n分析和讨论你的实验结果：改进方法是否带来了预期的性能提升？效果如何？如果没有达到预期效果，可能的原因是什么？\n简述你在实现和实验过程中遇到的主要挑战以及你是如何解决的。\n\n思考: Rainbow DQN集成了多种技术。你认为这些技术之间可能存在怎样的相互作用（例如，哪些技术组合可能效果特别好，哪些可能存在冲突或冗余）？请阐述你的想法。\n\n\n\n\n\n\n\n\n\n下周预习重点\n\n\n\n在深入了解了基于价值的DQN方法之后，下周我们将探索另一大类强化学习算法：基于策略的方法 (Policy-Based Methods)。请思考并尝试了解以下问题：\n\n策略梯度 (Policy Gradient) 的核心思想是什么？ 它与基于价值的方法（如DQN直接学习Q值）有何根本不同？（提示：策略梯度方法直接学习策略函数 \\(\\pi(a|s)\\)）\nREINFORCE 算法是如何工作的？ 它是最基础的策略梯度算法之一，尝试理解其更新规则背后的直觉。它的主要优缺点是什么？\nActor-Critic 方法试图解决什么问题？ 它如何结合基于价值和基于策略的方法？其基本框架是怎样的？\n思考: 为什么DQN通常不适用于连续动作空间（例如，控制机器人的关节角度）？下周我们将学习的基于策略的方法（特别是Actor-Critic的变种）如何解决这个问题？",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>第七周：深入探索DQN——改进、调优与实战</span>"
    ]
  }
]