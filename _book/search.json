[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "智能计算",
    "section": "",
    "text": "欢迎来到《智能计算》课程！\n本课程专为经济管理类学生设计，旨在帮助大家快速掌握强化学习的基本概念和应用技能，并结合 AI 辅助编程工具，提升学习效率和实践能力。\n通过本课程，你将：\n\n了解强化学习的核心思想和算法\n掌握使用 Python 和常用库进行强化学习编程\n能够应用强化学习解决实际商业问题\n\n让我们一起开始强化学习的探索之旅吧！\n前往课程大纲\n开始第一周学习",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>课程介绍</span>"
    ]
  },
  {
    "objectID": "syllbus.html",
    "href": "syllbus.html",
    "title": "课程大纲",
    "section": "",
    "text": "课程概述\n本课程为经管学生设计，旨在弱化数学理论和公式，侧重强化学习的实战应用，并结合 AI 辅助编程。课程共计 16 周，每周两次课，每次 90 分钟。课程以项目为中心，贯穿始终。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程目标",
    "href": "syllbus.html#课程目标",
    "title": "课程大纲",
    "section": "课程目标",
    "text": "课程目标\n完成本课程后，学生将能够：\n\n理解核心概念: 解释强化学习的核心概念，并区分其与监督学习和无监督学习的不同。\n掌握编程工具: 熟练使用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，以及 Gymnasium, Stable Baselines3 等 Python 库进行强化学习编程。\n应用经典算法: 运用 Q-Learning, DQN, PPO 等经典强化学习算法，解决至少两种不同类型的商业问题 (例如：动态定价、推荐系统)。\n团队协作: 有效地进行团队合作，完成五个具有实际商业应用价值的强化学习小组项目，并撰写项目报告。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#课程结构",
    "href": "syllbus.html#课程结构",
    "title": "课程大纲",
    "section": "课程结构",
    "text": "课程结构\n课程分为四个阶段，循序渐进地引导学生从理论到实践，最终完成小组项目，并拓展到更广泛的商业应用场景和伦理考量。\n\n阶段一 (1-2周): 强化学习入门\n\n目标：理解基本概念，体验简单环境。\n\n阶段二 (3-8周): 经典算法实践\n\n目标：掌握 Q-Learning 和深度 Q 网络 (DQN) 算法，解决中等难度问题。\n\n阶段三 (9-13周): 高级算法探索\n\n目标：了解策略梯度方法，尝试更复杂的环境和算法。\n\n阶段四 (14-16周): 小组项目实战与商业应用拓展\n\n目标：团队合作，应用强化学习解决实际商业问题，并拓展到更广泛的商业应用场景和伦理考量。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#每周课程安排",
    "href": "syllbus.html#每周课程安排",
    "title": "课程大纲",
    "section": "每周课程安排",
    "text": "每周课程安排\n以下是详细的每周课程内容和项目安排，每次课程 90 分钟。\n\n第一阶段 (1-2周): 强化学习入门\n\n第1周 (共2次课)\n\n第一次课: 什么是强化学习？\n\n内容：\n\n讲解强化学习的核心概念：智能体 (Agent)、环境 (Environment)、动作 (Action)、状态 (State)、奖励 (Reward)。\n对比强化学习与监督学习、无监督学习。\n展示强化学习在商业领域的应用案例 (例如：推荐系统、动态定价、库存管理)。\n\n\n第二次课: AI 辅助编程入门与平衡杆 (CartPole) 初体验\n\n内容：\n\n介绍并安装 AI 辅助编程工具 (例如：GitHub Copilot, Tabnine)。\n演示如何使用 AI 工具辅助编写 Python 代码，并针对平衡杆游戏 (CartPole) 进行初步的演示和代码演练。\n介绍强化学习常用 Python 库 (例如：Gymnasium, Stable Baselines3)，强调其易用性和对初学者的友好性。\n\n实践：\n\n引导学生使用 AI 工具和库，体验平衡杆 (CartPole) 环境。\n完成简单的 Python 编程练习，为后续强化学习编程打基础。\n\n\n\n\n\n第2周 (共2次课)\n\n第一次课: 强化学习框架与迷宫环境 (Grid World) 搭建\n\n内容：\n\n解释马尔可夫决策过程 (MDP) 的基本思想 (无需深入数学细节，侧重直观理解)。\n讲解策略 (Policy)、价值函数 (Value Function, 包括 V 函数和 Q 函数) 的概念，强调直观理解。\n探讨探索 (Exploration) 与利用 (Exploitation) 的平衡问题，用商业案例 (例如：新市场尝试 vs. 现有市场深耕) 进行类比。\n\n实践：\n\n详细讲解如何使用 Gymnasium 库搭建迷宫环境 (Grid World)。\n\n\n第二次课: 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n内容：\n\n提供迷宫环境 (Grid World) 的代码框架，并指导学生使用 AI 工具进行代码补全和修改。\n\n实践：\n\n学生以小组为单位，搭建迷宫环境 (Grid World)。\n\n答疑：\n\n解答学生在环境搭建过程中遇到的问题，确保所有小组都能成功搭建迷宫环境 (Grid World)。\n布置小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n\n\n\n\n\n\n第二阶段 (3-8周): 经典算法实践\n\n第3周 (共2次课)\n\n第一次课: Q-Learning 算法详解\n\n内容：\n\n直观解释 Q-Table 和 Q-Learning 的更新规则 (避免复杂的数学公式，用表格和例子说明)。\n梳理 Q-Learning 算法的步骤流程。\n结合动态定价案例，演示如何使用 Q-Learning 算法解决简单动态定价问题 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目一：Q-Learning 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Q-Learning 算法代码，应用于迷宫寻宝 (Grid World) 项目。\n\n内容：\n\n讲解超参数 (学习率、折扣因子) 的作用，以及如何进行简单的参数调整 (直观理解)。\n\n检查点：\n\n小组项目一检查点：确保学生小组能够运行基本的 Q-Learning 智能体，在迷宫环境 (Grid World) 中进行探索。\n\n\n\n\n\n第4周 (共2次课)\n\n第一次课: Q-Learning 算法优化与改进 / 小组项目一提交 / 优秀小组项目一讲解 (3组)\n\n内容：\n\n讨论 Q-Learning 算法的局限性 (例如：状态空间爆炸问题)。\n介绍 ε-greedy 策略等探索策略，提升智能体的探索能力。\n讲解 Q-Table 初始化、奖励函数设计等实用技巧，提升 Q-Learning 算法的性能。\n\n管理：\n\n小组项目一：迷宫寻宝 (Grid World) 提交 (第 4 周第一次课前)。\n优秀小组项目一讲解 (3 组)。\n\n\n第二次课: 小组项目一：Q-Learning 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 Q-Learning 算法代码，优化智能体在迷宫环境 (Grid World) 中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第5周 (共2次课)\n\n第一次课: 深度学习的引入 - 深度 Q 网络 (DQN) 初探\n\n内容：\n\n解释 Q-Learning 算法在复杂问题上的瓶颈 (状态空间爆炸)。\n引入深度学习 (神经网络) 的基本概念 (无需深入数学细节，强调神经网络的表示能力)。\n讲解深度 Q 网络 (DQN) 的基本原理，如何使用神经网络近似 Q 函数。\n对比 DQN 与 Q-Learning 算法的异同。\n\n\n第二次课: DQN 算法关键技术 - 经验回放与目标网络\n\n内容：\n\n深入讲解 DQN 算法的两个关键技术：经验回放 (Experience Replay) 和目标网络 (Target Network) 的作用和原理。\n讲解如何使用经验回放和目标网络来提升 DQN 算法的稳定性和性能。\n\n代码：\n\n演示如何在 DQN 算法代码中加入经验回放和目标网络 (使用 AI 辅助编程)。\n\n\n\n\n\n第6周 (共2次课)\n\n第一次课: DQN 算法详解 / 小组项目二提交 / 优秀小组项目二讲解 (3组)\n\n内容：\n\n梳理 DQN 算法的完整步骤流程。\n结合平衡杆 (CartPole) 案例，演示如何使用 DQN 算法解决平衡杆问题 (使用 AI 辅助编程)。\n\n管理：\n\n小组项目二：平衡杆 (CartPole) 提交 (第 6 周第一次课前)。\n优秀小组项目二讲解 (3 组)。\n\n\n第二次课: 小组项目二：DQN 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 DQN 算法代码，应用于平衡杆 (CartPole) 项目。\n\n内容：\n\n讲解神经网络结构设计 (层数、神经元数量) 的基本原则 (无需深入理论，强调试错和调参)。\n\n检查点：\n\n小组项目二检查点：确保学生小组能够运行基本的 DQN 智能体，在平衡杆 (CartPole) 环境中进行探索。\n\n\n\n\n\n第7周 (共2次课)\n\n第一次课: DQN 算法改进与调优\n\n内容：\n\n讨论 DQN 算法的改进方向 (例如：Double DQN, Dueling DQN 等)。\n探讨不同的探索策略 (例如：ε-greedy 退火策略、Noisy Networks 等)，进一步提升智能体的探索能力。\n讲解 DQN 算法的调参技巧 (学习率、batch size、网络结构等)，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n\n第二次课: 小组项目二：DQN 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 DQN 算法代码，优化智能体在平衡杆 (CartPole) 环境中的表现。\n\n指导：\n\n指导学生使用调试工具和 AI 工具，解决编程中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第8周 (共2次课)\n\n第一次课: 策略梯度方法 - Policy Gradient 算法初步 / 小组项目三提交 / 优秀小组项目三讲解 (3组)\n\n内容：\n\n介绍策略梯度 (Policy Gradient) 方法的基本思想 (直接优化策略)。\n讲解 Policy Gradient 算法 (例如：REINFORCE 算法) 的基本原理和流程 (无需深入数学推导，侧重直观理解)。\n对比策略梯度方法与价值方法 (Q-Learning, DQN) 的异同。\n\n管理：\n\n小组项目三：资源分配 模拟环境 提交 (第 8 周第一次课前)。\n优秀小组项目三讲解 (3 组)。\n\n\n第二次课: 小组项目三：Policy Gradient 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 Policy Gradient 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讨论策略梯度方法中的探索问题，以及如何进行有效的探索。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 Policy Gradient 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n\n第三阶段 (9-13周): 高级算法探索\n\n第9周 (共2次课)\n\n第一次课: Actor-Critic 算法 - A2C 算法详解\n\n内容：\n\n讲解 Actor-Critic 算法的基本框架 (结合策略梯度和价值方法)。\n深入讲解 A2C (Advantage Actor-Critic) 算法的原理和优势 (例如：更稳定、更高效)。\n\n代码：\n\n演示 A2C 算法的代码实现 (使用 AI 辅助编程)。\n\n\n第二次课: 小组项目三：A2C 算法优化与改进\n\n内容：\n\n讨论 A2C 算法的改进方向 (例如：GAE-优势函数估计)。\n讲解 A2C 算法的调参技巧，以及如何使用 TensorBoard 等可视化工具监控训练过程。\n\n实践：\n\n指导学生尝试改进 A2C 算法，提升算法性能。\n\n\n\n\n\n第10周 (共2次课)\n\n第一次课: 近端策略优化 (PPO) 算法 - PPO 算法详解\n\n内容：\n\n讲解近端策略优化 (PPO) 算法的原理和优势 (例如：性能稳定、易于调参)。\n深入讲解 PPO 算法的核心机制：Clipping 和 PPO-Penalty。\n对比 PPO 算法与 A2C 算法的异同。\n\n\n第二次课: 小组项目三：PPO 算法编程实践\n\n实践：\n\n指导学生以小组为单位，使用 Python 和 AI 工具，编写 PPO 算法代码，应用于资源分配模拟环境项目。\n\n内容：\n\n讲解 PPO 算法的关键超参数 (例如：clip ratio, value function coefficient 等) 的作用，以及如何进行参数调整。\n\n检查点：\n\n小组项目三检查点：确保学生小组能够运行基本的 PPO 智能体，在资源分配模拟环境中进行探索。\n\n\n\n\n\n第11周 (共2次课)\n\n第一次课: PPO 算法的改进与应用\n\n内容：\n\n讨论 PPO 算法的改进方向 (例如：PPO-Clip, PPO-Penalty 的选择)。\n探讨 PPO 算法在不同商业场景中的应用案例 (例如：推荐系统、动态定价、机器人控制等)。\n介绍 PPO 算法的变体和前沿研究方向。\n\n\n第二次课: 小组项目三：PPO 算法优化与问题解决\n\n实践：\n\n学生小组继续完善 PPO 算法代码，优化智能体在资源分配模拟环境中的表现，并尝试应用更高级的探索策略。\n\n指导：\n\n指导学生解决在更复杂算法和环境中遇到的问题。\n\n讨论：\n\n组织学生小组分享项目进展和遇到的问题，进行集体讨论和解答。\n\n\n\n\n\n第12周 (共2次课)\n\n第一次课: 强化学习前沿技术与发展趋势 / 小组项目四提交 / 优秀小组项目四讲解 (3组)\n\n内容：\n\n介绍强化学习领域的前沿技术和发展趋势，例如：模仿学习 (Imitation Learning)、逆强化学习 (Inverse Reinforcement Learning)、分层强化学习 (Hierarchical Reinforcement Learning) 等 (简单介绍思想，激发学生兴趣)。\n展望强化学习未来的发展方向和潜在突破。\n引导学生思考强化学习技术可能对商业和社会带来的变革。\n\n管理：\n\n小组项目四：商业案例应用 提交 (第 12 周第一次课前)。\n优秀小组项目四讲解 (3 组)。\n\n\n第二次课: 小组项目四：商业案例应用 项目实践与问题解决\n\n实践：\n\n学生小组进行小组项目四：商业案例应用 的项目实践，进行环境搭建、算法选择、代码编写和实验验证。\n\n指导：\n\n指导学生解决在项目实践中遇到的问题。\n\n讨论：\n\n鼓励学生小组之间交流项目进展和经验，互相学习，共同进步。\n\n\n\n\n\n第13周 (共2次课)\n\n第一次课: 小组项目四成果展示 / 小组综合项目布置与答疑\n\n展示：\n\n小组项目四成果展示 (部分小组，时间允许的情况下，可以放在本次课，也可以放在第 12 周第二次课，根据实际情况灵活安排)。\n\n内容：\n\n布置小组综合项目：开放式商业问题，明确项目要求、评分标准和提交时间。\n\n答疑：\n\n针对小组综合项目进行初步答疑，解答学生关于项目选题、方向等问题。\n\n\n第二次课: 小组综合项目 - 开放式商业问题介绍与选题\n\n内容：\n\n详细介绍小组综合项目：开放式商业问题，鼓励学生结合课程所学知识和技能，以及自身专业背景和兴趣，选择具有实际商业价值和应用前景的项目。\n\n指导：\n\n指导学生小组进行小组综合项目选题，鼓励创新性和个性化。\n\n分组：\n\n学生维持小组队伍，开始讨论小组综合项目选题和分工。\n\n\n\n\n\n\n第四阶段 (14-16周): 小组项目实战与商业应用拓展\n\n第14-15周 (共4次课): 小组综合项目开发与指导\n\n内容：\n\n学生以小组为单位，进行小组综合项目的设计、开发和实验。\n\n指导：\n\n教师在课堂上提供项目指导和技术支持，解答学生在项目开发过程中遇到的问题。\n\n协作：\n\n鼓励学生充分利用 AI 辅助编程工具，提高开发效率，加强团队协作。\n\n检查：\n\n第 15 周进行小组综合项目中期检查，了解项目进展，及时发现和解决问题。\n\n\n\n\n第15周 (共2次课)\n\n第一次课: (内容待定)\n第二次课: 小组综合项目准备与答疑\n\n内容：\n\n学生小组继续进行小组综合项目的开发和完善，准备项目展示材料。\n\n答疑：\n\n教师提供项目答疑和指导，帮助学生解决项目开发中遇到的最后问题。\n\n\n\n\n\n第16周 (共2次课): 小组综合项目展示与总结\n\n第一次课: 小组综合项目展示 / 小组综合项目提交 / 小组综合项目展示 (全部小组)\n\n展示：\n\n各小组进行项目成果展示，讲解项目背景、问题定义、算法选择、实验结果和商业价值。\n\n答辩：\n\n接受教师和同学的提问和点评。\n\n管理：\n\n小组综合项目：开放式商业问题 提交 (第 16 周第一次课前)。\n\n\n第二次课: 课程总结与未来展望\n\n总结：\n\n回顾课程内容，总结强化学习的核心概念、算法和应用。\n\n展望：\n\n展望强化学习在商业领域的未来发展前景，鼓励学生继续深入学习和探索。\n\n伦理讨论：\n\n探讨强化学习在商业应用中可能引发的伦理问题，例如：算法歧视、数据隐私、自动化对就业的影响等，培养学生的社会责任感和批判性思维。\n\n评估：\n\n进行课程评估和反馈，收集学生对课程的意见和建议，为课程改进提供参考。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#项目设置-贯穿课程",
    "href": "syllbus.html#项目设置-贯穿课程",
    "title": "课程大纲",
    "section": "项目设置 (贯穿课程)",
    "text": "项目设置 (贯穿课程)\n\n小组项目 (共5个)\n\n小组项目一 (迷宫寻宝 Grid World)\n\n学习目标：掌握 Q-Learning 算法的原理和基本实现，理解探索与利用的概念。\n周期：2 周\n提交时间：第 4 周第一次课前\n\n小组项目二 (平衡杆 CartPole)\n\n学习目标：掌握 DQN 算法的原理和使用，理解经验回放和目标网络的作用，初步接触神经网络。\n周期：2 周\n提交时间：第 6 周第一次课前\n\n小组项目三 (资源分配 模拟环境)\n\n学习目标：掌握策略梯度方法 (PPO) 的原理和应用，理解策略梯度与价值方法的区别，学习处理连续动作空间问题。\n周期：2 周\n提交时间：第 8 周第一次课前\n\n小组项目四 (商业案例应用)\n\n学习目标：综合运用所学强化学习算法和工具，解决实际商业问题，提升问题建模和方案设计能力。\n周期：2 周\n提交时间：第 12 周第一次课前\n\n小组综合项目 (开放式商业问题)\n\n学习目标：培养学生的团队合作、项目管理和实际问题解决能力，提升综合应用强化学习知识的能力。\n周期：3 周\n提交时间：第 16 周第一次课前\n\n小组人数：建议每组 2-3 人。\n项目形式: 小组合作完成，包括项目方案设计、环境搭建、算法实现、实验验证、结果分析和项目报告。 每次小组项目提交后，选取 3 个优秀小组在课堂上进行项目讲解和展示，分享经验和成果。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#ai-辅助编程工具",
    "href": "syllbus.html#ai-辅助编程工具",
    "title": "课程大纲",
    "section": "AI 辅助编程工具",
    "text": "AI 辅助编程工具\n\n代码辅助: GitHub Copilot, Tabnine 等\n\n功能：代码自动补全、代码片段生成、代码错误检查等，提高编程效率，降低编程难度。\n\n强化学习库: Gymnasium (原 OpenAI Gym)\n\n功能：提供各种强化学习环境，方便学生进行算法测试和验证。\n\n算法库: Stable Baselines3\n\n功能：封装了多种常用的强化学习算法 (Q-Learning, DQN, PPO 等)，易于使用，方便学生快速上手。\n\n可视化工具: TensorBoard 等\n\n功能：方便学生监控强化学习训练过程，分析实验结果，调试算法。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "syllbus.html#考核方式",
    "href": "syllbus.html#考核方式",
    "title": "课程大纲",
    "section": "考核方式",
    "text": "考核方式\n\n小组项目一 ~ 四 (40%): 根据项目完成质量、代码质量、实验结果和项目报告进行评分，平均分配到四个项目。\n小组综合项目 (20%): 根据项目创新性、实用性、完成度、展示效果和项目报告进行评分 (小组内成员评分可以考虑组内互评和贡献度)。\n课堂参与 (10%): 根据课堂讨论、提问、作业完成情况等进行评分，鼓励学生积极参与课堂互动。\n期末考试 (30%): 期末考试采用闭卷形式，重点考察学生对强化学习基础理论的掌握程度。",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>课程大纲</span>"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "第一次课：什么是强化学习？",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第一次课什么是强化学习",
    "href": "week1.html#第一次课什么是强化学习",
    "title": "第一周：强化学习入门",
    "section": "强化学习基本流程",
    "text": "核心定义\n\n\n\n强化学习是一种通过与环境交互来学习的机器学习方法。智能体通过不断尝试和犯错来学习最优策略。\n\n\n\n1. 强化学习的核心概念\n\n\n\n\n\n\n以电商推荐系统为例理解核心概念\n\n\n\n\n智能体 (Agent)\n\n负责学习和做出决策的实体\n例如：推荐算法就是一个智能体，它决定向用户推荐什么商品\n\n\n\n环境 (Environment)\n\n智能体所处的外部世界\n例如：用户群体、商品库、市场状况等\n环境会对智能体的动作做出响应\n\n\n\n状态 (State)\n\n环境在某一时刻的描述\n例如：用户的历史浏览记录、搜索关键词、当前页面停留时间等\n智能体根据状态来决定下一步动作\n\n\n\n动作 (Action)\n\n智能体可以采取的操作\n例如：推荐特定商品、调整商品展示顺序、发送个性化优惠券等\n\n\n\n奖励 (Reward)\n\n环境对智能体动作的反馈\n例如：用户点击推荐商品(正奖励)、直接关闭页面(负奖励)\n奖励信号指导智能体改进决策\n\n\n\n\n\n\n\n\n\n\n强化学习交互过程可视化\n\n\n\n\n\n\n强化学习交互过程\n\n\n\n强化学习基本流程\n\n智能体观察环境，获取当前状态\n基于当前状态和策略选择一个动作\n执行动作，与环境交互\n环境转移到新状态，并给予智能体奖励\n智能体根据获得的奖励更新其策略\n重复上述过程，直到学习收敛或达到终止条件\n\n\n\n\n\n\n2. 机器学习方法对比\n\n\n\n\n\n\n学习方法对比\n\n\n\n\n\n\n机器学习方法对比\n\n\n\n\n\n\n\n\n\n\n三种学习方法的本质差异\n\n\n\n监督学习：有”老师”（标签）直接告诉模型正确答案，模型通过对比其预测与正确答案之间的差距来学习。\n无监督学习：没有”老师”，模型需要自己从数据中寻找规律和模式，不依赖外部反馈。\n强化学习：有”评价者”而非”老师”，评价者只告诉模型其行为的好坏（奖励信号），但不直接告诉正确答案，模型需要通过尝试和探索来找到获取最大奖励的策略。\n\n\n\n\n3. 商业应用案例详解\n\n\n\n\n\n\n智能推荐系统\n\n\n\n\n应用场景：电商平台、内容平台\n实现方式：\n\n收集用户行为数据作为状态\n推荐算法作为智能体\n用户反馈作为奖励信号\n\n商业价值：\n\n提高用户转化率\n增加平台收入\n改善用户体验\n\n\n\n\n\n\n\n\n\n\n动态定价系统\n\n\n\n\n应用场景：酒店预订、网约车平台\n实现方式：\n\n市场供需作为状态\n价格调整作为动作\n成交量和收入作为奖励\n\n商业价值：\n\n优化收入管理\n平衡供需关系\n提高资源利用率",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#强化学习基本流程",
    "href": "week1.html#强化学习基本流程",
    "title": "第一周：强化学习入门",
    "section": "",
    "text": "智能体观察环境，获取当前状态\n基于当前状态和策略选择一个动作\n执行动作，与环境交互\n环境转移到新状态，并给予智能体奖励\n智能体根据获得的奖励更新其策略\n重复上述过程，直到学习收敛或达到终止条件",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "href": "week1.html#第二次课ai辅助编程入门与平衡杆实践",
    "title": "第一周：强化学习入门",
    "section": "第二次课：AI辅助编程入门与平衡杆实践",
    "text": "第二次课：AI辅助编程入门与平衡杆实践\n\n1. AI辅助编程工具入门\n\n\n\n\n\n\n注意事项\n\n\n\nAI辅助编程工具可以显著提高编程效率，但要注意理解生成的代码逻辑\n\n\n\n\n\n\n\n\nGitHub Copilot 安装步骤\n\n\n\n\n打开 VS Code\n转到扩展市场\n搜索 “GitHub Copilot”\n点击安装\n\n\n\n\n\n\n\n\n\n实用技巧\n\n\n\n\n使用清晰的注释引导生成\n分步骤编写复杂逻辑\n及时检查生成的代码\n理解并修改不合适的建议\n\n\n\n\n\n2. 强化学习环境搭建\n\n\n\n\n\n\nGymnasium 库基础使用\n\n\n\n# 安装 Gymnasium\npip install gymnasium\n\n# 基本使用示例\nimport gymnasium as gym\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 获取环境信息\nprint(f\"动作空间: {env.action_space}\")\nprint(f\"状态空间: {env.observation_space}\")\n\n\n\n\n\n\n\n\nStable Baselines3 库基础使用\n\n\n\n# 安装 Stable Baselines3\npip install stable-baselines3\n\n# 基本使用示例\nfrom stable_baselines3 import DQN\n\n# 创建模型\nmodel = DQN(\"MlpPolicy\", env, verbose=1)\n\n# 训练模型\nmodel.learn(total_timesteps=10000)\n\n\n\n\n3. CartPole 环境实践\n\n\n\n\n\n\nCartPole 环境说明\n\n\n\nCartPole 是强化学习入门的经典环境，目标是通过左右移动小车来保持杆子平衡\n\n\n\n\n\n\n\n\nCartPole 环境可视化\n\n\n\n\n\n\nCartPole环境\n\n\n\n\n\n\n\n\n\n\n环境参数\n\n\n\nimport gymnasium as gym\nimport numpy as np\n\n# 创建环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 环境参数说明\nprint(\"状态空间包含4个连续值:\")\nprint(\"- 小车位置: [-4.8, 4.8]\")\nprint(\"- 小车速度: [-∞, ∞]\")\nprint(\"- 杆子角度: [-0.418, 0.418]\")\nprint(\"- 杆子角速度: [-∞, ∞]\")\n\nprint(\"\\n动作空间包含2个离散动作:\")\nprint(\"- 0: 向左推\")\nprint(\"- 1: 向右推\")\n\n\n\n\n\n\n\n\n随机动作示例代码\n\n\n\n# 运行随机策略\nobservation, info = env.reset()\ntotal_reward = 0\n\nfor step in range(100):\n    # 随机选择动作\n    action = env.action_space.sample()\n    \n    # 执行动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    total_reward += reward\n    \n    # 打印当前状态\n    print(f\"Step {step}: State = {observation}, Reward = {reward}\")\n    \n    # 判断是否需要重置\n    if terminated or truncated:\n        print(f\"Episode finished after {step + 1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()\n\n\n\n\n\n\n\n\n简单规则策略实现示例\n\n\n\n对于CartPole任务，我们可以实现一个基于当前状态的简单规则策略，而不是随机选择动作：\ndef simple_rule_policy(observation):\n    \"\"\"\n    基于杆子角度和角速度的简单规则策略\n    \n    参数：\n        observation: 环境观测值，包含4个状态变量\n        \n    返回：\n        action: 0(向左推)或1(向右推)\n    \"\"\"\n    # 解析观测值\n    cart_position, cart_velocity, pole_angle, pole_velocity = observation\n    \n    # 如果杆子正在向右倾斜，则向右推车（让杆子回到中心）\n    if pole_angle &gt; 0:\n        return 1\n    # 如果杆子正在向左倾斜，则向左推车\n    else:\n        return 0\n\n# 使用规则策略运行环境\nobservation, info = env.reset()\ntotal_reward = 0\n\nfor step in range(1000):  # 设置更大的步数上限\n    # 使用规则策略选择动作\n    action = simple_rule_policy(observation)\n    \n    # 执行动作\n    observation, reward, terminated, truncated, info = env.step(action)\n    total_reward += reward\n    \n    # 判断是否需要重置\n    if terminated or truncated:\n        print(f\"Episode finished after {step + 1} steps\")\n        print(f\"Total reward: {total_reward}\")\n        break\n\nenv.close()\n这个简单的规则策略基于杆子的倾斜角度来决定将小车向哪个方向推动，虽然简单，但通常比随机策略能获得更好的性能。在强化学习中，我们的目标是通过学习来自动发现这样的策略，甚至找到更好的策略。\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n环境配置\n\n安装 Python 3.8+\n配置 VS Code 和 GitHub Copilot\n安装所需 Python 库\n\n编程练习\n\n修改随机动作示例，尝试实现简单的规则策略\n记录并分析不同策略的表现\n使用 AI 辅助工具优化代码\n\n思考题\n\n强化学习如何应用到你的专业领域？\nCartPole 环境中的状态空间设计有什么特点？\n为什么需要重置环境？\n\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料\n\nGymnasium 官方文档\nStable Baselines3 入门教程\n马尔可夫决策过程基础概念\n\n视频资源\n\nCartPole 环境详解\n强化学习基础概念讲解\n\n下周预习重点\n\n马尔可夫决策过程\n价值函数与策略函数\nGrid World 环境介绍",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>第一周：强化学习入门</span>"
    ]
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "第一次课：强化学习框架与迷宫环境 (Grid World) 搭建",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "href": "week2.html#第一次课强化学习框架与迷宫环境-grid-world-搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "",
    "text": "马尔可夫决策过程 (MDP)\n\n\n\n马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习的核心框架，用于形式化描述智能体与环境的交互过程。\n\n\n\n\n\n\n\n\nMDP 的核心要素\n\n\n\n\n状态 (State, S)\n\n环境的描述，包含了智能体做出决策所需的信息\n马尔可夫性质: 当前状态包含了所有历史信息，未来的状态只依赖于当前状态和动作，而与过去的历史无关\n在迷宫环境中，状态可以是智能体在迷宫中的位置坐标\n\n\n\n动作 (Action, A)\n\n智能体在每个状态下可以采取的行为\n在迷宫环境中，动作可以是向上、下、左、右移动\n\n\n\n转移概率 (Transition Probability, P)\n\n智能体在状态 \\(s\\) 采取动作 \\(a\\) 后，转移到下一个状态 \\(s'\\) 的概率\n\\(P(s'|s, a)\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 的概率\n在确定性迷宫环境中，转移概率是确定的；在非确定性环境中可能存在随机性\n\n\n\n奖励 (Reward, R)\n\n智能体在与环境交互后获得的反馈信号，用于评价动作的好坏\n\\(R(s, a, s')\\) 表示在状态 \\(s\\) 采取动作 \\(a\\) 转移到状态 \\(s'\\) 后获得的奖励\n在迷宫寻宝游戏中，到达宝藏获得正奖励，撞墙或陷阱获得负奖励\n\n\n\n策略 (Policy, \\(\\pi\\))\n\n智能体根据当前状态选择动作的规则，可以是确定性的或随机性的\n\\(\\pi(a|s)\\) 表示在状态 \\(s\\) 下选择动作 \\(a\\) 的概率\n强化学习的目标是学习最优策略，使得智能体获得最大的累积奖励\n\n\n\n\n\n\n\n\n\n\nMDP 过程示意图\n\n\n\n\n\n\n马尔可夫决策过程\n\n\n\n\n\n\n\n\n\n\n价值函数 (Value Function)\n\n\n\n价值函数用于评估在给定状态或状态-动作对下，未来预期累积奖励的期望值。价值函数是强化学习算法的核心概念之一。\n\nV 函数 (State Value Function)\n\n\\(V_{\\pi}(s)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，未来可以获得的期望累积奖励\nV 函数评估的是状态的价值，即处于某个状态的好坏程度\nV 函数越大，表示当前状态越好，未来可以获得的期望奖励越高\n\n\n\nQ 函数 (Action Value Function)\n\n\\(Q_{\\pi}(s, a)\\) 表示在策略 \\(\\pi\\) 下，从状态 \\(s\\) 出发，选择动作 \\(a\\) 后的期望累积奖励\nQ 函数评估的是状态-动作对的价值\nQ 函数越大，表示在当前状态下，该动作越好\n\n\n\nV 函数和 Q 函数的关系\n\\(V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) Q_{\\pi}(s, a)\\)\n\n\n\n\n\n\n\n\n\n价值函数的数学表达\n\n\n\n\nV 函数的数学定义\n\\[V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s \\right]\\]\n\n\\(\\gamma\\) 是折扣因子，取值范围 [0, 1]\n\\(R_t\\) 是在时间步 t 获得的奖励\n\\(\\mathbb{E}_{\\pi}\\) 表示在策略 \\(\\pi\\) 下的期望\n\n\n\nQ 函数的数学定义\n\\[Q_{\\pi}(s, a) = \\mathbb{E}_{\\pi}\\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a \\right]\\]\n\n\n贝尔曼方程\n贝尔曼方程是强化学习中的核心等式，描述了当前状态的价值与下一个状态价值之间的关系：\nV 函数的贝尔曼方程： \\[V_{\\pi}(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V_{\\pi}(s')]\\]\nQ 函数的贝尔曼方程： \\[Q_{\\pi}(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q_{\\pi}(s',a')]\\]\n\n\n最优价值函数\n强化学习的目标是找到最优策略，对应的最优价值函数定义为：\n最优状态价值函数： \\[V^*(s) = \\max_{\\pi} V_{\\pi}(s) = \\max_{a} Q^*(s,a)\\]\n最优动作价值函数： \\[Q^*(s,a) = \\max_{\\pi} Q_{\\pi}(s,a) = \\mathbb{E}\\left[R_{t+1} + \\gamma \\max_{a'} Q^*(S_{t+1},a') | S_t=s, A_t=a\\right]\\]\n如果我们知道最优价值函数，最优策略可以通过选择每个状态下价值最高的动作得到： \\[\\pi^*(s) = \\arg\\max_{a} Q^*(s,a)\\]\n\n\n\n\n\n\n\n\n\n探索与利用的平衡\n\n\n\n探索 (Exploration) 与利用 (Exploitation) 是强化学习中一个核心的权衡问题：\n\n探索 (Exploration)\n\n尝试新的动作，探索未知的状态和动作空间\n可能发现更优策略，但也可能浪费时间在无用区域\n\n\n\n利用 (Exploitation)\n\n根据已知经验选择当前最优动作\n保证稳定收益，但可能错过更好的策略\n\n\n\n\\(\\epsilon\\)-greedy 策略\n\n以概率 \\(\\epsilon\\) 随机选择动作 (探索)\n以概率 \\(1-\\epsilon\\) 选择当前最优动作 (利用)\n\\(\\epsilon\\) 随训练进行逐渐减小\n\n\n\n\n\n\n\n\n\n\n探索与利用的平衡图示\n\n\n\n\n\n\n探索与利用的平衡\n\n\n\n\n\n\n\n\n\n\n商业案例：新市场尝试 vs. 现有市场深耕\n\n\n\n\n新市场尝试 (探索)\n\n进入新的市场领域\n开发新的产品线\n拓展新的客户群体\n\n\n\n现有市场深耕 (利用)\n\n优化现有产品\n提高客户满意度\n提升市场份额\n\n\n\n平衡策略\n\n双元策略：同时进行探索和利用\n阶段性策略：不同发展阶段侧重不同策略\n\n\n\n\n\n\n\n\n\n\nGrid World 环境搭建\n\n\n\n\nGymnasium 基础使用\n# 安装 Gymnasium\n# pip install gymnasium\n\nimport gymnasium as gym\n\n# 创建 CartPole 环境\nenv = gym.make(\"CartPole-v1\", render_mode=\"human\")\n\n# 重置环境\nobservation, info = env.reset()\n\n# 随机选择动作\naction = env.action_space.sample()\n\n# 执行动作\nobservation, reward, terminated, truncated, info = env.step(action)\n\n# 关闭环境\nenv.close()\n\n\n自定义 Grid World 环境\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\n\nclass GridWorldEnv(gym.Env):\n    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n\n    def __init__(self, render_mode=None, size=5):\n        super().__init__()\n        self.size = size  # Grid world size\n        self.window_size = 512  # PyGame window size\n\n        # 动作空间：上下左右\n        self.action_space = spaces.Discrete(4)\n        # 观测空间：智能体位置\n        self.observation_space = spaces.Discrete(size * size)\n\n        # Grid world 地图\n        self._grid_map = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 2, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0]\n        ])\n\n        # 初始化位置信息\n        self._target_location = np.array([2, 2])\n        self._trap_location = np.array([4, 4])\n        self._agent_start_location = np.array([0, 0])\n        self._agent_location = np.array([0, 0])\n\n        self.render_mode = render_mode\n\n        # 初始化渲染\n        if render_mode is not None:\n            import pygame\n            pygame.init()\n            pygame.font.init()\n            self.window = pygame.display.set_mode(\n                (self.window_size, self.window_size)\n            )\n            self.clock = pygame.time.Clock()\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完成 Grid World 环境的其他方法实现：\n\nstep() 方法：处理动作执行和奖励计算\nrender() 方法：可视化当前环境状态\nclose() 方法：清理资源\n\n实现一个简单的策略：\n\n使用 \\(\\epsilon\\)-greedy 策略\n记录并分析智能体表现\n\n思考题：\n\nMDP 的马尔可夫性质在实际应用中是否总是成立？\n如何在你的专业领域应用探索与利用的概念？\nGrid World 环境中，不同的奖励设计会如何影响智能体的行为？\n\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料：\n\nSutton & Barto 强化学习教材第3章\nGymnasium 自定义环境教程\nPyGame 基础教程\n\n视频资源：\n\nDavid Silver 强化学习课程第2讲\nGrid World 环境实现教学视频\n\n下周预习重点：\n\n动态规划算法\n值迭代与策略迭代\nQ-learning 算法基础",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "href": "week2.html#第二次课小组项目一迷宫寻宝-grid-world-环境搭建",
    "title": "第二周：强化学习框架与迷宫环境",
    "section": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建",
    "text": "第二次课：小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n1. 小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n项目目标：\n\n以小组为单位，基于第一次课的代码框架，独立完成迷宫环境 (Grid World) 的搭建。\n扩展迷宫地图，设计更复杂的迷宫场景 (例如：增加更多障碍物、宝藏、陷阱等)。\n实现基本的环境渲染，能够可视化智能体在迷宫中的探索过程。\n\n代码框架：\n\n提供第一次课中 GridWorldEnv 类的代码框架 (作为项目的基础)。\n小组需要自行完成代码的补全、修改和扩展。\n\nAI 辅助工具：\n\n鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具，提高开发效率。\n但强调：AI 工具是辅助手段，学生需要理解代码逻辑，不能完全依赖 AI 工具生成代码，而忽略代码理解和调试。\n\n项目提交：\n\n小组提交完整的 Grid World 环境代码 (Python 文件)。\n无需提交项目报告。\n\n\n\n\n2. 答疑与指导\n\n解答学生在环境搭建过程中遇到的问题。\n重点关注：\n\nGymnasium 库的使用方法。\n自定义环境类的结构和接口。\n状态空间、动作空间、奖励函数的设计。\n环境渲染的实现。\nAI 辅助工具的使用技巧。\n\n\n\n\n3. 布置小组项目一：迷宫寻宝 (Grid World) 环境搭建\n\n明确项目要求、提交时间和评分标准 (本次项目不评分，作为后续项目的基础)。\n鼓励小组积极探索、尝试，遇到问题及时提问。\n建议小组：\n\n提前开始项目，预留充足的开发和调试时间。\n分工合作，提高开发效率。\n充分利用 AI 辅助工具，但也要注重代码理解和调试能力。\n相互交流、学习，共同解决问题。\n\n\n\n\n课后作业\n\n完成小组项目一：迷宫寻宝 (Grid World) 环境搭建。\n思考题：\n\n在 Grid World 环境中，如何设计更有效的奖励函数，以引导智能体更快地找到宝藏？\n如果迷宫地图非常大，状态空间会变得很大，对强化学习算法会产生什么影响？\n如何使用 AI 辅助工具更高效地进行强化学习代码开发？\n\n\n\n\n预习资料\n\n阅读材料：\n\nGymnasium 官方文档 (自定义环境部分)。\n强化学习算法基础：Q-Learning 算法初步。\n探索-利用平衡的更多策略 (例如：\\(\\epsilon\\)-greedy 退火策略、UCB 算法等)。\n\n视频资源：\n\nGrid World 环境搭建详解。\nQ-Learning 算法原理讲解 (初步了解)。\n探索与利用的平衡策略讲解。\n\n下周预习重点：\n\nQ-Learning 算法原理和步骤。\nQ-Table 的更新规则。\n使用 Q-Learning 算法解决 Grid World 迷宫寻宝问题。\n\n\n\n请注意:\n\n代码框架: 第一次课的代码框架，指的是第一次课中 GridWorldEnv 类的代码。学生需要基于这个代码框架进行扩展和修改，完成小组项目一。\n环境注册: 请确保在运行 Grid World 环境代码之前，已经执行了环境注册代码 (register(...))。可以将注册代码放在单独的文件中，或者放在运行环境代码的脚本文件的开头。\nAI 辅助工具: 鼓励学生使用 AI 辅助工具，但务必强调理解代码逻辑的重要性，避免过度依赖 AI 工具。\n小组项目: 小组项目一旨在让学生熟悉 Gymnasium 库和自定义环境的流程，为后续的强化学习算法实践打下基础。\n\n\n\n\n\n\n\nGrid World 环境可视化\n\n\n\n\n\n\nGrid World 环境\n\n\n\n\n\n\n\n\n\n\n完整的Grid World环境实现 - step()方法\n\n\n\ndef step(self, action):\n    \"\"\"\n    执行一步动作，返回下一个状态、奖励、是否终止等信息\n    \n    参数:\n        action: 动作，0=上, 1=右, 2=下, 3=左\n        \n    返回:\n        observation: 新的状态\n        reward: 获得的奖励\n        terminated: 是否到达终止状态（目标或陷阱）\n        truncated: 是否达到最大步数\n        info: 额外信息\n    \"\"\"\n    # 动作映射到方向变化 (行,列)\n    direction = {\n        0: (-1, 0),  # 上\n        1: (0, 1),   # 右\n        2: (1, 0),   # 下\n        3: (0, -1)   # 左\n    }\n    \n    # 计算新位置\n    delta_row, delta_col = direction[action]\n    new_position = self._agent_location + np.array([delta_row, delta_col])\n    \n    # 检查是否越界或撞墙\n    if (\n        0 &lt;= new_position[0] &lt; self.size \n        and 0 &lt;= new_position[1] &lt; self.size \n        and self._grid_map[new_position[0], new_position[1]] != 1\n    ):\n        self._agent_location = new_position\n    \n    # 获取当前位置的单元格类型\n    current_cell = self._grid_map[self._agent_location[0], self._agent_location[1]]\n    \n    # 初始化奖励和终止状态\n    reward = -0.1  # 每一步的小惩罚，鼓励快速到达目标\n    terminated = False\n    truncated = False\n    \n    # 根据当前位置计算奖励和是否终止\n    if np.array_equal(self._agent_location, self._target_location):\n        # 到达目标\n        reward = 1.0\n        terminated = True\n    elif np.array_equal(self._agent_location, self._trap_location):\n        # 掉入陷阱\n        reward = -1.0\n        terminated = True\n    \n    # 将智能体位置转换为离散观测空间的索引\n    observation = self._agent_location[0] * self.size + self._agent_location[1]\n    \n    # 如果需要渲染，渲染当前帧\n    if self.render_mode == \"human\":\n        self.render()\n    \n    return observation, reward, terminated, truncated, {}\n\n\n\n\n\n\n\n\nQ-Learning 算法流程\n\n\n\n\n\n\nQ-Learning 算法流程\n\n\n\n\n\n\n\n\n\n\n价值迭代算法\n\n\n\n\n\n\n价值迭代算法",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>第二周：强化学习框架与迷宫环境</span>"
    ]
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "第一次课：Q-Learning 算法详解",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第一次课q-learning-算法详解",
    "href": "week3.html#第一次课q-learning-算法详解",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "",
    "text": "Q-Learning 算法简介\n\n\n\nQ-Learning 是一种基于价值的离线强化学习算法，用于学习最优 Q 函数，从而得到最优策略。\n\n核心特点\n\n核心思想: 通过不断试错和更新 Q-Table，逐步逼近最优 Q 函数\n离线特性: 学习的策略与实际执行的策略可以不同\n最优 Q 函数: 表示在状态 \\(s\\) 下，执行动作 \\(a\\) 后的最大期望累积奖励\n最优策略: 在每个状态下选择 Q 函数值最大的动作\n\n\n\n\n\n\n\n\n\n\nQ-Table 详解\n\n\n\nQ-Table 是 Q-Learning 算法中用于存储 Q 函数值的表格。\n\n表格结构\n\n\n\n\n\n\n\n\n\n\n状态 (State)\n动作 1 (Action 1)\n动作 2 (Action 2)\n…\n动作 n (Action n)\n\n\n\n\n状态 1 (S1)\n\\(Q(S1, A1)\\)\n\\(Q(S1, A2)\\)\n…\n\\(Q(S1, An)\\)\n\n\n状态 2 (S2)\n\\(Q(S2, A1)\\)\n\\(Q(S2, A2)\\)\n…\n\\(Q(S2, An)\\)\n\n\n…\n…\n…\n…\n…\n\n\n状态 m (Sm)\n\\(Q(Sm, A1)\\)\n\\(Q(Sm, A2)\\)\n…\n\\(Q(Sm, An)\\)\n\n\n\n\n\n关键操作\n\n初始化: 通常初始化为 0 或小的随机值\n更新: 在交互过程中不断更新 Q 值\n查询: 根据当前状态查询对应的 Q 值\n\n\n\n\n\n\n\n\n\n\nQ-Learning 更新规则\n\n\n\nQ-Learning 使用时序差分学习方法来更新 Q 值，是一种无模型的强化学习方法。\n\n更新公式\n新的 Q(s, a)  &lt;-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))\n\n\n关键参数\n\n学习率 (\\(\\alpha\\)): 控制更新幅度 [0, 1]\n\n\\(\\alpha\\) 较大：学习快但不稳定\n\\(\\alpha\\) 较小：学习慢但稳定\n\n折扣因子 (\\(\\gamma\\)): 控制未来奖励重要性 [0, 1]\n\n\\(\\gamma\\) 接近 0：重视即时奖励\n\\(\\gamma\\) 接近 1：重视未来奖励\n\n\n\n\n\n\n\n\n\n\n\n算法流程图\n\n\n\n\n\n\n\n\ngraph LR\n    S[\"状态 s\"] --&gt; A[\"动作 a\"]\n    A --&gt; E[\"环境\"]\n    E --&gt; SP[\"新状态 s'\"]\n    E --&gt; R[\"奖励 r\"]\n    SP --&gt; QP[\"最大Q值\"]\n    R --&gt; TD[\"TD目标\"]\n    QP --&gt; TD\n    TD --&gt; U[\"更新Q表\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n算法步骤详解\n\n\n\n\n初始化 Q-Table\n\n创建状态-动作表格\n初始化所有 Q 值为 0\n\n循环训练 Episodes\n\n重置环境到初始状态\n在每个 episode 中进行多步交互\n直到达到目标或最大步数\n\n每步操作\n\n选择动作（探索策略）\n执行动作获取反馈\n更新 Q-Table\n更新当前状态\n\n训练完成\n\nQ-Table 收敛或达到预设次数\n提取最优策略\n\n\n\n\n\n\n\n\n\n\n动态定价案例\n\n\n\n\n场景设定\n\n商品: 单一商品销售\n状态: 库存水平（高/中/低）\n动作: 价格调整（涨/跌/维持）\n目标: 最大化累积利润\n\n\n\n奖励设计\n\n\n\n状态 (库存)\n动作 (价格调整)\n奖励 (利润)\n\n\n\n\n高库存\n降价\n+5 (销量增加)\n\n\n高库存\n维持原价\n+2 (正常销量)\n\n\n高库存\n涨价\n-1 (销量减少)\n\n\n中库存\n降价\n+3 (销量略增)\n\n\n中库存\n维持原价\n+4 (正常销量)\n\n\n中库存\n涨价\n+1 (销量略减)\n\n\n低库存\n降价\n-2 (缺货风险)\n\n\n低库存\n维持原价\n+6 (高利润率)\n\n\n低库存\n涨价\n+8 (更高利润率)\n\n\n\n\n\n\n\n\n\n\n\n\n动态定价代码示例\n\n\n\n# 初始化 Q-Table\nq_table = {}\n\n# 定义状态空间和动作空间\nstates = [\"High\", \"Medium\", \"Low\"]\nactions = [\"Increase\", \"Maintain\", \"Decrease\"]\n\n# Q-Learning 算法训练循环\nepisodes = 1000\nlearning_rate = 0.1\ndiscount_factor = 0.9\nepsilon = 0.1\n\nfor episode in range(episodes):\n    current_state = random.choice(states)\n    \n    for step in range(steps_per_episode):\n        # epsilon-greedy 策略选择动作\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.choice(actions)\n        else:\n            action = max(actions, key=lambda a: q_table.get((current_state, a), 0))\n            \n        # 获取奖励和下一状态\n        next_state = ...  # 状态转移\n        reward = get_reward(current_state, action)\n        \n        # 更新 Q 值\n        old_q = q_table.get((current_state, action), 0)\n        next_max_q = max([q_table.get((next_state, a), 0) for a in actions])\n        new_q = old_q + learning_rate * (reward + discount_factor * next_max_q - old_q)\n        q_table[(current_state, action)] = new_q\n        \n        current_state = next_state",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "href": "week3.html#第二次课小组项目一q-learning-算法编程实践",
    "title": "第三周：Q-Learning 算法详解与实践",
    "section": "第二次课：小组项目一：Q-Learning 算法编程实践",
    "text": "第二次课：小组项目一：Q-Learning 算法编程实践\n\n\n\n\n\n\n项目要求\n\n\n\n\n目标\n\n使用 Python 和 AI 工具编写 Q-Learning 算法\n应用于迷宫寻宝 (Grid World) 项目\n实现智能体自主探索和学习\n\n\n\n提交内容\n\n完整的 Q-Learning 算法代码\n修改后的 Grid World 环境代码（如有）\n可视化结果（可选）\n\n\n\n\n\n\n\n\n\n\n超参数调整指南\n\n\n\n\n关键超参数\n\n学习率 (\\(\\alpha\\))\n折扣因子 (\\(\\gamma\\))\n探索率 (\\(\\epsilon\\))\n\n\n\n调整方法\n\n经验调整\n\n基于经验和直觉\n快速但可能不是最优\n\n网格搜索\n\n遍历预定义参数组合\n全面但耗时\n\n随机搜索\n\n随机采样参数组合\n平衡效率和效果\n\n\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n完成 Q-Learning 算法实现\n调整超参数并记录效果\n分析不同参数对学习效果的影响\n尝试改进算法性能\n\n\n\n\n\n\n\n\n\n预习资料\n\n\n\n\n阅读材料\n\nSutton & Barto 第6章\nQ-Learning 相关论文\n\n视频资源\n\nDavid Silver RL Course\nQ-Learning 实现教程\n\n下周预习重点\n\nDQN 算法原理\n神经网络基础\nPyTorch 入门",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>第三周：Q-Learning 算法详解与实践</span>"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "第一次课：Q-Learning 算法优化与改进",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第一次课q-learning-算法优化与改进",
    "href": "week4.html#第一次课q-learning-算法优化与改进",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "",
    "text": "Q-Learning 算法的局限性\n\n\n\n\n状态空间爆炸问题\n\n状态空间过大时，Q-Table 规模庞大\n连续状态空间需要离散化处理\n存储和计算成本高\n\n\n\n探索-利用困境\n\n需要平衡探索新动作和利用已知最优动作\n\\(\\epsilon\\)-greedy 策略探索效率较低\n可能浪费时间在无用区域\n\n\n\n收敛性问题\n\n需要满足特定条件才能收敛\n环境可能不满足 MDP 假设\n可能收敛到局部最优解\n\n\n\n超参数敏感性\n\n性能受超参数影响大\n需要经验和技巧调整\n调整过程耗时耗力\n\n\n\n\n\n\n\n\n\n\n探索策略详解\n\n\n\n\n\\(\\epsilon\\)-greedy 策略\n\n以 \\(\\epsilon\\) 概率随机探索\n以 \\(1-\\epsilon\\) 概率选择最优动作\n简单但探索效率低\n\n\n\n\\(\\epsilon\\)-greedy 退火策略\n\n训练初期增加探索（大 \\(\\epsilon\\)）\n训练后期减少探索（小 \\(\\epsilon\\)）\n线性或指数衰减方式\n\n\n\n高级探索策略\n\nUCB 算法：考虑动作不确定性\nThompson Sampling：概率采样\nSoftmax 策略：基于 Q 值分布\n\n\n\n\n\n\n\n\n\n\nQ-Table 初始化技巧\n\n\n\n\n零值初始化\n\n简单易实现\n可能导致初始探索不足\n收敛速度较慢\n\n\n\n随机值初始化\n\n鼓励初始探索\n打破对称性\n需谨慎选择范围\n\n\n\n乐观初始化\n\n初始化为较大值\n鼓励探索未知动作\n适用于稀疏奖励环境\n\n\n\n基于领域知识\n\n利用先验知识\n加速学习过程\n通用性较差\n\n\n\n\n\n\n\n\n\n\n奖励函数设计原则\n\n\n\n\n基本原则\n\n与任务目标一致\n考虑稀疏vs密集奖励\n注意奖励尺度\n\n\n\n奖励类型\n\n稀疏奖励\n\n只在目标状态给予奖励\n更符合真实场景\n学习难度大\n\n密集奖励\n\n提供中间过程奖励\n加速学习过程\n需要careful设计\n\n\n\n\n示例（迷宫寻宝）\n# 稀疏奖励\nrewards = {\n    'goal': 10,      # 到达宝藏\n    'trap': -10,     # 陷阱\n    'default': 0     # 其他状态\n}\n\n# 密集奖励\nrewards = {\n    'goal': 10,      # 到达宝藏\n    'trap': -10,     # 陷阱\n    'step': -0.1,    # 每步惩罚\n    'distance': lambda d: 1/d  # 距离奖励\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week4.html#第二次课小组项目实践",
    "href": "week4.html#第二次课小组项目实践",
    "title": "第四周：Q-Learning 算法优化与改进",
    "section": "第二次课：小组项目实践",
    "text": "第二次课：小组项目实践\n\n\n\n\n\n\n项目优化方向\n\n\n\n\n算法优化\n\n实现退火探索策略\n优化 Q-Table 初始化\n改进奖励函数设计\n\n\n\n代码优化\n\n提高运行效率\n使用 NumPy 向量化\n添加可视化功能\n\n\n\n性能优化\n\n系统调整超参数\n记录实验结果\n分析优化效果\n\n\n\n\n\n\n\n\n\n\n调试技巧\n\n\n\n\n基本工具\n\nprint 语句跟踪\npdb 调试器使用\n日志记录分析\n\n\n\nAI 辅助\n\n使用 GitHub Copilot\n代码自动补全\n错误快速定位\n\n\n\n问题解决\n\n环境代码检查\n算法逻辑验证\n超参数调整\n奖励设计分析\n\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n继续优化项目代码\n准备项目报告\n预习下周内容：\n\nDouble Q-Learning\nExperience Replay\n深度强化学习基础\n\n\n\n\n\n\n\n\n\n\n下周预习重点\n\n\n\n\n项目报告准备\n答辩 PPT 制作\nSarsa 算法学习\n深度强化学习入门",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>第四周：Q-Learning 算法优化与改进</span>"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "第五周：项目一展示与深度 Q 网络初探",
    "section": "",
    "text": "第一次课：小组项目一展示与点评",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目一展示与深度 Q 网络初探</span>"
    ]
  },
  {
    "objectID": "week5.html#第一次课小组项目一展示与点评",
    "href": "week5.html#第一次课小组项目一展示与点评",
    "title": "第五周：项目一展示与深度 Q 网络初探",
    "section": "",
    "text": "项目展示规范\n\n\n\n\n每组展示时间：5-10分钟\n展示内容：问题定义、算法实现、实验结果、优化方法、遇到的挑战和解决方案\n展示后问答：5分钟\n\n\n\n\n\n\n\n\n\n项目评分标准\n\n\n\n\n技术实现 (40%)\n\n算法实现的正确性\n代码质量和结构\n探索策略的有效性\n参数调优的合理性\n\n\n\n实验结果 (30%)\n\n算法收敛性\n智能体表现\n可视化质量\n结果分析深度\n\n\n\n展示质量 (20%)\n\n展示清晰度\n技术表达准确性\n问题回答能力\n时间控制\n\n\n\n团队协作 (10%)\n\n任务分工合理性\n协作效率\n项目完整性\n\n\n\n\n\n\n\n\n\n\n优秀项目分享要点\n\n\n\n\n创新性解决方案\n有效的探索策略改进\n巧妙的奖励函数设计\n高效的算法实现\n直观的可视化展示\n\n\n\n\n\n\n\n\n\n常见问题与改进方向\n\n\n\n\nQ表设计不合理\n探索策略过于简单\n奖励函数设计不当\n参数调整不充分\n代码效率有待提高\n可视化展示不直观",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目一展示与深度 Q 网络初探</span>"
    ]
  },
  {
    "objectID": "week5.html#第二次课深度-q-网络-dqn-详解",
    "href": "week5.html#第二次课深度-q-网络-dqn-详解",
    "title": "第五周：项目一展示与深度 Q 网络初探",
    "section": "第二次课：深度 Q 网络 (DQN) 详解",
    "text": "第二次课：深度 Q 网络 (DQN) 详解\n\n\n\n\n\n\nQ-Learning 的局限性\n\n\n\n\n状态空间爆炸问题\n\n传统 Q 表无法处理大规模或连续状态空间\n例如：在 Atari 游戏中，即使假设每个像素只有两种颜色（例如黑白），84×84像素的图像输入也会产生超过2^(84*84)种不同状态。实际上，Atari 游戏通常是彩色的，状态空间会更加庞大。\n现实问题中状态往往是高维的，表格表示方法不可行\n\n\n\n泛化能力不足\n\n表格型 Q-Learning 无法泛化到未见过的状态\n每个状态都需要单独学习，样本利用率低\n无法有效利用状态之间的相似性和共享特征\n\n\n\n\n\n\n\n\n\n\n神经网络基础\n\n\n\n\n神经网络结构\n\n输入层：接收状态信息（如游戏屏幕像素或状态向量）\n隐藏层：提取特征，发现模式（通常使用ReLU激活函数）\n输出层：预测每个动作的 Q 值（通常使用线性激活函数）\n\n\n\n函数近似\n\n神经网络作为函数近似器 Q(s,a;θ)，θ是网络参数\n输入状态s，输出各动作a的价值估计\n通过随机梯度下降优化参数θ\n\n\n\n表示能力\n\n能够自动提取状态特征，无需人工设计\n通过共享参数捕捉状态间的相似性\n具有强大的泛化能力，能预测未见过状态的价值\n\n\n\n\n\n\n\n\n\n\n深度 Q 网络 (DQN) 原理\n\n\n\n\n基本思想\n\n用神经网络替代 Q 表格\n网络输入：状态向量\n网络输出：每个动作的 Q 值估计\n损失函数：均方误差 MSE(target_Q, predicted_Q)\n\n\n\n两大关键创新\n\n经验回放 (Experience Replay)\n\n存储交互经验元组 (s, a, r, s’)\n从经验缓冲区随机采样进行批量学习\n打破样本相关性，提高数据效率，稳定训练\n\n目标网络 (Target Network)\n\n维护两个网络：Q网络（频繁更新）和目标网络（定期更新）\nQ网络用于选择动作和当前估计\n目标网络用于计算目标Q值\n减少目标移动问题，提高训练稳定性\n\n\n\n\nDQN 算法流程\n# 初始化\nQ网络和目标网络（相同初始参数）\n经验回放缓冲区 D，容量为N\n\nfor 每个回合:\n    初始化状态 s\n    \n    for 每个时间步 t:\n        # 交互阶段\n        根据ε-贪婪策略选择动作 a（基于Q网络）\n        执行动作 a，获得奖励 r 和下一状态 s'\n        将经验 (s, a, r, s', done) 存入缓冲区 D\n        \n        # 学习阶段\n        从 D 中随机采样小批量经验\n        计算目标 Q 值：\n            若 s' 为终止状态: y = r\n            否则: y = r + γ * max_a' Q'(s', a')  # Q'是目标网络\n        \n        最小化损失: L = (Q(s, a) - y)²  # Q是Q网络\n        使用梯度下降更新Q网络参数\n        \n        s = s'\n        \n        # 定期更新目标网络\n        每 C 步: 目标网络参数 ← Q网络参数\n\n\nDQN 完整架构图\n\n    \n    \n    \n    \n    深度Q网络(DQN)架构图\n    \n    \n    \n    DQN智能体\n    \n    \n    \n    环境\n    (CartPole/Atari)\n    \n    \n    \n    Q网络\n    (频繁更新)\n    \n    \n    \n    目标网络\n    (定期更新)\n    \n    \n    \n    经验回放缓冲区\n    (存储和采样经验)\n    \n    \n    \n    选择动作\n    \n    \n    \n    经验\n    (s,a,r,s',done)\n    \n    \n    \n    随机采样批次\n    \n    \n    \n    目标Q值\n    \n    \n    \n    定期复制参数\n    \n    \n    \n    TD损失计算\n    (Q(s,a) - target)²\n    \n    \n    \n        \n            \n        \n    \n    \n    \n    \n    \n        ■ 神经网络\n        ■ 环境\n        ■ 经验回放\n        ■ 损失计算\n    \n\n上图展示了DQN算法的完整架构，包含了所有核心组件及其交互关系：\n\n环境与智能体交互：Q网络选择动作，环境返回新状态和奖励\n经验存储：经验元组(s,a,r,s’,done)被存储到经验回放缓冲区\n批量学习：从缓冲区随机采样经验批次用于训练\n目标计算：目标网络提供稳定的Q值估计\n网络更新：根据TD损失更新Q网络参数\n参数同步：定期将Q网络参数复制到目标网络\n\n这一架构设计解决了传统Q-learning在处理复杂状态空间时的局限性，通过两大创新（经验回放和目标网络）提高了学习稳定性和效率。\n\n\n\n\n\n\n\n\n\n经验回放详解\n\n\n\n\n核心作用\n\n打破样本相关性：连续采样的状态高度相关，随机采样打破这种相关性\n提高数据效率：每个经验可以被多次使用，减少环境交互次数\n稳定训练：平滑采样分布，减少参数更新的方差\n均衡经验分布：不同情景的经验都有机会被学习，避免过拟合\n\n\n\n实现要点\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)  # 固定容量的双端队列\n    \n    def add(self, state, action, reward, next_state, done):\n        # 存储经验元组\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        # 随机采样批次经验\n        experiences = random.sample(self.buffer, batch_size)\n        \n        # 解析为分离的数组，便于批量处理\n        states, actions, rewards, next_states, dones = zip(*experiences)\n        return np.array(states), np.array(actions), np.array(rewards), \\\n               np.array(next_states), np.array(dones)\n\n\n进阶版本\n\n优先经验回放 (Prioritized Experience Replay)：根据TD误差分配优先级，重要经验被更频繁采样\n分层经验回放 (Hindsight Experience Replay)：重新标记目标，从失败经验中学习\n\n\n\n\n\n\n\n\n\n\n目标网络详解\n\n\n\n\n核心原理\n\n在 Q-learning 中使用同一网络同时产生目标和当前估计会导致不稳定\n移动目标问题：当前网络参数θ影响目标值，目标值又用于更新θ，形成不稳定循环\n目标网络”冻结”目标值计算，降低训练不稳定性\n\n\n\n关键实现\n# 初始化\nq_network = create_model()      # 主网络\ntarget_network = create_model() # 目标网络\ntarget_network.set_weights(q_network.get_weights())  # 初始权重相同\n\n# 学习过程\ndef learn(experiences):\n    states, actions, rewards, next_states, dones = experiences\n    \n    # 使用目标网络计算下一状态的最大Q值\n    target_q_values = target_network.predict(next_states)\n    max_target_q = np.max(target_q_values, axis=1)\n    \n    # 计算TD目标\n    targets = rewards + (gamma * max_target_q * (1 - dones))\n    \n    # 更新主网络\n    current_q = q_network.predict(states)\n    for i, action in enumerate(actions):\n        current_q[i][action] = targets[i]\n    q_network.fit(states, current_q, epochs=1, verbose=0)\n    \n# 定期更新目标网络\nif step % update_frequency == 0:\n    target_network.set_weights(q_network.get_weights())\n\n\n更新策略\n\n硬更新：每C步完全复制参数（原始DQN方法）\n软更新：每步用小比例τ混合参数 θ_target = τ·θ + (1-τ)·θ_target（DDPG等算法采用）\n\n\n\n\n\n\n\n\n\n\nDQN 与 Q-Learning 对比\n\n\n\n\n相同点\n\n都是基于时序差分(TD)学习的价值函数方法\n都使用最大化动作价值函数(Q)的方法\n都采用ε-贪婪策略平衡探索与利用\n都应用折扣因子γ来权衡短期与长期奖励\n\n\n\n关键区别\n\n\n\n特性\n表格型 Q-Learning\n深度 Q 网络 (DQN)\n\n\n\n\n价值表示\nQ表格\n神经网络\n\n\n状态空间\n小规模离散状态\n大规模/连续状态\n\n\n更新方式\n单样本更新\n批量更新\n\n\n参数数量\n状态数×动作数\n神经网络权重数量\n\n\n稳定性技巧\n无\n经验回放+目标网络\n\n\n泛化能力\n无\n强\n\n\n样本效率\n低\n高\n\n\n计算复杂度\n低\n高\n\n\n\n\n\n\n\n\n\n\n\n\n重要扩展和改进\n\n\n\n\n双重DQN (Double DQN)\n\n解决Q值过高估计问题\n使用主网络选择动作，目标网络评估其价值\n\n\n\n决斗网络架构 (Dueling Network)\n\n将Q值分解为状态价值V(s)和优势函数A(s,a)\nQ(s,a) = V(s) + A(s,a) - mean(A(s,:))\n提高价值估计的稳定性\n\n\n\n分布式DQN (Distributional DQN)\n\n学习奖励分布而非单一期望值\n捕捉环境的随机性和不确定性\n\n\n\nRainbow DQN\n\n集成六种DQN改进技术\n包括优先经验回放、多步学习、噪声网络等\n\n\n\n\n\n\n\n\n\n\n课后作业\n\n\n\n\n阅读DQN原始论文：Playing Atari with Deep Reinforcement Learning\n下载并运行本周代码示例，熟悉经验回放和目标网络的实现方式\n选择一个简单环境（如CartPole），实现基础版DQN算法\n分析不同超参数（如探索率、更新频率）对训练性能的影响\n\n\n\n\n\n\n\n\n\n下周预习重点\n\n\n\n\nDQN变体算法（Double DQN、Dueling DQN）的工作原理\n连续动作空间强化学习算法（DDPG、SAC）的基础知识\n策略梯度方法的基本原理\n深度强化学习的稳定性和采样效率问题",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>第五周：项目一展示与深度 Q 网络初探</span>"
    ]
  }
]