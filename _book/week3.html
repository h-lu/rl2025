<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>第三周：Q-Learning 算法详解与实践 – 智能计算</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./week4.html" rel="next">
<link href="./week2.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xkm/sYwpb+ilR5gUw==" crossorigin="anonymous" referrerpolicy="no-referrer">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles/custom.css">
<link rel="stylesheet" href="styles/mermaid.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./week3.html"><span class="chapter-title">第三周：Q-Learning 算法详解与实践</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">智能计算</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程介绍</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./syllbus.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">课程大纲</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">第一周：强化学习入门</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">第二周：强化学习框架与迷宫环境</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week3.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">第三周：Q-Learning 算法详解与实践</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">第四周：Q-Learning 算法优化与改进</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#课程目标" id="toc-课程目标" class="nav-link active" data-scroll-target="#课程目标">课程目标</a></li>
  <li><a href="#第一次课q-learning-算法详解" id="toc-第一次课q-learning-算法详解" class="nav-link" data-scroll-target="#第一次课q-learning-算法详解">第一次课：Q-Learning 算法详解</a>
  <ul class="collapse">
  <li><a href="#q-learning-算法核心思想" id="toc-q-learning-算法核心思想" class="nav-link" data-scroll-target="#q-learning-算法核心思想">1. Q-Learning 算法核心思想</a></li>
  <li><a href="#q-table-q-值表" id="toc-q-table-q-值表" class="nav-link" data-scroll-target="#q-table-q-值表">2. Q-Table (Q 值表)</a></li>
  <li><a href="#q-learning-更新规则-时序差分学习" id="toc-q-learning-更新规则-时序差分学习" class="nav-link" data-scroll-target="#q-learning-更新规则-时序差分学习">3. Q-Learning 更新规则 (时序差分学习)</a></li>
  <li><a href="#q-learning-算法步骤流程" id="toc-q-learning-算法步骤流程" class="nav-link" data-scroll-target="#q-learning-算法步骤流程">4. Q-Learning 算法步骤流程</a></li>
  <li><a href="#动态定价案例-结合商业案例演示-q-learning-应用" id="toc-动态定价案例-结合商业案例演示-q-learning-应用" class="nav-link" data-scroll-target="#动态定价案例-结合商业案例演示-q-learning-应用">5. 动态定价案例 (结合商业案例，演示 Q-Learning 应用)</a></li>
  </ul></li>
  <li><a href="#第二次课小组项目一q-learning-算法编程实践" id="toc-第二次课小组项目一q-learning-算法编程实践" class="nav-link" data-scroll-target="#第二次课小组项目一q-learning-算法编程实践">第二次课：小组项目一：Q-Learning 算法编程实践</a>
  <ul class="collapse">
  <li><a href="#小组项目一q-learning-算法编程实践-迷宫寻宝-grid-world" id="toc-小组项目一q-learning-算法编程实践-迷宫寻宝-grid-world" class="nav-link" data-scroll-target="#小组项目一q-learning-算法编程实践-迷宫寻宝-grid-world">1. 小组项目一：Q-Learning 算法编程实践 (迷宫寻宝 Grid World)</a></li>
  <li><a href="#超参数讲解与调整-学习率折扣因子" id="toc-超参数讲解与调整-学习率折扣因子" class="nav-link" data-scroll-target="#超参数讲解与调整-学习率折扣因子">2. 超参数讲解与调整 (学习率、折扣因子)</a></li>
  <li><a href="#小组项目一检查点q-learning-智能体探索" id="toc-小组项目一检查点q-learning-智能体探索" class="nav-link" data-scroll-target="#小组项目一检查点q-learning-智能体探索">3. 小组项目一检查点：Q-Learning 智能体探索</a></li>
  <li><a href="#课后作业" id="toc-课后作业" class="nav-link" data-scroll-target="#课后作业">课后作业</a></li>
  <li><a href="#预习资料" id="toc-预习资料" class="nav-link" data-scroll-target="#预习资料">预习资料</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">第三周：Q-Learning 算法详解与实践</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="课程目标" class="level2">
<h2 class="anchored" data-anchor-id="课程目标">课程目标</h2>
<ul>
<li>理解 Q-Learning 算法的核心思想和原理</li>
<li>掌握 Q-Table 的概念和更新规则</li>
<li>学习使用 Q-Learning 算法解决迷宫寻宝 (Grid World) 问题</li>
<li>了解超参数 (学习率、折扣因子) 的作用，并进行简单调整</li>
<li>掌握 Q-Learning 算法的基本编程实现</li>
</ul>
</section>
<section id="第一次课q-learning-算法详解" class="level2">
<h2 class="anchored" data-anchor-id="第一次课q-learning-算法详解">第一次课：Q-Learning 算法详解</h2>
<section id="q-learning-算法核心思想" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-算法核心思想">1. Q-Learning 算法核心思想</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-Learning 是一种<strong>基于价值 (Value-based)</strong> 的<strong>离线 (Off-policy)</strong> 强化学习算法，用于学习<strong>最优 Q 函数</strong>，从而得到<strong>最优策略</strong>。</p>
</div>
</div>
<ul>
<li><strong>核心思想</strong>: 通过不断<strong>试错 (Trial-and-Error)</strong> 和<strong>更新 Q-Table</strong>，逐步逼近最优 Q 函数。</li>
<li><strong>离线 (Off-policy)</strong>: 学习的策略 (Q 函数对应的策略) 与实际执行的策略 (探索策略，例如 <span class="math inline">\(\epsilon\)</span>-greedy 策略) 可以不同。</li>
<li><strong>最优 Q 函数 (<span class="math inline">\(Q^*(s, a)\)</span>)</strong>: 表示在状态 <span class="math inline">\(s\)</span> 下，执行动作 <span class="math inline">\(a\)</span>，并<strong>之后都采取最优策略</strong>所能获得的<strong>最大期望累积奖励</strong>。</li>
<li><strong>最优策略 (<span class="math inline">\(\pi^*(s)\)</span>)</strong>: 在每个状态 <span class="math inline">\(s\)</span> 下，选择能使 Q 函数值 <span class="math inline">\(Q^*(s, a)\)</span> 最大的动作 <span class="math inline">\(a\)</span>。</li>
</ul>
</section>
<section id="q-table-q-值表" class="level3">
<h3 class="anchored" data-anchor-id="q-table-q-值表">2. Q-Table (Q 值表)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-Table 是 Q-Learning 算法中用于<strong>存储 Q 函数值</strong>的表格。表格的<strong>行</strong>表示<strong>状态 (State)</strong>，<strong>列</strong>表示<strong>动作 (Action)</strong>，<strong>单元格</strong>存储的是 <strong>Q 值</strong> <span class="math inline">\(Q(s, a)\)</span>。</p>
</div>
</div>
<ul>
<li><p><strong>表格结构</strong>:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 6%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>状态 (State)</th>
<th>动作 1 (Action 1)</th>
<th>动作 2 (Action 2)</th>
<th>…</th>
<th>动作 n (Action n)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>状态 1 (S1)</td>
<td><span class="math inline">\(Q(S1, A1)\)</span></td>
<td><span class="math inline">\(Q(S1, A2)\)</span></td>
<td>…</td>
<td><span class="math inline">\(Q(S1, An)\)</span></td>
</tr>
<tr class="even">
<td>状态 2 (S2)</td>
<td><span class="math inline">\(Q(S2, A1)\)</span></td>
<td><span class="math inline">\(Q(S2, A2)\)</span></td>
<td>…</td>
<td><span class="math inline">\(Q(S2, An)\)</span></td>
</tr>
<tr class="odd">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="even">
<td>状态 m (Sm)</td>
<td><span class="math inline">\(Q(Sm, A1)\)</span></td>
<td><span class="math inline">\(Q(Sm, A2)\)</span></td>
<td>…</td>
<td><span class="math inline">\(Q(Sm, An)\)</span></td>
</tr>
</tbody>
</table></li>
<li><p><strong>初始化</strong>: Q-Table 通常<strong>初始化为 0</strong> 或者<strong>小的随机值</strong>。</p></li>
<li><p><strong>更新</strong>: 在智能体与环境交互的过程中，<strong>不断更新 Q-Table 中的 Q 值</strong>，使其逐渐逼近真实的最优 Q 函数值。</p></li>
<li><p><strong>查询</strong>: 在决策时，根据当前状态 <span class="math inline">\(s\)</span>，<strong>查询 Q-Table 中对应行的 Q 值</strong>，选择 Q 值最大的动作 (或者根据探索策略选择动作)。</p></li>
</ul>
</section>
<section id="q-learning-更新规则-时序差分学习" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-更新规则-时序差分学习">3. Q-Learning 更新规则 (时序差分学习)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-Learning 使用<strong>时序差分 (Temporal Difference, TD)</strong> 学习方法来更新 Q 值。TD 学习是一种<strong>无模型 (Model-free)</strong> 的强化学习方法，<strong>无需事先知道环境的转移概率和奖励函数</strong>，通过<strong>采样</strong>和<strong>迭代</strong>的方式进行学习。</p>
</div>
</div>
<ul>
<li><p><strong>更新公式 (无需数学公式，侧重直观理解)</strong>:</p>
<pre><code>新的 Q(s, a)  &lt;-  旧的 Q(s, a)  +  学习率 * (TD 目标 - 旧的 Q(s, a))</code></pre>
<ul>
<li><strong>学习率 (<span class="math inline">\(\alpha\)</span>)</strong>: 控制每次更新的幅度，取值范围通常为 <span class="math inline">\([0, 1]\)</span>。
<ul>
<li><strong><span class="math inline">\(\alpha\)</span> 较大</strong>: 更新幅度大，学习速度快，但容易<strong>不稳定</strong>，可能<strong>震荡</strong>。</li>
<li><strong><span class="math inline">\(\alpha\)</span> 较小</strong>: 更新幅度小，学习速度慢，但<strong>稳定</strong>，收敛性好。</li>
</ul></li>
<li><strong>TD 目标 (TD Target)</strong>: 表示我们<strong>期望</strong>的 Q 值，是<strong>对未来累积奖励的估计</strong>。
<ul>
<li><strong>TD 目标 = 即时奖励 (Reward) + 折扣因子 (<span class="math inline">\(\gamma\)</span>) * 未来最优 Q 值 (下一状态的最大 Q 值)</strong></li>
<li><strong>未来最优 Q 值</strong>: 在下一个状态 <span class="math inline">\(s'\)</span> 下，所有可能动作 <span class="math inline">\(a'\)</span> 中，Q 值最大的那个值，即 <span class="math inline">\(\max_{a'} Q(s', a')\)</span>。</li>
</ul></li>
<li><strong>折扣因子 (<span class="math inline">\(\gamma\)</span>)</strong>: 控制未来奖励的重要性，取值范围通常为 <span class="math inline">\([0, 1]\)</span>。
<ul>
<li><strong><span class="math inline">\(\gamma\)</span> 接近 0</strong>: 更关注<strong>即时奖励</strong>，<strong>短视</strong>。</li>
<li><strong><span class="math inline">\(\gamma\)</span> 接近 1</strong>: 更关注<strong>未来奖励</strong>，<strong>有远见</strong>。</li>
</ul></li>
</ul></li>
<li><p><strong>更新过程</strong>:</p>
<ol type="1">
<li>智能体在状态 <span class="math inline">\(s\)</span> 下，根据策略 (例如 <span class="math inline">\(\epsilon\)</span>-greedy 策略) 选择动作 <span class="math inline">\(a\)</span>。</li>
<li>智能体执行动作 <span class="math inline">\(a\)</span>，环境转移到下一个状态 <span class="math inline">\(s'\)</span>，并返回奖励 <span class="math inline">\(r\)</span>。</li>
<li>根据 <strong>Q-Learning 更新规则</strong>，更新 Q-Table 中 <span class="math inline">\(Q(s, a)\)</span> 的值。</li>
<li>将当前状态更新为 <span class="math inline">\(s'\)</span>，重复步骤 1-3，直到 episode 结束。</li>
</ol></li>
<li><p><strong>Q-Learning 更新规则图示</strong>:</p></li>
</ul>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    S["状态 s"] --&gt; A["动作 a"]
    A --&gt; E["环境"]
    E --&gt; SP["新状态 s'"]
    E --&gt; R["奖励 r"]
    SP --&gt; QP["最大Q值"]
    R --&gt; TD["TD目标"]
    QP --&gt; TD
    TD --&gt; U["更新Q表"]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="q-learning-算法步骤流程" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-算法步骤流程">4. Q-Learning 算法步骤流程</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Q-Learning 算法的步骤流程可以总结为：<strong>初始化 Q-Table</strong>，<strong>循环迭代 episodes</strong>，<strong>在每个 episode 中，循环迭代 steps</strong>，<strong>选择动作</strong>，<strong>执行动作</strong>，<strong>更新 Q-Table</strong>。</p>
</div>
</div>
<ol type="1">
<li><strong>初始化 Q-Table</strong>: 创建一个 Q-Table，<strong>行数为状态空间大小</strong>，<strong>列数为动作空间大小</strong>，<strong>所有 Q 值初始化为 0</strong> (或其他小值)。</li>
<li><strong>循环迭代 Episodes</strong>: 进行<strong>多次 episodes</strong> 训练，让智能体不断与环境交互，学习优化策略。
<ul>
<li><strong>For each episode</strong>:
<ol type="a">
<li><strong>初始化环境</strong>: 重置环境到初始状态 <span class="math inline">\(s\)</span>。</li>
<li><strong>循环迭代 Steps</strong>: 在每个 episode 中，进行<strong>多步交互</strong>，直到 episode 结束 (例如到达目标状态或达到最大步数)。
<ul>
<li><strong>For each step</strong>:
<ol type="i">
<li><strong>选择动作</strong>: 根据当前状态 <span class="math inline">\(s\)</span>，使用<strong>探索策略</strong> (例如 <span class="math inline">\(\epsilon\)</span>-greedy 策略) 从 Q-Table 中选择一个动作 <span class="math inline">\(a\)</span>。</li>
<li><strong>执行动作</strong>: 智能体在环境中执行动作 <span class="math inline">\(a\)</span>，环境返回<strong>下一个状态 <span class="math inline">\(s'\)</span> 和奖励 <span class="math inline">\(r\)</span></strong>。</li>
<li><strong>更新 Q-Table</strong>: 使用 <strong>Q-Learning 更新规则</strong>，根据 <span class="math inline">\((s, a, r, s')\)</span> 更新 Q-Table 中 <span class="math inline">\(Q(s, a)\)</span> 的值。</li>
<li><strong>更新状态</strong>: 将当前状态更新为 <span class="math inline">\(s'，s \leftarrow s'\)</span>。</li>
<li><strong>判断 Episode 结束</strong>: 判断是否到达终止状态 (terminated) 或截断状态 (truncated)，如果 episode 结束，则跳出 step 循环。</li>
</ol></li>
</ul></li>
</ol></li>
</ul></li>
<li><strong>训练结束</strong>: 当 Q-Table 收敛 (Q 值变化很小) 或者达到预设的训练 episodes 数量时，训练结束。</li>
<li><strong>策略提取</strong>: 训练结束后，可以从 Q-Table 中提取最优策略。对于每个状态 <span class="math inline">\(s\)</span>，最优策略 <span class="math inline">\(\pi^*(s)\)</span> 是选择能使 Q 值 <span class="math inline">\(Q^*(s, a)\)</span> 最大的动作 <span class="math inline">\(a\)</span>。</li>
</ol>
</section>
<section id="动态定价案例-结合商业案例演示-q-learning-应用" class="level3">
<h3 class="anchored" data-anchor-id="动态定价案例-结合商业案例演示-q-learning-应用">5. 动态定价案例 (结合商业案例，演示 Q-Learning 应用)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>动态定价是一种<strong>根据市场供需变化</strong>，<strong>实时调整商品或服务价格</strong>的定价策略。强化学习可以用于<strong>学习最优的动态定价策略</strong>，以最大化收益或利润。</p>
</div>
</div>
<section id="动态定价场景简化" class="level4">
<h4 class="anchored" data-anchor-id="动态定价场景简化">5.1 动态定价场景简化</h4>
<ul>
<li><strong>场景</strong>: 在线零售平台，销售<strong>单一商品</strong> (例如：某品牌的热门手机)。</li>
<li><strong>状态 (State)</strong>: <strong>商品库存水平</strong> (例如：高库存、中库存、低库存)。</li>
<li><strong>动作 (Action)</strong>: <strong>价格调整</strong> (例如：涨价、降价、维持原价)。</li>
<li><strong>奖励 (Reward)</strong>: <strong>销售利润</strong> (例如：销售额 - 成本)。</li>
<li><strong>目标</strong>: <strong>最大化</strong>一段时间内的<strong>累积利润</strong>。</li>
</ul>
</section>
<section id="q-learning-动态定价步骤-简化版" class="level4">
<h4 class="anchored" data-anchor-id="q-learning-动态定价步骤-简化版">5.2 Q-Learning 动态定价步骤 (简化版)</h4>
<ol type="1">
<li><p><strong>状态空间</strong>: 假设商品库存水平分为 3 个状态：<code>高库存 (High)</code>，<code>中库存 (Medium)</code>，<code>低库存 (Low)</code>。</p></li>
<li><p><strong>动作空间</strong>: 假设价格调整分为 3 个动作：<code>涨价 (Increase)</code>，<code>降价 (Decrease)</code>，<code>维持原价 (Maintain)</code>。</p></li>
<li><p><strong>Q-Table</strong>: 创建一个 <span class="math inline">\(3 \times 3\)</span> 的 Q-Table，初始化为 0。</p></li>
<li><p><strong>奖励函数</strong>: 假设奖励函数如下 (简化示例)：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>状态 (库存)</th>
<th>动作 (价格调整)</th>
<th>奖励 (利润)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>高库存</td>
<td>降价</td>
<td>+5 (销量增加)</td>
</tr>
<tr class="even">
<td>高库存</td>
<td>维持原价</td>
<td>+2 (正常销量)</td>
</tr>
<tr class="odd">
<td>高库存</td>
<td>涨价</td>
<td>-1 (销量减少)</td>
</tr>
<tr class="even">
<td>中库存</td>
<td>降价</td>
<td>+3 (销量略增)</td>
</tr>
<tr class="odd">
<td>中库存</td>
<td>维持原价</td>
<td>+4 (正常销量)</td>
</tr>
<tr class="even">
<td>中库存</td>
<td>涨价</td>
<td>+1 (销量略减)</td>
</tr>
<tr class="odd">
<td>低库存</td>
<td>降价</td>
<td>-2 (缺货风险)</td>
</tr>
<tr class="even">
<td>低库存</td>
<td>维持原价</td>
<td>+6 (高利润率)</td>
</tr>
<tr class="odd">
<td>低库存</td>
<td>涨价</td>
<td>+8 (更高利润率)</td>
</tr>
</tbody>
</table>
<p><strong>注意</strong>: 这只是一个简化的奖励函数示例，实际应用中奖励函数会更复杂，需要根据具体业务场景进行设计。</p></li>
<li><p><strong>Q-Learning 训练</strong>: 进行 episodes 训练，使用 <span class="math inline">\(\epsilon\)</span>-greedy 策略选择动作，并根据奖励函数和 Q-Learning 更新规则更新 Q-Table。</p></li>
<li><p><strong>最优策略</strong>: 训练结束后，Q-Table 会学习到最优的动态定价策略。例如，可能学习到：</p>
<ul>
<li><strong>高库存</strong>: 应该 <strong>降价</strong> 以快速清理库存。</li>
<li><strong>中库存</strong>: 应该 <strong>维持原价</strong> 以获得稳定利润。</li>
<li><strong>低库存</strong>: 可以 <strong>涨价</strong> 以提高利润率 (但需注意缺货风险)。</li>
</ul></li>
</ol>
</section>
<section id="ai-辅助编程演示-使用-python-和-ai-工具演示动态定价代码" class="level4">
<h4 class="anchored" data-anchor-id="ai-辅助编程演示-使用-python-和-ai-工具演示动态定价代码">5.3 AI 辅助编程演示 (使用 Python 和 AI 工具，演示动态定价代码)</h4>
<ul>
<li><p><strong>可以使用 Python 模拟动态定价环境</strong> (状态、动作、奖励函数等)。</p></li>
<li><p><strong>使用 AI 辅助编程工具 (例如 GitHub Copilot)</strong>，辅助编写 Q-Learning 算法代码，应用于动态定价问题。</p></li>
<li><p><strong>演示代码</strong> (伪代码示例，仅供参考)：</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 初始化 Q-Table (字典或 NumPy 数组)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>q_table <span class="op">=</span> {} <span class="co">#  状态为 (库存状态)，动作为 (价格调整动作)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义状态空间和动作空间 (例如使用枚举类型)</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>states <span class="op">=</span> [<span class="st">"High"</span>, <span class="st">"Medium"</span>, <span class="st">"Low"</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>actions <span class="op">=</span> [<span class="st">"Increase"</span>, <span class="st">"Maintain"</span>, <span class="st">"Decrease"</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义奖励函数 (根据状态和动作返回奖励值)</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_reward(state, action):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (根据奖励函数表格返回奖励值)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">pass</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Q-Learning 算法训练循环</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>episodes <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>discount_factor <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span> <span class="co">#  epsilon-greedy 策略的探索率</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> episode <span class="kw">in</span> <span class="bu">range</span>(episodes):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 初始化状态 (随机选择初始库存状态)</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    current_state <span class="op">=</span> random.choice(states)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(steps_per_episode): <span class="co">#  每个 episode 的步数</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 使用 epsilon-greedy 策略选择动作</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">&lt;</span> epsilon:</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> random.choice(actions) <span class="co">#  探索：随机选择动作</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># 利用：选择 Q 值最大的动作</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: q_table.get((current_state, a), <span class="dv">0</span>))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 执行动作，获取下一个状态和奖励 (模拟环境交互)</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        next_state <span class="op">=</span> ... <span class="co">#  根据当前状态和动作，模拟状态转移</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> get_reward(current_state, action)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Q-Learning 更新规则</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        old_q_value <span class="op">=</span> q_table.get((current_state, action), <span class="dv">0</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        next_max_q <span class="op">=</span> <span class="bu">max</span>([q_table.get((next_state, a), <span class="dv">0</span>) <span class="cf">for</span> a <span class="kw">in</span> actions]) <span class="co">#  下一个状态的最大 Q 值</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        new_q_value <span class="op">=</span> old_q_value <span class="op">+</span> learning_rate <span class="op">*</span> (reward <span class="op">+</span> discount_factor <span class="op">*</span> next_max_q <span class="op">-</span> old_q_value)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        q_table[(current_state, action)] <span class="op">=</span> new_q_value <span class="co">#  更新 Q-Table</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 更新状态</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        current_state <span class="op">=</span> next_state</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="co"># 训练结束，输出学习到的 Q-Table 和最优策略</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Q-Table:"</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(q_table)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="co">#  提取最优策略 (对于每个状态，选择 Q 值最大的动作)</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>optimal_policy <span class="op">=</span> {}</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> state <span class="kw">in</span> states:</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    optimal_action <span class="op">=</span> <span class="bu">max</span>(actions, key<span class="op">=</span><span class="kw">lambda</span> a: q_table.get((state, a), <span class="dv">0</span>))</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    optimal_policy[state] <span class="op">=</span> optimal_action</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Optimal Policy:"</span>)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(optimal_policy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>注意</strong>: 这只是一个非常简化的动态定价示例，用于演示 Q-Learning 的基本应用思路。实际商业场景中的动态定价问题会更加复杂，需要考虑更多因素 (例如：竞争对手价格、季节性因素、促销活动等)，并使用更复杂的强化学习算法。</p></li>
</ul>
</section>
</section>
</section>
<section id="第二次课小组项目一q-learning-算法编程实践" class="level2">
<h2 class="anchored" data-anchor-id="第二次课小组项目一q-learning-算法编程实践">第二次课：小组项目一：Q-Learning 算法编程实践</h2>
<section id="小组项目一q-learning-算法编程实践-迷宫寻宝-grid-world" class="level3">
<h3 class="anchored" data-anchor-id="小组项目一q-learning-算法编程实践-迷宫寻宝-grid-world">1. 小组项目一：Q-Learning 算法编程实践 (迷宫寻宝 Grid World)</h3>
<ul>
<li><strong>项目目标</strong>:
<ul>
<li>以小组为单位，<strong>使用 Python 和 AI 工具</strong>，<strong>编写 Q-Learning 算法代码</strong>，应用于<strong>迷宫寻宝 (Grid World) 项目</strong>。</li>
<li><strong>实现 Q-Learning 智能体</strong>，使其能够在迷宫环境中<strong>自主探索</strong>，并<strong>学习找到宝藏的最优路径</strong>。</li>
<li><strong>可视化智能体在迷宫中的探索过程</strong> (例如：绘制智能体路径、Q-Table 热力图等，<strong>可选</strong>)。</li>
</ul></li>
<li><strong>代码框架</strong>:
<ul>
<li>可以使用<strong>第一次课提供的 Grid World 环境代码</strong> (或者小组自行搭建的 Grid World 环境)。</li>
<li>小组需要<strong>自行编写 Q-Learning 算法代码</strong>，并<strong>与 Grid World 环境进行集成</strong>。</li>
</ul></li>
<li><strong>AI 辅助工具</strong>:
<ul>
<li><strong>鼓励学生充分利用 GitHub Copilot, Tabnine 等 AI 辅助编程工具</strong>，提高开发效率。</li>
<li><strong>但强调</strong>: AI 工具是辅助手段，学生需要理解 Q-Learning 算法原理和代码逻辑，<strong>不能完全依赖</strong> AI 工具生成代码，而忽略算法理解和调试。</li>
</ul></li>
<li><strong>项目提交</strong>:
<ul>
<li>小组<strong>提交</strong>完整的 Q-Learning 算法代码 (Python 文件)，以及<strong>修改后的 Grid World 环境代码</strong> (如果环境代码有修改)。</li>
<li><strong>无需提交</strong>项目报告。</li>
</ul></li>
</ul>
</section>
<section id="超参数讲解与调整-学习率折扣因子" class="level3">
<h3 class="anchored" data-anchor-id="超参数讲解与调整-学习率折扣因子">2. 超参数讲解与调整 (学习率、折扣因子)</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>超参数 (Hyperparameters) 是强化学习算法中需要<strong>手动设置的参数</strong>，例如学习率 (<span class="math inline">\(\alpha\)</span>)、折扣因子 (<span class="math inline">\(\gamma\)</span>)、探索率 (<span class="math inline">\(\epsilon\)</span>) 等。<strong>超参数的选择会直接影响算法的性能和收敛速度</strong>。</p>
</div>
</div>
<ul>
<li><strong>常用超参数</strong>:
<ul>
<li><strong>学习率 (<span class="math inline">\(\alpha\)</span>)</strong>: 已在第一次课中讲解。</li>
<li><strong>折扣因子 (<span class="math inline">\(\gamma\)</span>)</strong>: 已在第一次课中讲解。</li>
<li><strong>探索率 (<span class="math inline">\(\epsilon\)</span>)</strong>: <span class="math inline">\(\epsilon\)</span>-greedy 策略中的探索概率，已在第二次课中讲解。</li>
</ul></li>
<li><strong>超参数调整</strong>:
<ul>
<li><strong>经验调整</strong>: 根据经验和直觉进行调整 (trial-and-error)。</li>
<li><strong>网格搜索 (Grid Search)</strong>: 在超参数空间中，<strong>预先定义一组候选值</strong>，<strong>遍历所有可能的组合</strong>，<strong>训练模型并评估性能</strong>，选择性能最佳的超参数组合。</li>
<li><strong>随机搜索 (Random Search)</strong>: 在超参数空间中，<strong>随机采样</strong>一定数量的超参数组合，<strong>训练模型并评估性能</strong>，选择性能最佳的超参数组合。</li>
<li><strong>自动化超参数优化方法</strong>: 例如 Bayesian Optimization, Hyperband 等 (更高级的方法，本课程不深入讲解)。</li>
</ul></li>
<li><strong>超参数调整建议 (针对 Q-Learning 和 Grid World)</strong>:
<ul>
<li><strong>学习率 (<span class="math inline">\(\alpha\)</span>)</strong>: 可以尝试 <code>0.1, 0.3, 0.5, 0.7</code> 等值。</li>
<li><strong>折扣因子 (<span class="math inline">\(\gamma\)</span>)</strong>: 可以尝试 <code>0.8, 0.9, 0.95, 0.99</code> 等值。</li>
<li><strong>探索率 (<span class="math inline">\(\epsilon\)</span>)</strong>: 初始值可以设置为 <code>1.0</code> (完全探索)，然后<strong>逐渐衰减</strong>到较小的值 (例如 <code>0.1</code> 或 <code>0.01</code>)。衰减方式可以是<strong>线性衰减</strong>、<strong>指数衰减</strong>等。</li>
</ul></li>
<li><strong>超参数调整实践</strong>:
<ul>
<li><strong>在 Q-Learning 代码中，将超参数设置为可调节的变量</strong>。</li>
<li><strong>尝试不同的超参数组合</strong>，观察智能体在 Grid World 环境中的表现 (例如：是否能更快找到宝藏，是否能避免陷阱，平均 episode 奖励等)。</li>
<li><strong>记录实验结果</strong>，分析超参数对算法性能的影响。</li>
</ul></li>
</ul>
</section>
<section id="小组项目一检查点q-learning-智能体探索" class="level3">
<h3 class="anchored" data-anchor-id="小组项目一检查点q-learning-智能体探索">3. 小组项目一检查点：Q-Learning 智能体探索</h3>
<ul>
<li><strong>检查点目标</strong>: <strong>确保学生小组能够运行基本的 Q-Learning 智能体</strong>，<strong>在迷宫环境 (Grid World) 中进行探索</strong>。</li>
<li><strong>检查内容</strong>:
<ul>
<li><strong>Q-Learning 算法代码是否能够正常运行</strong>，<strong>没有明显的 bug</strong>。</li>
<li><strong>智能体是否能够在迷宫环境中移动</strong>，<strong>并与环境进行交互</strong>。</li>
<li><strong>Q-Table 是否能够正常更新</strong> (可以通过打印 Q-Table 或 Q 值变化来观察)。</li>
<li><strong>智能体是否能够进行初步的探索</strong> (例如：在迷宫中随机移动，尝试不同的路径)。</li>
<li><strong>不需要智能体达到最优性能</strong>，<strong>重点是代码能够跑起来，并且智能体能够进行基本的探索</strong>。</li>
</ul></li>
<li><strong>检查方式</strong>:
<ul>
<li><strong>小组演示</strong>: 每个小组<strong>简单演示</strong> Q-Learning 智能体在 Grid World 环境中的运行情况。</li>
<li><strong>代码 review (可选)</strong>: 教师可以<strong>抽查部分小组的代码</strong>，进行简单的代码 review，<strong>指出潜在问题和改进方向</strong>。</li>
<li><strong>答疑</strong>: 解答学生在 Q-Learning 算法编程和环境集成过程中遇到的问题。</li>
</ul></li>
</ul>
</section>
<section id="课后作业" class="level3">
<h3 class="anchored" data-anchor-id="课后作业">课后作业</h3>
<ol type="1">
<li><strong>完成小组项目一：Q-Learning 算法编程实践</strong>，实现 Q-Learning 智能体在迷宫环境中寻宝。</li>
<li><strong>调整 Q-Learning 算法的超参数</strong> (学习率、折扣因子、探索率等)，观察超参数对智能体性能的影响，并尝试找到一组较好的超参数组合。</li>
<li><strong>思考题</strong>:
<ul>
<li>Q-Learning 算法有什么优点和缺点？</li>
<li>Q-Learning 算法适用于什么类型的问题？有什么局限性？</li>
<li>如何改进 Q-Learning 算法，以提高其性能和适用范围？ (例如：Double Q-Learning, Prioritized Experience Replay 等，可以查阅资料了解)</li>
</ul></li>
</ol>
</section>
<section id="预习资料" class="level3">
<h3 class="anchored" data-anchor-id="预习资料">预习资料</h3>
<ol type="1">
<li><strong>阅读材料</strong>:
<ul>
<li>Q-Learning 算法的改进版本：Double Q-Learning, Dueling Q-Network 等 (初步了解思想)。</li>
<li>ε-greedy 退火策略、UCB 算法等更高级的探索策略。</li>
<li>动态定价的更多商业应用案例。</li>
</ul></li>
<li><strong>视频资源</strong>:
<ul>
<li>Q-Learning 算法代码实现详解 (Python)。</li>
<li>超参数调整技巧和经验分享。</li>
<li>强化学习算法的探索策略进阶。</li>
</ul></li>
<li><strong>下周预习重点</strong>:
<ul>
<li>Q-Learning 算法的优化和改进方向。</li>
<li>探索策略的进一步探讨 (例如 ε-greedy 退火策略)。</li>
<li>Q-Table 初始化、奖励函数设计等实用技巧。</li>
<li>小组项目一提交和优秀项目讲解准备。</li>
</ul></li>
</ol>
<hr>
<p><strong>请注意</strong>:</p>
<ul>
<li><strong>代码框架</strong>: 可以使用第二次课提供的 Grid World 环境代码，或者小组自行搭建的环境。重点是<strong>自行编写 Q-Learning 算法代码</strong>，并与环境集成。</li>
<li><strong>AI 辅助工具</strong>: 鼓励使用 AI 工具，但务必强调理解算法原理和代码逻辑的重要性，避免过度依赖 AI 工具。</li>
<li><strong>小组项目</strong>: 小组项目一旨在让学生<strong>实践 Q-Learning 算法</strong>，<strong>解决迷宫寻宝问题</strong>，并<strong>初步了解超参数调整</strong>。为后续更复杂的强化学习算法和项目打下基础。</li>
<li><strong>检查点</strong>: 本次课设有小组项目一检查点，旨在<strong>及时发现学生在编程实践中遇到的问题</strong>，并提供<strong>指导和帮助</strong>，确保所有小组都能顺利进行后续的学习和项目。</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./week2.html" class="pagination-link" aria-label="第二周：强化学习框架与迷宫环境">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">第二周：强化学习框架与迷宫环境</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./week4.html" class="pagination-link" aria-label="第四周：Q-Learning 算法优化与改进">
        <span class="nav-page-text"><span class="chapter-title">第四周：Q-Learning 算法优化与改进</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>