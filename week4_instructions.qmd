---
title: "Week 4 - 教师指导手册"
subtitle: "蒙特卡洛方法 - 从完整经验中学习"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解模型已知 (Model-Based) vs. 模型未知 (Model-Free) RL 的区别，并认识到无模型方法的重要性。
    *   学生掌握蒙特卡洛 (MC) 预测的核心思想：通过平均完整回合的回报来估计价值函数。
    *   学生理解首次访问 (First-Visit) MC 和每次访问 (Every-Visit) MC 的区别。
    *   学生能够理解 MC 评估 Vπ 和 Qπ 的算法流程。
    *   学生能够运行 Lab 2 代码（Blackjack 环境），使用 MC 方法评估一个简单策略的价值函数。
    *   学生能够理解 MC 方法的主要优点（无模型、无偏）和缺点（高方差、需完整回合、仅限回合制）。
*   **次要目标:**
    *   培养学生动手实现和运行基本 RL 算法的能力。
    *   训练学生通过可视化价值函数来理解策略评估结果。
    *   为下周学习 TD 方法做铺垫，理解 MC 的局限性。

# 重点概念 (Key Concepts)

*   无模型 (Model-Free) RL vs. 模型已知 (Model-Based) RL
*   蒙特卡洛 (Monte Carlo, MC) 预测
*   完整回合 (Episode)
*   回报 (Return) G_t (作为 MC 的目标)
*   首次访问 (First-Visit) MC
*   每次访问 (Every-Visit) MC
*   MC 评估 Vπ / Qπ 算法
*   MC 的优缺点 (无偏、高方差、需完整回合)
*   Blackjack 环境 (作为 Lab 示例)

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 7 (约 90 分钟):**
    *   回顾 Bellman 最优方程与最优性 (5 分钟)。
    *   引出无模型问题 (15-20 分钟): 强调现实中 P, R 未知的普遍性，引出无模型学习的必要性。
    *   MC 预测核心思想 (25-30 分钟): 类比用多次投掷硬币估计正反面概率（大数定律）。讲解通过平均样本回报 G_t 估计期望回报 Vπ 的思路。
    *   首次访问 vs. 每次访问 MC (15 分钟): 解释两者的区别和联系。
    *   MC 评估 Vπ 算法伪代码讲解 (10-15 分钟): 逐步解释算法流程，特别是从后往前计算 G_t 和更新 V(s) 的过程。
    *   Q&A (5 分钟)
*   **Session 8 (约 90 分钟): Lab 2**
    *   回顾 MC 预测思想 (5 分钟)。
    *   MC 评估 Qπ 算法讲解 (10 分钟): 与 Vπ 评估对比，强调需要记录 (s, a) 对的回报。提及探索性开端问题（但不必深入）。
    *   Lab 2 目标与环境介绍 (15 分钟): 介绍 Blackjack 环境的状态、动作、奖励，明确 Lab 任务是评估一个固定策略。
    *   指导运行 Lab 2 代码 (35-40 分钟): **重点环节**。带领学生理解代码结构（策略定义、主循环、回报计算、价值更新、可视化）。确保学生能成功运行并看到价值函数图像。鼓励学生尝试修改策略或回合数。
    *   讨论 MC 方法的优缺点 (10 分钟): 结合 Lab 运行经验（可能需要很多回合才能得到平滑的价值函数）和理论讲解 MC 的局限性。
    *   Lab 提交要求说明 & Q&A (5 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 7

*   **无模型引入:**
    *   **提问:** “我们上周讲的 Bellman 方程求解（如值迭代）需要知道 P 和 R，但在我们之前讨论的网约车定价、商品推荐问题里，P 和 R 容易知道吗？” 引导学生认识到模型未知的普遍性。
*   **MC 核心思想:**
    *   **简单类比:** “想知道一个不均匀骰子掷出各点数的概率，怎么办？掷很多次，统计频率。” MC 类似，想知道状态 s 的价值（期望回报），就在策略 π 下多次经历状态 s，计算每次经历后的实际回报 G_t，然后求平均。
    *   **强调完整回合:** 清晰说明 MC 需要等到一个回合完全结束后，才能计算出 G_t。
*   **首次/每次访问:**
    *   用一个简单的 Gridworld 回合轨迹举例说明两者的区别。例如，一个回合路径是 S1 -> S2 -> S1 -> S3(结束)。首次访问只用第一次到 S1 时的回报，每次访问用两次到 S1 的回报。
*   **Vπ 伪代码:**
    *   **逐步解释:** 重点解释为何要从后往前计算 G_t (因为 G_t 定义的是从 t 时刻**之后**的回报)，以及如何累积和平均回报来更新 V(s)。
    *   **增量更新 (可选):** 可以简单提及 V(s) ← V(s) + α * (G - V(s)) 或 V(s) ← V(s) + (1/N(s)) * (G - V(s)) 这种增量更新方式，避免存储所有回报，更节省内存。

## Session 8 (Lab 2)

*   **Qπ 评估:** 强调与 Vπ 评估类似，但统计单元是 (s, a) 对。
*   **Lab 代码指导:**
    *   **分步讲解:** 策略定义部分 (`simple_policy`) -> 主循环生成回合 (`episode.append`) -> 回合结束后处理数据（从后往前算 G，更新 V）-> 可视化。
    *   **可视化解读:** 引导学生理解 3D 价值函数图的含义：X 轴是庄家明牌，Y 轴是玩家点数，Z 轴是该状态的预期回报（价值）。观察价值是否随玩家点数增加而增加，随庄家明牌增大而减小（大致趋势）。区分有无可用 Ace 的情况。
    *   **动手尝试:** 鼓励学生修改 `simple_policy`（例如改成更保守的策略，如点数 < 15 才 hit），重新运行，观察价值函数的变化。这有助于加深对策略评估的理解。
*   **MC 优缺点讨论:**
    *   **结合 Lab:** “大家运行 Lab 时感觉需要多少回合才能得到比较稳定的价值图像？是不是很多？” 引出高方差、收敛慢的问题。
    *   **提问:** “如果 CartPole 任务永远不结束（持续性任务），MC 方法还能用吗？” 引出 MC 依赖完整回合的局限性。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 为什么 MC 评估要从回合末尾往前计算回报 G_t？**
    *   A: 因为回报 G_t 的定义是**从时间步 t 开始的未来**折扣奖励总和 (G_t = R_{t+1} + γR_{t+2} + ...)。从回合末尾 T-1 开始计算最方便：G_{T-1} = R_T。然后 G_{T-2} = R_{T-1} + γG_{T-1}，依此类推，G_t = R_{t+1} + γG_{t+1}。这样可以避免重复计算。
*   **Q: 首次访问和每次访问 MC 哪个更好？**
    *   A: 理论上两者都能收敛。首次访问在理论分析中更常用。每次访问利用了更多数据，有时可能收敛稍快，但引入了一些额外的复杂性（例如，一个回合内多次访问的回报是否独立？）。在实践中，差异通常不大，选择哪个都可以。
*   **Q: Lab 代码里的 `defaultdict(float)` 和 `defaultdict(list)` 是什么意思？**
    *   A: `defaultdict` 是 Python `collections` 模块里的一个数据结构。它类似于普通字典，但当你访问一个不存在的 key 时，它不会报错，而是会自动用一个预设的“默认工厂”函数来创建这个 key 的默认值。`defaultdict(float)` 表示默认值是 0.0，`defaultdict(list)` 表示默认值是空列表 `[]`。这在 RL 中很方便，因为我们不需要在使用 V[state] 或 Returns[state] 之前检查 state 是否已经存在。
*   **Q: MC 方法看起来很简单，但缺点也很明显（高方差、需完整回合），它还有用吗？**
    *   A: MC 方法是理解 RL 的重要基础，并且在某些特定场景下仍然有用，例如：1) 回合制任务且回合长度适中；2) 需要无偏估计的场景；3) 作为更复杂算法（如某些策略梯度方法、AlphaGo 的早期版本）的一部分。更重要的是，理解 MC 的思想和局限性，有助于我们理解下一周要学的 TD 方法为何被提出以及它的优势所在。

# 与后续课程的联系 (Connections to Future Topics)

*   MC 方法是理解**策略梯度**算法（如 REINFORCE）的基础，因为 REINFORCE 使用 MC 方法来估计回报 G_t。
*   MC 的高方差和依赖完整回合的缺点，直接引出了下一周要学习的**时序差分 (TD) 学习**，TD 通过自举 (Bootstrapping) 来克服这些缺点。理解 MC 的不足有助于更好地理解 TD 的动机和优势。

# 教师准备建议 (Preparation Suggestions)

*   准备好清晰解释无模型学习必要性的例子。
*   准备好 MC 核心思想的类比。
*   准备好 Vπ 和 Qπ 评估的伪代码讲解。
*   确保 Lab 2 的 Blackjack 代码能够顺利运行，并准备好解读可视化结果。
*   预先思考几个可以引导学生讨论的关于 MC 优缺点的问题。