---
title: "Week 14 - 教师指导手册"
subtitle: "商业案例分析 2 - 个性化推荐与营销"
format:
  html:
    toc: true
    toc-location: left
---

# 教学目标 (Learning Objectives)

*   **主要目标:**
    *   学生理解为何以及如何在个性化推荐/营销场景中使用强化学习（对比传统方法）。
    *   学生能够将推荐/营销问题形式化为 MDP，深入讨论状态表示（用户/情境特征、Embedding）、动作空间（候选生成+排序、动作嵌入）和奖励设计（显式/隐式反馈、长期价值、多目标）的挑战。
    *   学生理解探索与利用在推荐系统中的重要性及其挑战。
    *   学生了解适用于推荐场景的 RL 算法类别（DQN, AC, Bandit, Offline RL）。
    *   学生能够识别并批判性地思考 RL 推荐系统可能引发的伦理问题（过滤气泡、公平性、隐私、操纵）。
*   **次要目标:**
    *   进一步加强学生将 RL 应用于复杂商业问题的建模能力。
    *   培养学生对 RL 应用伦理风险的敏感性和批判性思维。
    *   将 RL 与推荐系统这一重要商业应用领域联系起来。

# 重点概念 (Key Concepts)

*   RL 在推荐/营销中的应用动机 (序贯性、交互性、长期目标)
*   推荐系统的 MDP 定义:
    *   状态 S (用户特征、行为历史、情境、Embedding)
    *   动作 A (候选生成+排序、动作嵌入)
    *   奖励 R (点击、转化、LTV、多样性、稀疏性、延迟性)
*   探索 vs. 利用在推荐中的体现 (新颖性、多样性、长尾)
*   适用于推荐的 RL 算法类别 (DQN, AC, Bandit, Offline RL)
*   推荐系统的伦理问题:
    *   过滤气泡 / 信息茧房
    *   公平性 (用户、物品/内容提供者)
    *   数据隐私
    *   操纵与成瘾

# 时间分配建议 (Suggested Time Allocation - 2 Sessions)

*   **Session 27 (约 90 分钟): 推荐系统 MDP 定义与分析**
    *   回顾上周动态定价案例要点 (5 分钟)。
    *   引入个性化推荐/营销场景 (10 分钟): 介绍电商、内容平台等应用，引出传统方法的局限性和 RL 的动机。
    *   **深度案例分析：电商商品推荐 MDP 定义 (55-60 分钟):** **核心环节**。
        *   **引导讨论 S:** 讨论用户状态的复杂性，如何表示高维、动态、序列化的信息（Embedding, RNN 等概念介绍，不要求深入）。
        *   **引导讨论 A:** 重点讨论动作空间巨大的问题及处理方法（候选生成+排序，动作嵌入）。
        *   **引导讨论 R:** **重点深入讨论**。区分显式/隐式反馈，讨论点击、转化、LTV 等不同奖励的优劣、稀疏性、延迟性问题，以及多目标平衡的挑战。
    *   探索 vs. 利用在推荐中的讨论 (10 分钟): 结合新颖性、多样性、长尾推荐等概念讨论探索的重要性。
    *   Q&A (5 分钟)
*   **Session 28 (约 90 分钟): 算法选择与伦理思辨**
    *   回顾推荐 MDP 定义要点 (5 分钟)。
    *   可选算法讨论 (15 分钟): 简要介绍 DQN, AC, Bandit, Offline RL 在推荐场景下的适用性。
    *   **伦理思辨 (55-60 分钟):** **核心环节**。
        *   **分组或全班讨论:** 针对过滤气泡、公平性、隐私、操纵这四个方面进行深入讨论。
        *   **引导思考:** RL 如何可能加剧或缓解这些问题？奖励函数设计与伦理风险的关系？有哪些技术或非技术手段可以应对？
        *   **强调责任:** 引导学生认识到技术从业者的伦理责任。
    *   总结与 Q&A (10 分钟)。

# 教学活动与建议 (Teaching Activities & Suggestions)

## Session 27

*   **RL 动机:**
    *   **对比:** “传统协同过滤告诉你用户 A 和用户 B 相似，但没告诉你现在给 A 推荐 B 刚买的东西是不是最佳时机，也没考虑推荐后 A 的反应对后续推荐的影响。RL 考虑了这种动态交互和长期效果。”
*   **MDP 定义讨论 (核心):**
    *   **状态 S:**
        *   **Embedding:** 可以简单解释为“给每个用户/商品一个浓缩的特征向量，相似的用户/商品向量距离近”。
        *   **序列:** 强调用户行为是序列，需要捕捉时序信息（简单提及 RNN/LSTM）。
    *   **动作 A:**
        *   **候选+排序:** 这是工业界常用范式，将 RL 用于排序阶段。
        *   **动作嵌入:** 解释为将动作空间映射到连续空间，使得 AC 方法可用。
    *   **奖励 R (深入讨论):**
        *   **Clickbait 例子:** “如果只奖励点击，系统会不会倾向于推荐标题党或封面吸引人但内容质量差的东西？”
        *   **LTV 挑战:** “LTV 怎么衡量？需要多长时间？如何归因到某次推荐上？” 引导学生认识到长期奖励的困难。
        *   **多样性:** “总是推荐同类商品用户会不会腻？如何奖励多样性？”
*   **探索:**
    *   **商业价值:** 强调探索不仅是为了模型学习，本身也有商业价值（发现用户新兴趣、推广新品）。

## Session 28

*   **算法选择:** 简要说明，不必深入算法细节。重点是根据问题特点（动作空间、是否需要 Off-Policy）匹配算法类别。
*   **伦理思辨 (核心):**
    *   **分组任务:** 可以让不同小组负责深入讨论一个伦理问题，然后分享观点。
    *   **引导性问题:**
        *   过滤气泡：“你是否感觉自己手机里的新闻越来越单一？这有什么坏处？” “推荐系统该不该主动推荐一些你不一定喜欢但可能有价值的信息？”
        *   公平性：“为什么某些群体在平台上获得的机会更少？算法是加剧了还是可以缓解这种不公？” “平台应该优先考虑用户满意度还是内容提供者的公平曝光？”
        *   隐私：“你愿意用多少个人信息来换取更精准的推荐？” “平台收集的数据边界在哪里？”
        *   操纵：“短视频让你停不下来，是算法太懂你，还是在利用你的弱点？” “技术中立吗？”
    *   **开放讨论:** 鼓励学生从用户、平台、内容提供者、社会等多个角度思考，没有标准答案，重在引发思考和讨论。
    *   **教师角色:** 保持中立，引导讨论，提供不同观点，总结关键权衡。

# 潜在学生问题与解答 (Potential Student Questions & Answers)

*   **Q: 推荐系统的状态和动作空间都这么大，RL 真的能处理吗？**
    *   A: 这正是深度强化学习（DRL）发挥作用的地方。通过深度神经网络进行函数逼近（近似 V 或 Q 或策略 π），以及使用 Embedding 等技术处理高维稀疏输入，DRL 使得处理巨大状态空间成为可能。对于巨大的动作空间，通常采用候选生成+排序或动作嵌入等方法来规避直接在全量动作空间上操作的困难。
*   **Q: 奖励 R 这么难定，工业界是怎么做的？**
    *   A: 工业界通常采用复合奖励，最常见的是以**点击 (CTR)** 作为主要的基础奖励信号（因为它密集且易于获取），然后结合**转化 (CVR)**、**时长 (Dwell Time)**、**LTV 估计**等更强的信号进行加权或作为辅助目标。同时，也会加入一些关于**多样性、新颖性、公平性**的启发式规则或约束来缓解单一指标优化带来的问题。奖励函数的设计是一个持续迭代和实验的过程。
*   **Q: Offline RL 听起来很好，可以直接用历史数据训练吗？有什么风险？**
    *   A: Offline RL 的主要风险是**分布偏移 (Distribution Shift)**。因为训练数据是由过去的某个策略（日志策略）生成的，而我们想学习的新策略可能与它差异很大。如果直接套用像 Q-Learning 这样的 Off-Policy 算法，可能会严重高估某些未在数据中充分覆盖的动作的价值，导致学到的策略在线上表现很差。因此，Offline RL 需要使用特殊的技术来处理这种分布差异，例如策略约束、保守 Q 学习 (Conservative Q-Learning, CQL) 等。
*   **Q: 讨论的伦理问题感觉很难解决，是不是意味着不应该用 RL 做推荐？**
    *   A: 不是说不应该用，而是要**负责任地用**。技术本身是中立的，关键在于设计者和应用者的目标与方法。认识到这些伦理风险是第一步，然后需要在算法设计（如奖励函数、公平性约束）、产品设计（如用户控制权、透明度）和治理层面（如审计、法规）采取措施来缓解这些风险。这是一个需要持续关注和改进的过程。

# 与后续课程的联系 (Connections to Future Topics)

*   本周对推荐系统 MDP 定义和挑战的讨论，为期末项目（如果选择相关方向）提供了具体的思路和素材。
*   对伦理问题的讨论，将在下周总结实践挑战和负责任 AI 时进一步升华。
*   对 Offline RL 的提及，为后续可能的前沿方向介绍做了铺垫。

# 教师准备建议 (Preparation Suggestions)

*   熟悉电商/内容推荐的基本流程和常用指标。
*   准备好引导学生深入讨论推荐系统 MDP 定义（特别是 S, A, R）的问题。
*   查找一些关于推荐系统伦理问题的案例或文章，作为讨论素材。
*   对适用于推荐的 RL 算法类别有基本了解。
*   准备好引导伦理讨论的框架和关键问题。